"""
ç½‘é¡µåˆ†ææ’ä»¶ - ç½‘é¡µåˆ†æå™¨æ¨¡å—

è¿™ä¸ªæ¨¡å—æ˜¯ç½‘é¡µåˆ†æçš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ï¼š
- ç½‘é¡µå†…å®¹çš„å¼‚æ­¥æŠ“å–
- URLçš„æå–å’ŒéªŒè¯
- ç½‘é¡µå†…å®¹çš„ç»“æ„åŒ–è§£æ
- ç‰¹å®šç±»å‹å†…å®¹çš„æå–ï¼ˆå›¾ç‰‡ã€é“¾æ¥ã€è¡¨æ ¼ç­‰ï¼‰
- ç½‘é¡µæˆªå›¾çš„æ•è·

ä½¿ç”¨å¼‚æ­¥HTTPå®¢æˆ·ç«¯å’ŒBeautifulSoupè¿›è¡Œç½‘é¡µå¤„ç†ï¼Œæ”¯æŒä»£ç†ã€é‡è¯•ç­‰é«˜çº§åŠŸèƒ½ã€‚
"""

import gc
import io
import re
import time
from urllib.parse import urljoin, urlparse

import httpx
import psutil
from bs4 import BeautifulSoup
from PIL import Image

from astrbot.api import logger


# è‡ªå®šä¹‰å¼‚å¸¸ç±»
class WebAnalyzerException(Exception):
    """ç½‘é¡µåˆ†æå™¨åŸºç¡€å¼‚å¸¸ç±»"""

    pass


class NetworkError(WebAnalyzerException):
    """ç½‘ç»œç›¸å…³é”™è¯¯"""

    pass


class ParsingError(WebAnalyzerException):
    """ç½‘é¡µè§£æç›¸å…³é”™è¯¯"""

    pass


class ScreenshotError(WebAnalyzerException):
    """ç½‘é¡µæˆªå›¾ç›¸å…³é”™è¯¯"""

    pass


class ContentExtractionError(WebAnalyzerException):
    """å†…å®¹æå–ç›¸å…³é”™è¯¯"""

    pass


class WebAnalyzer:
    """ç½‘é¡µåˆ†æå™¨æ ¸å¿ƒç±»

    è¿™ä¸ªç±»æä¾›äº†å®Œæ•´çš„ç½‘é¡µåˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - ç½‘é¡µå†…å®¹çš„å¼‚æ­¥æŠ“å–
    - URLçš„æå–å’ŒéªŒè¯
    - HTMLå†…å®¹çš„è§£æå’Œç»“æ„åŒ–
    - ç‰¹å®šç±»å‹å†…å®¹çš„æå–
    - ç½‘é¡µæˆªå›¾çš„æ•è·

    æ”¯æŒå¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç¡®ä¿èµ„æºçš„æ­£ç¡®é‡Šæ”¾ã€‚
    """

    # ç±»çº§åˆ«çš„æµè§ˆå™¨å®ä¾‹æ± ï¼Œç”¨äºå¤ç”¨æµè§ˆå™¨å®ä¾‹
    _browser_pool = []
    _max_browser_instances = 3  # æœ€å¤§æµè§ˆå™¨å®ä¾‹æ•°é‡
    _browser_last_used = {}  # è®°å½•æ¯ä¸ªæµè§ˆå™¨å®ä¾‹çš„æœ€åä½¿ç”¨æ—¶é—´
    _browser_lock = None  # æµè§ˆå™¨å®ä¾‹æ± é”
    _last_cleanup_time = 0  # ä¸Šæ¬¡æ¸…ç†æ—¶é—´ï¼Œç”¨äºå®šæœŸæ¸…ç†ä»»åŠ¡
    _cleanup_interval = 60 * 5  # æ¸…ç†é—´éš”ï¼Œ5åˆ†é’Ÿ
    _instance_timeout = 60 * 30  # å®ä¾‹è¶…æ—¶æ—¶é—´ï¼Œ30åˆ†é’Ÿæœªä½¿ç”¨åˆ™æ¸…ç†

    def __init__(
        self,
        max_content_length: int = 10000,
        timeout: int = 30,
        user_agent: str = None,
        proxy: str = None,
        retry_count: int = 3,
        retry_delay: int = 2,
        enable_memory_monitor: bool = True,
        memory_threshold: float = 80.0,  # å†…å­˜ä½¿ç”¨é˜ˆå€¼ç™¾åˆ†æ¯”
        enable_unified_domain: bool = True,  # æ˜¯å¦å¯ç”¨åŸŸåç»Ÿä¸€å¤„ç†
    ):
        """åˆå§‹åŒ–ç½‘é¡µåˆ†æå™¨

        Args:
            max_content_length: æå–çš„æœ€å¤§å†…å®¹é•¿åº¦ï¼Œé˜²æ­¢å†…å®¹è¿‡å¤§
            timeout: HTTPè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œå•ä½ä¸ºç§’
            user_agent: è¯·æ±‚æ—¶ä½¿ç”¨çš„User-Agentå¤´
            proxy: HTTPä»£ç†è®¾ç½®ï¼Œæ ¼å¼ä¸º http://host:port æˆ– https://host:port
            retry_count: è¯·æ±‚å¤±è´¥æ—¶çš„é‡è¯•æ¬¡æ•°
            retry_delay: é‡è¯•é—´éš”æ—¶é—´ï¼Œå•ä½ä¸ºç§’
            enable_memory_monitor: æ˜¯å¦å¯ç”¨å†…å­˜ç›‘æ§
            memory_threshold: å†…å­˜ä½¿ç”¨é˜ˆå€¼ç™¾åˆ†æ¯”ï¼Œè¶…è¿‡æ­¤é˜ˆå€¼æ—¶è‡ªåŠ¨é‡Šæ”¾å†…å­˜
            enable_unified_domain: æ˜¯å¦å¯ç”¨åŸŸåç»Ÿä¸€å¤„ç†ï¼ˆå¦‚google.comå’Œwww.google.comè§†ä¸ºåŒä¸€åŸŸåï¼‰
        """
        self.max_content_length = max_content_length
        self.timeout = timeout
        self.user_agent = (
            user_agent
            or "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )
        self.proxy = proxy
        self.retry_count = retry_count
        self.retry_delay = retry_delay
        self.client = None
        self.browser = None
        # å†…å­˜ç›‘æ§ç›¸å…³
        self.enable_memory_monitor = enable_memory_monitor
        self.memory_threshold = memory_threshold
        self.last_memory_check = time.time()
        self.memory_check_interval = 60 * 5  # å†…å­˜æ£€æŸ¥é—´éš”ï¼Œå•ä½ä¸ºç§’ï¼Œä»60ç§’å»¶é•¿åˆ°5åˆ†é’Ÿ
        # URLå¤„ç†ç›¸å…³
        self.enable_unified_domain = enable_unified_domain

        # åˆå§‹åŒ–æµè§ˆå™¨é”
        if not WebAnalyzer._browser_lock:
            import asyncio

            WebAnalyzer._browser_lock = asyncio.Lock()

    @staticmethod
    async def _cleanup_browser_pool():
        """å®šæœŸæ¸…ç†æµè§ˆå™¨å®ä¾‹æ± ï¼Œç§»é™¤è¿‡æœŸæˆ–æ— æ•ˆçš„å®ä¾‹

        è¯¥æ–¹æ³•ä¼šï¼š
        1. æ¸…ç†è¶…è¿‡30åˆ†é’Ÿæœªä½¿ç”¨çš„æµè§ˆå™¨å®ä¾‹
        2. æ¸…ç†å·²æ–­å¼€è¿æ¥çš„æµè§ˆå™¨å®ä¾‹
        3. ç¡®ä¿å®ä¾‹æ± å¤§å°ä¸è¶…è¿‡æœ€å¤§å€¼
        """
        try:
            current_time = time.time()

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡Œæ¸…ç†
            if (
                current_time - WebAnalyzer._last_cleanup_time
                < WebAnalyzer._cleanup_interval
            ):
                return

            async with WebAnalyzer._browser_lock:
                WebAnalyzer._last_cleanup_time = current_time
                valid_browsers = []

                for browser in WebAnalyzer._browser_pool:
                    last_used = WebAnalyzer._browser_last_used.get(id(browser), 0)
                    try:
                        # æ£€æŸ¥æµè§ˆå™¨å®ä¾‹æ˜¯å¦æœ‰æ•ˆï¼ˆæœªè¿‡æœŸä¸”å·²è¿æ¥ï¼‰
                        if (
                            current_time - last_used < WebAnalyzer._instance_timeout
                            and browser.is_connected()
                        ):
                            valid_browsers.append(browser)
                        else:
                            # å…³é—­è¿‡æœŸæˆ–å·²æ–­å¼€è¿æ¥çš„æµè§ˆå™¨å®ä¾‹
                            await browser.close()
                            logger.info("å…³é—­è¿‡æœŸæˆ–å·²æ–­å¼€è¿æ¥çš„æµè§ˆå™¨å®ä¾‹")
                    except Exception as e:
                        logger.error(f"æ£€æŸ¥æµè§ˆå™¨å®ä¾‹çŠ¶æ€å¤±è´¥: {e}, å°†å…³é—­è¯¥å®ä¾‹")
                        try:
                            await browser.close()
                        except Exception as close_e:
                            logger.error(f"å…³é—­å¼‚å¸¸æµè§ˆå™¨å®ä¾‹å¤±è´¥: {close_e}")

                # æ›´æ–°æµè§ˆå™¨å®ä¾‹æ± ï¼Œç¡®ä¿ä¸è¶…è¿‡æœ€å¤§å®ä¾‹æ•°é‡
                WebAnalyzer._browser_pool = valid_browsers[
                    : WebAnalyzer._max_browser_instances
                ]
                logger.debug(
                    f"æµè§ˆå™¨å®ä¾‹æ± æ¸…ç†å®Œæˆï¼Œå½“å‰æ± å¤§å°: {len(WebAnalyzer._browser_pool)}"
                )
        except Exception as e:
            logger.error(f"æ¸…ç†æµè§ˆå™¨å®ä¾‹æ± å¤±è´¥: {e}")

    def _check_memory_usage(self):
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œè¶…è¿‡é˜ˆå€¼æ—¶è‡ªåŠ¨é‡Šæ”¾å†…å­˜

        Returns:
            bool: å¦‚æœé‡Šæ”¾äº†å†…å­˜ï¼Œè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        current_time = time.time()
        # å®šæœŸæ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
        if current_time - self.last_memory_check < self.memory_check_interval:
            return False

        self.last_memory_check = current_time

        try:
            # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
            memory_info = psutil.virtual_memory()
            memory_usage = memory_info.percent

            logger.debug(f"å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ: {memory_usage:.1f}%")

            if memory_usage > self.memory_threshold:
                logger.warning(
                    f"å†…å­˜ä½¿ç”¨è¶…è¿‡é˜ˆå€¼ ({self.memory_threshold}%), è‡ªåŠ¨é‡Šæ”¾èµ„æº"
                )
                # é‡Šæ”¾å†…å­˜
                self._release_memory()
                return True
        except Exception as e:
            logger.error(f"æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µå¤±è´¥: {e}")

        return False

    async def _optimize_browser_pool(self):
        """å¼‚æ­¥ä¼˜åŒ–æµè§ˆå™¨å®ä¾‹æ± ï¼Œæ ¹æ®å†…å­˜ä½¿ç”¨æƒ…å†µè°ƒæ•´å®ä¾‹æ•°é‡

        æ ¹æ®å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µåŠ¨æ€è°ƒæ•´æµè§ˆå™¨å®ä¾‹æ± å¤§å°ï¼š
        - å†…å­˜ä½¿ç”¨ç‡ > 90%: åªä¿ç•™0ä¸ªå®ä¾‹
        - å†…å­˜ä½¿ç”¨ç‡ > 80%: åªä¿ç•™1ä¸ªå®ä¾‹
        - å†…å­˜ä½¿ç”¨ç‡ > 70%: åªä¿ç•™2ä¸ªå®ä¾‹
        - å†…å­˜ä½¿ç”¨ç‡ â‰¤70%: ä¿ç•™æœ€å¤§æ•°é‡å‡1ä¸ªå®ä¾‹
        """
        try:
            async with WebAnalyzer._browser_lock:
                # è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ
                memory_info = psutil.virtual_memory()
                memory_usage = memory_info.percent

                # æ ¹æ®å†…å­˜ä½¿ç”¨æƒ…å†µå†³å®šä¿ç•™çš„å®ä¾‹æ•°é‡
                if memory_usage > 90:
                    max_keep = 0
                elif memory_usage > 80:
                    max_keep = 1
                elif memory_usage > 70:
                    max_keep = 2
                else:
                    max_keep = WebAnalyzer._max_browser_instances - 1

                # é‡Šæ”¾è¶…å‡ºä¿ç•™æ•°é‡çš„æµè§ˆå™¨å®ä¾‹
                while len(WebAnalyzer._browser_pool) > max_keep:
                    browser = WebAnalyzer._browser_pool.pop()
                    try:
                        if browser.is_connected():
                            await browser.close()
                            logger.info(
                                f"é‡Šæ”¾ç©ºé—²æµè§ˆå™¨å®ä¾‹ï¼Œå½“å‰æ± å¤§å°: {len(WebAnalyzer._browser_pool)}"
                            )
                    except Exception as e:
                        logger.error(f"é‡Šæ”¾æµè§ˆå™¨å®ä¾‹å¤±è´¥: {e}")
                        # å¿½ç•¥å•ä¸ªå®ä¾‹é‡Šæ”¾å¤±è´¥ï¼Œç»§ç»­å¤„ç†å…¶ä»–å®ä¾‹
        except Exception as e:
            logger.error(f"ä¼˜åŒ–æµè§ˆå™¨å®ä¾‹æ± å¤±è´¥: {e}")

    def _release_memory(self):
        """é‡Šæ”¾å†…å­˜èµ„æº

        æ‰§è¡Œåƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾ä¸å†ä½¿ç”¨çš„èµ„æºï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨

        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. æ‰§è¡Œåƒåœ¾å›æ”¶é‡Šæ”¾å†…å­˜
        2. æ™ºèƒ½è°ƒæ•´æµè§ˆå™¨å®ä¾‹æ± å¤§å°
        3. å¢å¼ºå®¹é”™æœºåˆ¶ï¼Œç¡®ä¿å†…å­˜é‡Šæ”¾è¿‡ç¨‹ç¨³å®š
        """
        try:
            # æ‰§è¡Œåƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾å†…å­˜
            collected = gc.collect()
            logger.info(f"æ‰§è¡Œåƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾å†…å­˜ï¼Œå›æ”¶äº† {collected} ä¸ªå¯¹è±¡")

            # åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œæµè§ˆå™¨æ± ä¼˜åŒ–
            try:
                import asyncio

                loop = asyncio.get_event_loop()
                if loop.is_running():
                    loop.create_task(self._optimize_browser_pool())
                else:
                    # å¦‚æœäº‹ä»¶å¾ªç¯æœªè¿è¡Œï¼Œè®°å½•è­¦å‘Šä½†ä¸æŠ›å‡ºå¼‚å¸¸
                    logger.warning("äº‹ä»¶å¾ªç¯æœªè¿è¡Œï¼Œè·³è¿‡æµè§ˆå™¨å®ä¾‹æ± ä¼˜åŒ–")
            except Exception as e:
                logger.error(f"æ‰§è¡Œæµè§ˆå™¨å®ä¾‹æ± ä¼˜åŒ–å¤±è´¥: {e}")
        except Exception as e:
            logger.error(f"é‡Šæ”¾å†…å­˜èµ„æºå¤±è´¥: {e}")
            # å¢å¼ºå®¹é”™æœºåˆ¶ï¼Œç¡®ä¿å†…å­˜é‡Šæ”¾å¤±è´¥ä¸ä¼šå½±å“æ’ä»¶æ­£å¸¸è¿è¡Œ

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£

        åˆå§‹åŒ–å¼‚æ­¥HTTPå®¢æˆ·ç«¯ï¼Œé…ç½®ï¼š
        - è¯·æ±‚è¶…æ—¶æ—¶é—´
        - ä»£ç†è®¾ç½®ï¼ˆå¦‚æœæä¾›ï¼‰
        - å…¶ä»–HTTPå®¢æˆ·ç«¯å‚æ•°

        Returns:
            è¿”å›WebAnalyzerå®ä¾‹è‡ªèº«ï¼Œç”¨äºä¸Šä¸‹æ–‡ç®¡ç†
        """
        # é…ç½®å®¢æˆ·ç«¯å‚æ•°
        client_params = {"timeout": self.timeout}

        # æ·»åŠ ä»£ç†é…ç½®ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.proxy:
            client_params["proxies"] = {"http://": self.proxy, "https://": self.proxy}

        self.client = httpx.AsyncClient(**client_params)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£

        æ¸…ç†èµ„æºï¼Œç¡®ä¿ï¼š
        - å¼‚æ­¥HTTPå®¢æˆ·ç«¯æ­£ç¡®å…³é—­
        - æµè§ˆå™¨å®ä¾‹æ­£ç¡®å¤„ç†ï¼ˆæ”¾å›æ± ä¸­æˆ–å…³é—­ï¼‰
        - èµ„æºæ³„æ¼çš„é˜²æ­¢

        Args:
            exc_type: å¼‚å¸¸ç±»å‹ï¼ˆå¦‚æœæœ‰ï¼‰
            exc_val: å¼‚å¸¸å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
            exc_tb: å¼‚å¸¸å›æº¯ï¼ˆå¦‚æœæœ‰ï¼‰
        """
        if self.client:
            await self.client.aclose()

        if self.browser:
            try:
                # å°†æµè§ˆå™¨å®ä¾‹æ”¾å›æ± ä¸­ï¼Œä»¥ä¾¿å¤ç”¨
                async with WebAnalyzer._browser_lock:
                    # æ£€æŸ¥æµè§ˆå™¨å®ä¾‹æ˜¯å¦ä»ç„¶å¯ç”¨
                    if (
                        len(WebAnalyzer._browser_pool)
                        < WebAnalyzer._max_browser_instances
                    ):
                        # æ›´æ–°æœ€åä½¿ç”¨æ—¶é—´
                        WebAnalyzer._browser_last_used[id(self.browser)] = time.time()
                        # å°†æµè§ˆå™¨å®ä¾‹æ”¾å›æ± ä¸­
                        WebAnalyzer._browser_pool.append(self.browser)
                        logger.debug(
                            f"æµè§ˆå™¨å®ä¾‹å·²æ”¾å›æ± ä¸­ï¼Œå½“å‰æ± å¤§å°: {len(WebAnalyzer._browser_pool)}"
                        )
                    else:
                        # æ± å·²æ»¡ï¼Œå…³é—­æµè§ˆå™¨å®ä¾‹
                        await self.browser.close()
                        logger.debug("æµè§ˆå™¨å®ä¾‹æ± å·²æ»¡ï¼Œå…³é—­æµè§ˆå™¨å®ä¾‹")
            except Exception as e:
                logger.error(f"å¤„ç†æµè§ˆå™¨å®ä¾‹å¤±è´¥: {e}")
                # å‡ºç°é”™è¯¯æ—¶ï¼Œç¡®ä¿æµè§ˆå™¨å®ä¾‹è¢«å…³é—­
                try:
                    await self.browser.close()
                except Exception:
                    pass

        # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
        self._check_memory_usage()

    def extract_urls(
        self,
        text: str,
        enable_no_protocol: bool = False,
        default_protocol: str = "https",
    ) -> list[str]:
        """ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰HTTP/HTTPS URLé“¾æ¥

        ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ–‡æœ¬ä¸­çš„URLï¼Œæ”¯æŒï¼š
        - HTTPå’ŒHTTPSåè®®
        - å„ç§å¸¸è§çš„URLæ ¼å¼
        - æ’é™¤ä¸­æ–‡ç­‰éASCIIå­—ç¬¦ä½œä¸ºURLçš„ä¸€éƒ¨åˆ†
        - å¯é€‰è¯†åˆ«æ— åè®®å¤´çš„URLï¼ˆå¦‚ www.example.comï¼‰

        Args:
            text: è¦ä»ä¸­æå–URLçš„æ–‡æœ¬å†…å®¹
            enable_no_protocol: æ˜¯å¦è¯†åˆ«æ— åè®®å¤´çš„URL
            default_protocol: æ— åè®®å¤´URLä½¿ç”¨çš„é»˜è®¤åè®®ï¼ˆhttpæˆ–httpsï¼‰

        Returns:
            åŒ…å«æ‰€æœ‰æå–åˆ°çš„URLçš„åˆ—è¡¨
        """
        urls = self._extract_protocol_urls(text)
        if enable_no_protocol:
            no_protocol_urls = self._extract_no_protocol_urls(
                text, urls, default_protocol
            )
            urls.extend(no_protocol_urls)
        return urls

    def _extract_protocol_urls(self, text: str) -> list[str]:
        """æå–å¸¦åè®®å¤´çš„URL"""
        url_pattern = r"https?://[^\s\u4e00-\u9fff]+"
        return re.findall(url_pattern, text)

    def _extract_no_protocol_urls(
        self, text: str, existing_urls: list[str], default_protocol: str
    ) -> list[str]:
        """æå–æ— åè®®å¤´çš„URL"""
        text_for_no_protocol = self._remove_existing_urls(text, existing_urls)
        no_protocol_urls = self._find_no_protocol_urls(text_for_no_protocol)
        return self._format_no_protocol_urls(no_protocol_urls, default_protocol)

    def _remove_existing_urls(self, text: str, urls: list[str]) -> str:
        """ä»æ–‡æœ¬ä¸­ç§»é™¤å·²æå–çš„URL"""
        text_for_no_protocol = text
        for url in urls:
            text_for_no_protocol = text_for_no_protocol.replace(url, "")
        return text_for_no_protocol

    def _find_no_protocol_urls(self, text: str) -> list[str]:
        """æŸ¥æ‰¾æ— åè®®å¤´çš„URL"""
        no_protocol_pattern = r"(?:www\.)?[a-zA-Z0-9][a-zA-Z0-9-]{1,61}[a-zA-Z0-9](?:\.[a-zA-Z0-9][a-zA-Z0-9-]{0,61}[a-zA-Z0-9])+(?:/[^\s\u4e00-\u9fff]*)?"
        return re.findall(no_protocol_pattern, text)

    def _format_no_protocol_urls(
        self, urls: list[str], default_protocol: str
    ) -> list[str]:
        """æ ¼å¼åŒ–æ— åè®®å¤´çš„URL"""
        formatted_urls = []
        for url in urls:
            cleaned_url = url.rstrip(".,;:!?)'\"")
            full_url = f"{default_protocol}://{cleaned_url}"
            formatted_urls.append(full_url)
        return formatted_urls

    def is_valid_url(self, url: str) -> bool:
        """éªŒè¯URLæ ¼å¼æ˜¯å¦æœ‰æ•ˆ

        æ£€æŸ¥URLæ˜¯å¦ç¬¦åˆåŸºæœ¬æ ¼å¼è¦æ±‚ï¼š
        - å¿…é¡»åŒ…å«æœ‰æ•ˆçš„åè®®ï¼ˆhttp/httpsï¼‰
        - å¿…é¡»åŒ…å«æœ‰æ•ˆçš„åŸŸåæˆ–IPåœ°å€
        - å¿…é¡»èƒ½è¢«æ­£ç¡®è§£æ

        Args:
            url: è¦éªŒè¯çš„URLå­—ç¬¦ä¸²

        Returns:
            Trueè¡¨ç¤ºURLæ ¼å¼æœ‰æ•ˆï¼ŒFalseè¡¨ç¤ºæ— æ•ˆ
        """
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except Exception:
            return False

    def normalize_url(self, url: str) -> str:
        """è§„èŒƒåŒ–URLï¼Œç»Ÿä¸€æ ¼å¼

        å¯¹URLè¿›è¡Œè§„èŒƒåŒ–å¤„ç†ï¼š
        - ğŸ”„  è½¬æ¢ä¸ºå°å†™
        - ğŸ“  ç»Ÿä¸€å¤„ç†å°¾éƒ¨æ–œæ 
        - ğŸ§¹  å»é™¤å¤šä½™çš„æŸ¥è¯¢å‚æ•°å’Œç‰‡æ®µï¼ˆå¯é€‰ï¼‰
        - ğŸŒ  ç»Ÿä¸€åŸŸåæ ¼å¼ï¼ˆå¦‚google.comå’Œwww.google.comè§†ä¸ºåŒä¸€åŸŸåï¼‰

        Args:
            url: è¦è§„èŒƒåŒ–çš„URLå­—ç¬¦ä¸²

        Returns:
            è§„èŒƒåŒ–åçš„URLå­—ç¬¦ä¸²
        """
        try:
            parsed = urlparse(url)
            netloc = self._normalize_netloc(parsed.netloc.lower())
            normalized = parsed._replace(
                scheme=parsed.scheme.lower(),
                netloc=netloc,
                path=parsed.path.rstrip("/"),
            )
            return normalized.geturl()
        except Exception:
            return url

    def _normalize_netloc(self, netloc: str) -> str:
        """è§„èŒƒåŒ–ç½‘ç»œä½ç½®ï¼ˆåŸŸåæˆ–IPï¼‰"""
        if not self.enable_unified_domain or not netloc or "." not in netloc:
            return netloc
        if netloc.startswith("www.") or ".www." in netloc:
            return netloc
        if self._is_ip_address(netloc):
            return netloc
        return f"www.{netloc}"

    def _is_ip_address(self, netloc: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºIPåœ°å€"""
        try:
            import ipaddress

            ipaddress.ip_address(netloc)
            return True
        except ValueError:
            return False

    async def fetch_webpage(self, url: str) -> str:
        """å¼‚æ­¥æŠ“å–ç½‘é¡µHTMLå†…å®¹

        ä½¿ç”¨å¼‚æ­¥HTTPå®¢æˆ·ç«¯æŠ“å–ç½‘é¡µï¼Œæ”¯æŒï¼š
        - è‡ªå®šä¹‰User-Agent
        - è‡ªåŠ¨è·Ÿéšé‡å®šå‘
        - é…ç½®çš„ä»£ç†è®¾ç½®
        - æ™ºèƒ½é‡è¯•æœºåˆ¶ï¼ˆå¤±è´¥åè‡ªåŠ¨é‡è¯•ï¼‰

        Args:
            url: è¦æŠ“å–çš„ç½‘é¡µURL

        Returns:
            ç½‘é¡µçš„HTMLæ–‡æœ¬å†…å®¹

        Raises:
            NetworkError: å½“ç½‘ç»œè¯·æ±‚å¤±è´¥æ—¶æŠ›å‡º
        """
        headers = {
            "User-Agent": self.user_agent,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "DNT": "1",
            "Sec-GPC": "1",
        }

        # å®ç°é‡è¯•æœºåˆ¶ï¼Œæœ€å¤šå°è¯• retry_count + 1 æ¬¡
        for attempt in range(self.retry_count + 1):
            try:
                response = await self.client.get(
                    url, headers=headers, follow_redirects=True
                )
                response.raise_for_status()

                logger.info(
                    f"æŠ“å–ç½‘é¡µæˆåŠŸ: {url} (å°è¯• {attempt + 1}/{self.retry_count + 1})"
                )
                return response.text
            except Exception as e:
                if attempt < self.retry_count:
                    # è¿˜æœ‰é‡è¯•æ¬¡æ•°ï¼Œç­‰å¾… retry_delay ç§’åé‡è¯•
                    logger.warning(
                        f"æŠ“å–ç½‘é¡µå¤±è´¥ï¼Œå°†é‡è¯•: {url}, é”™è¯¯: {e} (å°è¯• {attempt + 1}/{self.retry_count + 1})"
                    )
                    import asyncio

                    await asyncio.sleep(self.retry_delay)
                else:
                    # é‡è¯•æ¬¡æ•°ç”¨å®Œï¼ŒæŠ›å‡ºç½‘ç»œé”™è¯¯
                    logger.error(
                        f"æŠ“å–ç½‘é¡µå¤±è´¥: {url}, é”™è¯¯: {e} (å°è¯• {attempt + 1}/{self.retry_count + 1})"
                    )
                    raise NetworkError(f"æŠ“å–ç½‘é¡µå¤±è´¥: {url}, é”™è¯¯: {str(e)}") from e

    def extract_content(self, html: str, url: str) -> dict:
        """ä»HTMLä¸­æå–ç»“æ„åŒ–çš„ç½‘é¡µå†…å®¹

        è§£æHTMLæ–‡æ¡£ï¼Œæå–å…³é”®å†…å®¹ï¼š
        - ç½‘é¡µæ ‡é¢˜
        - ä¸»è¦æ­£æ–‡å†…å®¹
        - æ”¯æŒå¤šç§å†…å®¹é€‰æ‹©ç­–ç•¥

        ä½¿ç”¨BeautifulSoupè¿›è¡ŒHTMLè§£æï¼Œä¼˜å…ˆé€‰æ‹©è¯­ä¹‰åŒ–æ ‡ç­¾
        ï¼ˆå¦‚articleã€mainç­‰ï¼‰æå–å†…å®¹ï¼Œç¡®ä¿æå–çš„å†…å®¹è´¨é‡ã€‚

        Args:
            html: ç½‘é¡µçš„HTMLæ–‡æœ¬å†…å®¹
            url: ç½‘é¡µçš„åŸå§‹URLï¼Œç”¨äºç»“æœè¿”å›

        Returns:
            åŒ…å«æ ‡é¢˜ã€å†…å®¹å’ŒURLçš„å­—å…¸

        Raises:
            ParsingError: å½“HTMLè§£æå¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            soup = BeautifulSoup(html, "lxml")

            # æå–ç½‘é¡µæ ‡é¢˜
            title_text = self._extract_title(soup)

            # æå–æ–‡ç« å†…å®¹
            content_text = self._extract_main_content(soup)

            # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé˜²æ­¢å†…å®¹è¿‡å¤§
            content_text = self._limit_content_length(content_text)

            return {"title": title_text, "content": content_text, "url": url}
        except Exception as e:
            logger.error(f"è§£æç½‘é¡µå†…å®¹å¤±è´¥: {e}")
            raise ParsingError(f"è§£æç½‘é¡µå†…å®¹å¤±è´¥: {url}, é”™è¯¯: {str(e)}") from e

    def _extract_title(self, soup: BeautifulSoup) -> str:
        """ä»BeautifulSoupå¯¹è±¡ä¸­æå–ç½‘é¡µæ ‡é¢˜

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            ç½‘é¡µæ ‡é¢˜æ–‡æœ¬
        """
        title = soup.find("title")
        return title.get_text().strip() if title else "æ— æ ‡é¢˜"

    def _extract_main_content(self, soup: BeautifulSoup) -> str:
        """ä»BeautifulSoupå¯¹è±¡ä¸­æå–ä¸»è¦å†…å®¹

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            æå–çš„ä¸»è¦å†…å®¹æ–‡æœ¬
        """
        # å°è¯•æå–æ–‡ç« å†…å®¹ï¼ˆä¼˜å…ˆé€‰æ‹©articleã€mainç­‰è¯­ä¹‰åŒ–æ ‡ç­¾ï¼‰
        content_selectors = [
            "article",  # è¯­ä¹‰åŒ–æ–‡ç« æ ‡ç­¾
            "main",  # è¯­ä¹‰åŒ–ä¸»å†…å®¹æ ‡ç­¾
            ".article-content",  # å¸¸è§æ–‡ç« å†…å®¹ç±»å
            ".post-content",  # å¸¸è§åšå®¢å†…å®¹ç±»å
            ".content",  # é€šç”¨å†…å®¹ç±»å
            "body",  # å…œåº•ï¼šä½¿ç”¨æ•´ä¸ªbody
        ]

        content_text = ""
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                # æ¸…ç†å†…å®¹ï¼Œç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
                cleaned_element = self._clean_content_element(element)
                text = cleaned_element.get_text(separator="\n", strip=True)
                if len(text) > len(content_text):
                    content_text = text

        # å¦‚æœæ²¡æ‰¾åˆ°åˆé€‚çš„å†…å®¹ï¼Œä½¿ç”¨bodyä½œä¸ºæœ€åçš„å…œåº•æ–¹æ¡ˆ
        if not content_text:
            body = soup.find("body")
            if body:
                cleaned_body = self._clean_content_element(body)
                content_text = cleaned_body.get_text(separator="\n", strip=True)

        return content_text

    def _clean_content_element(self, element: BeautifulSoup) -> BeautifulSoup:
        """æ¸…ç†å†…å®¹å…ƒç´ ï¼Œç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾

        Args:
            element: BeautifulSoupå…ƒç´ 

        Returns:
            æ¸…ç†åçš„BeautifulSoupå…ƒç´ 
        """
        # ç›´æ¥å¤„ç†å…ƒç´ ï¼Œä¸éœ€è¦åˆ›å»ºå‰¯æœ¬
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾ï¼Œé¿å…å¹²æ‰°å†…å®¹æå–
        for script in element.find_all(["script", "style"]):
            script.decompose()
        return element

    def _limit_content_length(self, content: str) -> str:
        """é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé˜²æ­¢å†…å®¹è¿‡å¤§

        Args:
            content: åŸå§‹å†…å®¹æ–‡æœ¬

        Returns:
            é™åˆ¶é•¿åº¦åçš„å†…å®¹æ–‡æœ¬
        """
        if len(content) > self.max_content_length:
            return content[: self.max_content_length] + "..."
        return content

    def crop_screenshot(
        self, screenshot_bytes: bytes, crop_area: tuple[int, int, int, int]
    ) -> bytes:
        """è£å‰ªæˆªå›¾

        Args:
            screenshot_bytes: åŸå§‹æˆªå›¾äºŒè¿›åˆ¶æ•°æ®
            crop_area: è£å‰ªåŒºåŸŸï¼Œæ ¼å¼ä¸º (left, top, right, bottom)

        Returns:
            è£å‰ªåçš„æˆªå›¾äºŒè¿›åˆ¶æ•°æ®
        """
        try:
            # å°†äºŒè¿›åˆ¶æ•°æ®è½¬æ¢ä¸ºImageå¯¹è±¡
            image = Image.open(io.BytesIO(screenshot_bytes))

            # è£å‰ªå›¾ç‰‡
            cropped_image = image.crop(crop_area)

            # å°†è£å‰ªåçš„å›¾ç‰‡è½¬æ¢å›äºŒè¿›åˆ¶æ•°æ®
            output = io.BytesIO()
            cropped_image.save(output, format="PNG")
            return output.getvalue()
        except Exception as e:
            logger.error(f"è£å‰ªæˆªå›¾å¤±è´¥: {e}")
            raise ScreenshotError(f"è£å‰ªæˆªå›¾å¤±è´¥: {str(e)}") from e

    async def capture_screenshot(
        self,
        url: str,
        quality: int = 80,
        width: int = 1280,
        height: int = 720,
        full_page: bool = False,
        wait_time: int = 2000,
        format: str = "jpeg",
    ) -> bytes:
        """ä½¿ç”¨Playwrightæ•è·ç½‘é¡µæˆªå›¾

        è‡ªåŠ¨å¤„ç†æµè§ˆå™¨çš„å®‰è£…å’Œé…ç½®ï¼Œæ”¯æŒï¼š
        - è‡ªå®šä¹‰åˆ†è¾¨ç‡å’Œè´¨é‡
        - å…¨å±æˆªå›¾æˆ–å¯è§†åŒºåŸŸæˆªå›¾
        - è‡ªå®šä¹‰ç­‰å¾…æ—¶é—´ï¼Œç¡®ä¿é¡µé¢åŠ è½½å®Œæˆ
        - æ”¯æŒJPEGå’ŒPNGæ ¼å¼

        Args:
            url: è¦æˆªå›¾çš„ç½‘é¡µURL
            quality: æˆªå›¾è´¨é‡ï¼ŒèŒƒå›´1-100
            width: æˆªå›¾å®½åº¦ï¼ˆåƒç´ ï¼‰
            height: æˆªå›¾é«˜åº¦ï¼ˆåƒç´ ï¼‰
            full_page: æ˜¯å¦æˆªå–æ•´ä¸ªé¡µé¢ï¼ŒFalseä»…æˆªå–å¯è§†åŒºåŸŸ
            wait_time: é¡µé¢åŠ è½½åç­‰å¾…çš„æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½
            format: æˆªå›¾æ ¼å¼ï¼Œæ”¯æŒ"jpeg"å’Œ"png"

        Returns:
            æˆªå›¾çš„äºŒè¿›åˆ¶æ•°æ®

        Raises:
            ScreenshotError: å½“æˆªå›¾å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            import os
            import subprocess
            import sys

            from playwright.async_api import async_playwright

            # åªåœ¨ç¬¬ä¸€æ¬¡æ‰§è¡Œæ—¶æ£€æŸ¥æµè§ˆå™¨å®‰è£…
            if not hasattr(self, "_playwright_browser_checked"):
                logger.info("æ­£åœ¨æ£€æŸ¥æµè§ˆå™¨...")
                # æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦å·²å®‰è£…
                browser_path = os.path.join(
                    os.path.expanduser("~"), ".cache", "ms-playwright", "chromium"
                )
                if os.path.exists(browser_path):
                    logger.info("æµè§ˆå™¨å·²å®‰è£…ï¼Œè·³è¿‡å®‰è£…æ­¥éª¤")
                else:
                    # å®‰è£…æµè§ˆå™¨
                    logger.info("æ­£åœ¨å®‰è£…æµè§ˆå™¨...")
                    result = subprocess.run(
                        [sys.executable, "-m", "playwright", "install", "chromium"],
                        capture_output=True,
                        text=True,
                    )

                    if result.returncode != 0:
                        logger.error(f"æµè§ˆå™¨å®‰è£…å¤±è´¥: {result.stderr}")
                        raise ScreenshotError(
                            f"æµè§ˆå™¨å®‰è£…å¤±è´¥: {result.stderr}"
                        ) from None
                    logger.info("æµè§ˆå™¨å®‰è£…æˆåŠŸ")

            # æ ‡è®°å·²æ£€æŸ¥æµè§ˆå™¨
            self._playwright_browser_checked = True

            logger.info("æ­£åœ¨å°è¯•æˆªå›¾...")
            screenshot_bytes = None
            playwright_instance = None

            try:
                # æ‰§è¡Œæµè§ˆå™¨å®ä¾‹æ± æ¸…ç†
                await self._cleanup_browser_pool()

                browser = None

                # å°è¯•ä»æµè§ˆå™¨å®ä¾‹æ± è·å–æµè§ˆå™¨å®ä¾‹
                async with WebAnalyzer._browser_lock:
                    # éå†æ± ä¸­çš„å®ä¾‹ï¼Œå¯»æ‰¾æœ‰æ•ˆå®ä¾‹
                    while WebAnalyzer._browser_pool and not browser:
                        candidate_browser = WebAnalyzer._browser_pool.pop(0)
                        try:
                            if candidate_browser.is_connected():
                                browser = candidate_browser
                                logger.debug("ä»æµè§ˆå™¨å®ä¾‹æ± è·å–æœ‰æ•ˆæµè§ˆå™¨å®ä¾‹")
                            else:
                                logger.warn("è·³è¿‡å·²æ–­å¼€è¿æ¥çš„æµè§ˆå™¨å®ä¾‹")
                                await candidate_browser.close()
                        except Exception as e:
                            logger.error(
                                f"æ£€æŸ¥æµè§ˆå™¨å®ä¾‹è¿æ¥çŠ¶æ€å¤±è´¥: {e}, å°†è·³è¿‡è¯¥å®ä¾‹"
                            )
                            try:
                                await candidate_browser.close()
                            except Exception:
                                pass

                if not browser:
                    # æ²¡æœ‰å¯ç”¨çš„æµè§ˆå™¨å®ä¾‹ï¼Œåˆ›å»ºæ–°çš„
                    logger.debug("åˆ›å»ºæ–°çš„æµè§ˆå™¨å®ä¾‹")
                    # å°è¯•å¯åŠ¨playwrightå¹¶æˆªå›¾
                    playwright_instance = await async_playwright().start()
                    # å¯åŠ¨æµè§ˆå™¨ï¼ˆæ— å¤´æ¨¡å¼ï¼Œä¸æ˜¾ç¤ºGUIï¼‰
                    browser = await playwright_instance.chromium.launch(
                        headless=True,
                        # å¢åŠ æµè§ˆå™¨å¯åŠ¨è¶…æ—¶æ—¶é—´åˆ°60ç§’
                        timeout=20000,
                        # æ·»åŠ é¢å¤–çš„å¯åŠ¨å‚æ•°ï¼Œæé«˜å…¼å®¹æ€§å’Œç¨³å®šæ€§
                        args=[
                            "--no-sandbox",  # ç¦ç”¨æ²™ç®±ï¼Œæé«˜å…¼å®¹æ€§
                            "--disable-setuid-sandbox",  # ç¦ç”¨setuidæ²™ç®±
                            "--disable-dev-shm-usage",  # ç¦ç”¨/dev/shmä½¿ç”¨
                            "--disable-gpu",  # ç¦ç”¨GPUåŠ é€Ÿ
                            # ç§»é™¤å›ºå®šç«¯å£ï¼Œè®©playwrightè‡ªåŠ¨åˆ†é…å¯ç”¨ç«¯å£
                        ],
                    )

                try:
                    # åˆ›å»ºæ–°çš„é¡µé¢ï¼Œè®¾ç½®è§†å£å’ŒUser-Agent
                    page = await browser.new_page(
                        viewport={"width": width, "height": height},
                        user_agent=self.user_agent,
                    )

                    # å¯¼èˆªåˆ°ç›®æ ‡URLï¼Œä½¿ç”¨æ›´å®½æ¾çš„ç­‰å¾…æ¡ä»¶
                    await page.goto(url, wait_until="domcontentloaded", timeout=60000)

                    # ç­‰å¾…æŒ‡å®šæ—¶é—´ï¼Œç¡®ä¿é¡µé¢å®Œå…¨åŠ è½½ï¼ˆå°¤å…¶æ˜¯åŠ¨æ€å†…å®¹ï¼‰
                    await page.wait_for_timeout(wait_time)

                    # æ•è·æˆªå›¾
                    screenshot_bytes = await page.screenshot(
                        full_page=full_page,  # æ˜¯å¦æˆªå–æ•´ä¸ªé¡µé¢
                        quality=quality,  # æˆªå›¾è´¨é‡
                        type=format,  # æˆªå›¾æ ¼å¼
                    )
                    logger.info("æˆªå›¾æˆåŠŸ")

                    # å…³é—­é¡µé¢ï¼Œä½†ä¿ç•™æµè§ˆå™¨å®ä¾‹ç”¨äºåç»­å¤ç”¨
                    await page.close()

                    # å¦‚æœæ˜¯æ–°åˆ›å»ºçš„æµè§ˆå™¨å®ä¾‹ï¼Œä¸å…³é—­ï¼Œè€Œæ˜¯å°†å…¶ä¿å­˜åˆ°self.browserä»¥ä¾¿åç»­å¤ç”¨
                    if not playwright_instance:
                        # ä»æ± ä¸­è·å–çš„æµè§ˆå™¨å®ä¾‹ï¼Œä½¿ç”¨åæ”¾å›æ± ä¸­
                        async with WebAnalyzer._browser_lock:
                            # æ›´æ–°æœ€åä½¿ç”¨æ—¶é—´
                            WebAnalyzer._browser_last_used[id(browser)] = time.time()
                            # å°†æµè§ˆå™¨å®ä¾‹æ”¾å›æ± ä¸­
                            WebAnalyzer._browser_pool.append(browser)
                            logger.debug(
                                f"æµè§ˆå™¨å®ä¾‹å·²æ”¾å›æ± ä¸­ï¼Œå½“å‰æ± å¤§å°: {len(WebAnalyzer._browser_pool)}"
                            )
                    else:
                        # æ–°åˆ›å»ºçš„æµè§ˆå™¨å®ä¾‹ï¼Œä¿å­˜åˆ°self.browser
                        self.browser = browser

                    return screenshot_bytes
                except Exception as new_page_error:
                    # å½“ä»æ± ä¸­è·å–çš„æµè§ˆå™¨å®ä¾‹æ— æ•ˆæ—¶ï¼Œæ•è·å¼‚å¸¸å¹¶å¤„ç†
                    if not playwright_instance:
                        # ä»æ± ä¸­è·å–çš„æµè§ˆå™¨å®ä¾‹ï¼Œè¯´æ˜å®ä¾‹å·²å¤±æ•ˆï¼Œä»æ± ä¸­ç§»é™¤å¹¶åˆ›å»ºæ–°å®ä¾‹
                        logger.error(
                            f"ä»æ± ä¸­è·å–çš„æµè§ˆå™¨å®ä¾‹æ— æ•ˆï¼Œé‡æ–°åˆ›å»ºæµè§ˆå™¨å®ä¾‹: {new_page_error}"
                        )
                        # å…³é—­æ— æ•ˆçš„æµè§ˆå™¨å®ä¾‹
                        try:
                            await browser.close()
                        except Exception:
                            pass

                        # åˆ›å»ºæ–°çš„æµè§ˆå™¨å®ä¾‹
                        if not playwright_instance:
                            playwright_instance = await async_playwright().start()
                        browser = await playwright_instance.chromium.launch(
                            headless=True,
                            timeout=20000,
                            args=[
                                "--no-sandbox",
                                "--disable-setuid-sandbox",
                                "--disable-dev-shm-usage",
                                "--disable-gpu",
                            ],
                        )

                        # ä½¿ç”¨æ–°åˆ›å»ºçš„æµè§ˆå™¨å®ä¾‹é‡æ–°å°è¯•æˆªå›¾
                        page = await browser.new_page(
                            viewport={"width": width, "height": height},
                            user_agent=self.user_agent,
                        )
                        await page.goto(
                            url, wait_until="domcontentloaded", timeout=60000
                        )
                        await page.wait_for_timeout(wait_time)
                        screenshot_bytes = await page.screenshot(
                            full_page=full_page,
                            quality=quality,
                            type=format,
                        )
                        await page.close()
                        logger.info("ä½¿ç”¨æ–°æµè§ˆå™¨å®ä¾‹æˆªå›¾æˆåŠŸ")

                        # ä¿å­˜æ–°åˆ›å»ºçš„æµè§ˆå™¨å®ä¾‹
                        self.browser = browser
                        return screenshot_bytes
                    else:
                        # æ–°åˆ›å»ºçš„æµè§ˆå™¨å®ä¾‹ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
                        raise
            finally:
                # å¦‚æœæœ‰ç›´æ¥åˆ›å»ºçš„playwrightå®ä¾‹ï¼Œç¡®ä¿å…³é—­
                if playwright_instance:
                    await playwright_instance.stop()
        except Exception as e:
            logger.error(f"æ•è·ç½‘é¡µæˆªå›¾å¤±è´¥: {url}, é”™è¯¯: {e}")
            raise ScreenshotError(f"æ•è·ç½‘é¡µæˆªå›¾å¤±è´¥: {url}, é”™è¯¯: {str(e)}") from e

    def extract_specific_content(
        self, html: str, url: str, extract_types: list[str]
    ) -> dict:
        """ä»HTMLä¸­æå–ç‰¹å®šç±»å‹çš„å†…å®¹

        æ ¹æ®æŒ‡å®šçš„æå–ç±»å‹ï¼Œä»HTMLæ–‡æ¡£ä¸­æå–ç»“æ„åŒ–æ•°æ®ï¼š
        - æ ‡é¢˜ï¼ˆtitleï¼‰
        - æ­£æ–‡å†…å®¹ï¼ˆcontentï¼‰
        - å›¾ç‰‡é“¾æ¥ï¼ˆimagesï¼‰
        - è¶…é“¾æ¥ï¼ˆlinksï¼‰
        - è¡¨æ ¼ï¼ˆtablesï¼‰
        - åˆ—è¡¨ï¼ˆlistsï¼‰
        - ä»£ç å—ï¼ˆcodeï¼‰
        - å…ƒä¿¡æ¯ï¼ˆmetaï¼‰
        - è§†é¢‘é“¾æ¥ï¼ˆvideosï¼‰
        - éŸ³é¢‘é“¾æ¥ï¼ˆaudiosï¼‰
        - å¼•ç”¨å—ï¼ˆquotesï¼‰
        - æ ‡é¢˜åˆ—è¡¨ï¼ˆheadingsï¼‰
        - æ®µè½ï¼ˆparagraphsï¼‰
        - æŒ‰é’®ï¼ˆbuttonsï¼‰
        - è¡¨å•ï¼ˆformsï¼‰

        Args:
            html: ç½‘é¡µçš„HTMLæ–‡æœ¬å†…å®¹
            url: ç½‘é¡µçš„åŸå§‹URLï¼Œç”¨äºå¤„ç†ç›¸å¯¹è·¯å¾„
            extract_types: è¦æå–çš„å†…å®¹ç±»å‹åˆ—è¡¨

        Returns:
            åŒ…å«æå–å†…å®¹çš„å­—å…¸ï¼Œé”®ä¸ºæå–ç±»å‹ï¼Œå€¼ä¸ºå¯¹åº”å†…å®¹
        """
        try:
            soup = BeautifulSoup(html, "lxml")
            extracted_content = {}

            # æå–æ ‡é¢˜
            if "title" in extract_types:
                title = soup.find("title")
                extracted_content["title"] = (
                    title.get_text().strip() if title else "æ— æ ‡é¢˜"
                )

            # æå–æ­£æ–‡å†…å®¹
            if "content" in extract_types:
                content_selectors = [
                    "article",  # è¯­ä¹‰åŒ–æ–‡ç« æ ‡ç­¾
                    "main",  # è¯­ä¹‰åŒ–ä¸»å†…å®¹æ ‡ç­¾
                    ".article-content",  # å¸¸è§æ–‡ç« å†…å®¹ç±»å
                    ".post-content",  # å¸¸è§åšå®¢å†…å®¹ç±»å
                    ".content",  # é€šç”¨å†…å®¹ç±»å
                    "body",  # å…œåº•æ–¹æ¡ˆ
                ]

                content_text = ""
                for selector in content_selectors:
                    element = soup.select_one(selector)
                    if element:
                        # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾ï¼Œé¿å…å¹²æ‰°å†…å®¹æå–
                        for script in element.find_all(["script", "style"]):
                            script.decompose()
                        text = element.get_text(separator="\n", strip=True)
                        if len(text) > len(content_text):
                            content_text = text

                # é™åˆ¶å†…å®¹é•¿åº¦
                if len(content_text) > self.max_content_length:
                    content_text = content_text[: self.max_content_length] + "..."

                extracted_content["content"] = content_text

            # æå–å›¾ç‰‡é“¾æ¥ï¼Œæœ€å¤šæå–10å¼ 
            if "images" in extract_types:
                images = []
                for img in soup.find_all("img"):
                    src = img.get("src")
                    if src:
                        # å¤„ç†ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹URL
                        full_url = urljoin(url, src)
                        alt_text = img.get("alt", "").strip()
                        images.append({"url": full_url, "alt": alt_text})
                extracted_content["images"] = images[:10]  # é™åˆ¶æœ€å¤š10å¼ å›¾ç‰‡

            # æå–è¶…é“¾æ¥ï¼Œæœ€å¤šæå–20ä¸ª
            if "links" in extract_types:
                links = []
                for a in soup.find_all("a", href=True):
                    href = a.get("href")
                    if href and not href.startswith("#"):  # è·³è¿‡é”šç‚¹é“¾æ¥
                        full_url = urljoin(url, href)
                        text = a.get_text().strip() or full_url  # é“¾æ¥æ–‡æœ¬ä¸ºç©ºæ—¶ä½¿ç”¨URL
                        links.append({"text": text, "url": full_url})
                extracted_content["links"] = links[:20]  # é™åˆ¶æœ€å¤š20ä¸ªé“¾æ¥

            # æå–è¡¨æ ¼ï¼Œæœ€å¤šæå–5ä¸ª
            if "tables" in extract_types:
                tables = []
                for table in soup.find_all("table"):
                    table_data = []
                    # æå–è¡¨å¤´
                    headers = []
                    thead = table.find("thead")
                    if thead:
                        for th in thead.find_all("th"):
                            headers.append(th.get_text().strip())

                    # æå–è¡¨ä½“
                    tbody = table.find("tbody") or table  # æ²¡æœ‰tbodyæ—¶ä½¿ç”¨tableæœ¬èº«
                    for row in tbody.find_all("tr"):
                        row_data = []
                        for cell in row.find_all(["td", "th"]):  # åŒæ—¶å¤„ç†tdå’Œth
                            row_data.append(cell.get_text().strip())
                        if row_data:  # è·³è¿‡ç©ºè¡Œ
                            table_data.append(row_data)

                    if table_data:  # åªæ·»åŠ æœ‰æ•°æ®çš„è¡¨æ ¼
                        tables.append({"headers": headers, "rows": table_data})
                extracted_content["tables"] = tables[:5]  # é™åˆ¶æœ€å¤š5ä¸ªè¡¨æ ¼

            # æå–åˆ—è¡¨ï¼Œæœ€å¤šæå–10ä¸ª
            if "lists" in extract_types:
                lists = []
                # æå–æ— åºåˆ—è¡¨
                for ul in soup.find_all("ul"):
                    list_items = []
                    for li in ul.find_all("li"):
                        list_items.append(li.get_text().strip())
                    if list_items:  # åªæ·»åŠ æœ‰å†…å®¹çš„åˆ—è¡¨
                        lists.append(
                            {
                                "type": "ul",  # åˆ—è¡¨ç±»å‹ï¼šæ— åºåˆ—è¡¨
                                "items": list_items[:20],  # æ¯ä¸ªåˆ—è¡¨æœ€å¤š20é¡¹
                            }
                        )

                # æå–æœ‰åºåˆ—è¡¨
                for ol in soup.find_all("ol"):
                    list_items = []
                    for li in ol.find_all("li"):
                        list_items.append(li.get_text().strip())
                    if list_items:  # åªæ·»åŠ æœ‰å†…å®¹çš„åˆ—è¡¨
                        lists.append(
                            {
                                "type": "ol",  # åˆ—è¡¨ç±»å‹ï¼šæœ‰åºåˆ—è¡¨
                                "items": list_items[:20],  # æ¯ä¸ªåˆ—è¡¨æœ€å¤š20é¡¹
                            }
                        )
                extracted_content["lists"] = lists[:10]  # é™åˆ¶æœ€å¤š10ä¸ªåˆ—è¡¨

            # æå–ä»£ç å—ï¼Œæœ€å¤šæå–5ä¸ª
            if "code" in extract_types:
                code_blocks = []
                for code in soup.find_all(["pre", "code"]):  # åŒæ—¶å¤„ç†preå’Œcodeæ ‡ç­¾
                    code_text = code.get_text().strip()
                    if code_text and len(code_text) > 10:  # è·³è¿‡çŸ­ä»£ç å—
                        # è·å–è¯­è¨€ç±»å‹
                        language = ""
                        if code.parent.name == "pre":
                            # æ£€æŸ¥preæ ‡ç­¾æ˜¯å¦æœ‰è¯­è¨€ç±»å
                            for cls in code.parent.get("class", []):
                                if cls.startswith("language-"):
                                    language = cls[9:]
                                    break
                                if cls.startswith("lang-"):
                                    language = cls[5:]
                                    break
                        elif code.get("class"):
                            # æ£€æŸ¥codeæ ‡ç­¾æ˜¯å¦æœ‰è¯­è¨€ç±»å
                            for cls in code.get("class", []):
                                if cls.startswith("language-"):
                                    language = cls[9:]
                                    break
                                if cls.startswith("lang-"):
                                    language = cls[5:]
                                    break
                        
                        # é™åˆ¶å•ä¸ªä»£ç å—é•¿åº¦
                        truncated_code = (
                            code_text[:1000] + "..."
                            if len(code_text) > 1000
                            else code_text
                        )
                        code_blocks.append({"code": truncated_code, "language": language})
                extracted_content["code_blocks"] = code_blocks[:5]  # é™åˆ¶æœ€å¤š5ä¸ªä»£ç å—

            # æå–å…ƒä¿¡æ¯
            if "meta" in extract_types:
                meta_info = {}
                # æå–æè¿°
                description = soup.find("meta", attrs={"name": "description"})
                if description:
                    meta_info["description"] = description.get("content", "").strip()

                # æå–å…³é”®è¯
                keywords = soup.find("meta", attrs={"name": "keywords"})
                if keywords:
                    meta_info["keywords"] = keywords.get("content", "").strip()

                # æå–ä½œè€…
                author = soup.find("meta", attrs={"name": "author"})
                if author:
                    meta_info["author"] = author.get("content", "").strip()

                # æå–å‘å¸ƒæ—¶é—´
                publish_time = soup.find(
                    "meta", attrs={"property": "article:published_time"}
                )
                if not publish_time:
                    publish_time = soup.find("meta", attrs={"name": "publish_date"})
                if publish_time:
                    meta_info["publish_time"] = publish_time.get("content", "").strip()

                # æå–ç½‘ç«™åç§°
                site_name = soup.find("meta", attrs={"property": "og:site_name"})
                if site_name:
                    meta_info["site_name"] = site_name.get("content", "").strip()

                # æå–og:title
                og_title = soup.find("meta", attrs={"property": "og:title"})
                if og_title:
                    meta_info["og_title"] = og_title.get("content", "").strip()

                # æå–og:description
                og_description = soup.find("meta", attrs={"property": "og:description"})
                if og_description:
                    meta_info["og_description"] = og_description.get("content", "").strip()

                extracted_content["meta"] = meta_info

            # æå–è§†é¢‘é“¾æ¥ï¼Œæœ€å¤šæå–5ä¸ª
            if "videos" in extract_types:
                videos = []
                # æŸ¥æ‰¾videoæ ‡ç­¾
                for video in soup.find_all("video"):
                    src = video.get("src")
                    if src:
                        full_url = urljoin(url, src)
                        videos.append({"url": full_url, "type": "video"})
                # æŸ¥æ‰¾iframeæ ‡ç­¾ï¼ˆå¯èƒ½åŒ…å«è§†é¢‘ï¼‰
                for iframe in soup.find_all("iframe"):
                    src = iframe.get("src")
                    if src:
                        full_url = urljoin(url, src)
                        videos.append({"url": full_url, "type": "iframe"})
                extracted_content["videos"] = videos[:5]  # é™åˆ¶æœ€å¤š5ä¸ªè§†é¢‘

            # æå–éŸ³é¢‘é“¾æ¥ï¼Œæœ€å¤šæå–5ä¸ª
            if "audios" in extract_types:
                audios = []
                # æŸ¥æ‰¾audioæ ‡ç­¾
                for audio in soup.find_all("audio"):
                    src = audio.get("src")
                    if src:
                        full_url = urljoin(url, src)
                        audios.append(full_url)
                # æŸ¥æ‰¾embedæ ‡ç­¾ï¼ˆå¯èƒ½åŒ…å«éŸ³é¢‘ï¼‰
                for embed in soup.find_all("embed"):
                    src = embed.get("src")
                    if src and (src.endswith(".mp3") or src.endswith(".wav") or src.endswith(".ogg")):
                        full_url = urljoin(url, src)
                        audios.append(full_url)
                extracted_content["audios"] = audios[:5]  # é™åˆ¶æœ€å¤š5ä¸ªéŸ³é¢‘

            # æå–å¼•ç”¨å—ï¼Œæœ€å¤šæå–10ä¸ª
            if "quotes" in extract_types:
                quotes = []
                # æŸ¥æ‰¾blockquoteæ ‡ç­¾
                for blockquote in soup.find_all("blockquote"):
                    quote_text = blockquote.get_text().strip()
                    if quote_text:
                        # æŸ¥æ‰¾å¼•ç”¨çš„ä½œè€…
                        cite = blockquote.find("cite")
                        author = cite.get_text().strip() if cite else ""
                        quotes.append({"text": quote_text, "author": author})
                extracted_content["quotes"] = quotes[:10]  # é™åˆ¶æœ€å¤š10ä¸ªå¼•ç”¨å—

            # æå–æ ‡é¢˜åˆ—è¡¨
            if "headings" in extract_types:
                headings = []
                # æŸ¥æ‰¾æ‰€æœ‰h1-h6æ ‡ç­¾
                for level in range(1, 7):
                    for heading in soup.find_all(f"h{level}"):
                        headings.append({
                            "level": level,
                            "text": heading.get_text().strip(),
                            "id": heading.get("id", "")
                        })
                extracted_content["headings"] = headings

            # æå–æ®µè½ï¼Œæœ€å¤šæå–20ä¸ª
            if "paragraphs" in extract_types:
                paragraphs = []
                for p in soup.find_all("p"):
                    text = p.get_text().strip()
                    if text:
                        paragraphs.append(text)
                extracted_content["paragraphs"] = paragraphs[:20]  # é™åˆ¶æœ€å¤š20ä¸ªæ®µè½

            # æå–æŒ‰é’®ï¼Œæœ€å¤šæå–10ä¸ª
            if "buttons" in extract_types:
                buttons = []
                for button in soup.find_all("button"):
                    text = button.get_text().strip()
                    onclick = button.get("onclick", "").strip()
                    buttons.append({
                        "text": text,
                        "onclick": onclick,
                        "type": button.get("type", "button")
                    })
                extracted_content["buttons"] = buttons[:10]  # é™åˆ¶æœ€å¤š10ä¸ªæŒ‰é’®

            # æå–è¡¨å•ï¼Œæœ€å¤šæå–5ä¸ª
            if "forms" in extract_types:
                forms = []
                for form in soup.find_all("form"):
                    form_data = {
                        "action": form.get("action", ""),
                        "method": form.get("method", "get"),
                        "inputs": [],
                        "buttons": []
                    }
                    # æå–è¡¨å•è¾“å…¥
                    for input_elem in form.find_all("input"):
                        form_data["inputs"].append({
                            "type": input_elem.get("type", "text"),
                            "name": input_elem.get("name", ""),
                            "value": input_elem.get("value", "")
                        })
                    # æå–è¡¨å•æ–‡æœ¬åŸŸ
                    for textarea in form.find_all("textarea"):
                        form_data["inputs"].append({
                            "type": "textarea",
                            "name": textarea.get("name", ""),
                            "value": textarea.get_text().strip()
                        })
                    # æå–è¡¨å•é€‰æ‹©
                    for select in form.find_all("select"):
                        options = []
                        for option in select.find_all("option"):
                            options.append({
                                "value": option.get("value", ""),
                                "text": option.get_text().strip(),
                                "selected": bool(option.get("selected"))
                            })
                        form_data["inputs"].append({
                            "type": "select",
                            "name": select.get("name", ""),
                            "options": options
                        })
                    # æå–è¡¨å•æŒ‰é’®
                    for button in form.find_all("button"):
                        form_data["buttons"].append({
                            "text": button.get_text().strip(),
                            "type": button.get("type", "submit")
                        })
                    forms.append(form_data)
                extracted_content["forms"] = forms[:5]  # é™åˆ¶æœ€å¤š5ä¸ªè¡¨å•

            return extracted_content
        except Exception as e:
            logger.error(f"æå–ç‰¹å®šå†…å®¹å¤±è´¥: {e}")
            raise ContentExtractionError(f"æå–ç‰¹å®šå†…å®¹å¤±è´¥: {str(e)}") from e
