"""
AstrBot ç½‘é¡µåˆ†ææ’ä»¶
è‡ªåŠ¨è¯†åˆ«ç”¨æˆ·å‘é€çš„ç½‘é¡µé“¾æ¥ï¼ŒæŠ“å–å†…å®¹å¹¶è°ƒç”¨LLMè¿›è¡Œåˆ†æå’Œæ€»ç»“
"""

from typing import List

from astrbot.api import AstrBotConfig
from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.star import Context, Star, register
from astrbot.api import logger

from .analyzer import WebAnalyzer
from .cache import CacheManager


@register("astrbot_plugin_web_analyzer", "Sakura520222", "è‡ªåŠ¨è¯†åˆ«ç½‘é¡µé“¾æ¥å¹¶è¿›è¡Œå†…å®¹åˆ†æå’Œæ€»ç»“", "1.2.2", "https://github.com/Sakura520222/astrbot_plugin_web_analyzer")
class WebAnalyzerPlugin(Star):
    """ç½‘é¡µåˆ†ææ’ä»¶ä¸»ç±»"""
    
    def __init__(self, context: Context, config: AstrBotConfig):
        super().__init__(context)
        self.config = config
        
        # ä»é…ç½®è·å–å‚æ•°å¹¶è¿›è¡ŒéªŒè¯
        # åŸºæœ¬é…ç½®éªŒè¯
        self.max_content_length = max(1000, config.get('max_content_length', 10000))  # è‡³å°‘1000å­—ç¬¦
        self.timeout = max(5, min(300, config.get('request_timeout', 30)))  # 5-300ç§’
        self.retry_count = max(0, min(10, config.get('retry_count', 3)))  # 0-10æ¬¡
        self.retry_delay = max(0, min(10, config.get('retry_delay', 2)))  # 0-10ç§’
        self.llm_enabled = bool(config.get('llm_enabled', True))
        self.auto_analyze = bool(config.get('auto_analyze', True))
        self.user_agent = config.get('user_agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        self.proxy = config.get('proxy', '')
        
        # éªŒè¯ä»£ç†æ ¼å¼ï¼ˆå¦‚æœæä¾›äº†ä»£ç†ï¼‰
        if self.proxy:
            try:
                from urllib.parse import urlparse
                parsed = urlparse(self.proxy)
                if not all([parsed.scheme, parsed.netloc]):
                    logger.warning(f"æ— æ•ˆçš„ä»£ç†æ ¼å¼: {self.proxy}ï¼Œå°†å¿½ç•¥ä»£ç†è®¾ç½®")
                    self.proxy = ''
            except Exception as e:
                logger.warning(f"è§£æä»£ç†å¤±è´¥: {self.proxy}ï¼Œå°†å¿½ç•¥ä»£ç†è®¾ç½®ï¼Œé”™è¯¯: {e}")
                self.proxy = ''
        
        self.allowed_domains = self._parse_domain_list(config.get('allowed_domains', ''))
        self.blocked_domains = self._parse_domain_list(config.get('blocked_domains', ''))
        
        # åˆ†æè®¾ç½®éªŒè¯
        analysis_settings = config.get('analysis_settings', {})
        self.enable_emoji = bool(analysis_settings.get('enable_emoji', True))
        self.enable_statistics = bool(analysis_settings.get('enable_statistics', True))
        self.max_summary_length = max(500, min(10000, analysis_settings.get('max_summary_length', 2000)))  # 500-10000å­—ç¬¦
        
        # æˆªå›¾è®¾ç½®éªŒè¯
        self.enable_screenshot = bool(analysis_settings.get('enable_screenshot', True))
        self.screenshot_quality = max(10, min(100, analysis_settings.get('screenshot_quality', 80)))  # 10-100
        self.screenshot_width = max(320, min(4096, analysis_settings.get('screenshot_width', 1280)))  # 320-4096px
        self.screenshot_height = max(240, min(4096, analysis_settings.get('screenshot_height', 720)))  # 240-4096px
        self.screenshot_full_page = bool(analysis_settings.get('screenshot_full_page', False))
        self.screenshot_wait_time = max(0, min(10000, analysis_settings.get('screenshot_wait_time', 2000)))  # 0-10000ms
        
        # éªŒè¯æˆªå›¾æ ¼å¼
        screenshot_format = analysis_settings.get('screenshot_format', 'jpeg').lower()
        if screenshot_format not in ['jpeg', 'png']:
            logger.warning(f"æ— æ•ˆçš„æˆªå›¾æ ¼å¼: {screenshot_format}ï¼Œå°†ä½¿ç”¨é»˜è®¤æ ¼å¼ jpeg")
            self.screenshot_format = 'jpeg'
        else:
            self.screenshot_format = screenshot_format
        
        # LLMæä¾›å•†é…ç½®
        self.llm_provider = config.get('llm_provider', '')
        
        # ç¾¤èŠé»‘åå•é…ç½®
        group_blacklist_text = config.get('group_blacklist', '')
        self.group_blacklist = self._parse_group_list(group_blacklist_text)
        
        # åˆå¹¶è½¬å‘é…ç½®
        self.merge_forward_enabled = bool(config.get('merge_forward_enabled', False))  # æ˜¯å¦å¯ç”¨åˆå¹¶è½¬å‘
        
        # è‡ªå®šä¹‰æç¤ºè¯é…ç½®
        self.custom_prompt = config.get('custom_prompt', '')  # è‡ªå®šä¹‰åˆ†ææç¤ºè¯
        
        # ç¿»è¯‘è®¾ç½®éªŒè¯
        translation_settings = config.get('translation_settings', {})
        self.enable_translation = bool(translation_settings.get('enable_translation', False))
        
        # éªŒè¯ç›®æ ‡è¯­è¨€
        self.target_language = translation_settings.get('target_language', 'zh').lower()
        valid_languages = ['zh', 'en', 'ja', 'ko', 'fr', 'de', 'es', 'ru', 'ar', 'pt']
        if self.target_language not in valid_languages:
            logger.warning(f"æ— æ•ˆçš„ç›®æ ‡è¯­è¨€: {self.target_language}ï¼Œå°†ä½¿ç”¨é»˜è®¤è¯­è¨€ zh")
            self.target_language = 'zh'
        
        self.translation_provider = translation_settings.get('translation_provider', 'llm')
        self.custom_translation_prompt = translation_settings.get('custom_translation_prompt', '')
        
        # ç¼“å­˜è®¾ç½®éªŒè¯
        cache_settings = config.get('cache_settings', {})
        self.enable_cache = bool(cache_settings.get('enable_cache', True))
        self.cache_expire_time = max(5, min(10080, cache_settings.get('cache_expire_time', 1440)))  # 5-10080åˆ†é’Ÿï¼ˆ7å¤©ï¼‰
        self.max_cache_size = max(10, min(1000, cache_settings.get('max_cache_size', 100)))  # 10-1000ä¸ª
        
        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        self.cache_manager = CacheManager(
            max_size=self.max_cache_size,
            expire_time=self.cache_expire_time
        )
        
        # å†…å®¹æå–è®¾ç½®éªŒè¯
        content_extraction_settings = config.get('content_extraction_settings', {})
        self.enable_specific_extraction = bool(content_extraction_settings.get('enable_specific_extraction', False))
        extract_types_text = content_extraction_settings.get('extract_types', 'title\ncontent')
        self.extract_types = [t.strip() for t in extract_types_text.split('\n') if t.strip()]
        
        # éªŒè¯æå–ç±»å‹
        valid_extract_types = ['title', 'content', 'images', 'links', 'tables', 'lists', 'code', 'meta']
        invalid_types = [t for t in self.extract_types if t not in valid_extract_types]
        if invalid_types:
            logger.warning(f"æ— æ•ˆçš„æå–ç±»å‹: {', '.join(invalid_types)}ï¼Œå°†å¿½ç•¥è¿™äº›ç±»å‹")
            self.extract_types = [t for t in self.extract_types if t in valid_extract_types]
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæå–ç±»å‹
        if not self.extract_types:
            self.extract_types = ['title', 'content']
        
        # æ·»åŠ metaç±»å‹åˆ°æå–ç±»å‹ä¸­ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
        if 'meta' not in self.extract_types:
            self.extract_types.append('meta')
        
        # åˆå§‹åŒ–åˆ†æå™¨
        self.analyzer = WebAnalyzer(
            max_content_length=self.max_content_length, 
            timeout=self.timeout, 
            user_agent=self.user_agent, 
            proxy=self.proxy, 
            retry_count=self.retry_count, 
            retry_delay=self.retry_delay
        )
        
        # è®°å½•é…ç½®éªŒè¯å®Œæˆ
        logger.info("æ’ä»¶é…ç½®éªŒè¯å®Œæˆ")
    
    def _parse_domain_list(self, domain_text: str) -> List[str]:
        """è§£æåŸŸååˆ—è¡¨æ–‡æœ¬ä¸ºåˆ—è¡¨"""
        if not domain_text:
            return []
        domains = [domain.strip() for domain in domain_text.split('\n') if domain.strip()]
        return domains
    
    def _parse_group_list(self, group_text: str) -> List[str]:
        """è§£æç¾¤èŠåˆ—è¡¨æ–‡æœ¬ä¸ºåˆ—è¡¨"""
        if not group_text:
            return []
        groups = [group.strip() for group in group_text.split('\n') if group.strip()]
        return groups
    
    def _is_group_blacklisted(self, group_id: str) -> bool:
        """æ£€æŸ¥ç¾¤èŠæ˜¯å¦åœ¨é»‘åå•ä¸­"""
        if not group_id or not self.group_blacklist:
            return False
        return group_id in self.group_blacklist
    
    def _is_domain_allowed(self, url: str) -> bool:
        """æ£€æŸ¥åŸŸåæ˜¯å¦å…è®¸è®¿é—®"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # æ£€æŸ¥æ˜¯å¦åœ¨ç¦æ­¢åˆ—è¡¨ä¸­
            if self.blocked_domains:
                for blocked_domain in self.blocked_domains:
                    if blocked_domain.lower() in domain:
                        return False
            
            # æ£€æŸ¥æ˜¯å¦åœ¨å…è®¸åˆ—è¡¨ä¸­ï¼ˆå¦‚æœå…è®¸åˆ—è¡¨ä¸ä¸ºç©ºï¼‰
            if self.allowed_domains:
                for allowed_domain in self.allowed_domains:
                    if allowed_domain.lower() in domain:
                        return True
                return False  # å¦‚æœå…è®¸åˆ—è¡¨ä¸ä¸ºç©ºä½†åŸŸåä¸åœ¨å…¶ä¸­ï¼Œåˆ™ç¦æ­¢
            
            return True  # å¦‚æœå…è®¸åˆ—è¡¨ä¸ºç©ºï¼Œåˆ™å…è®¸æ‰€æœ‰åŸŸå
        except Exception:
            return False
    
    @filter.command("ç½‘é¡µåˆ†æ", alias={'åˆ†æ', 'æ€»ç»“', 'web', 'analyze'})
    async def analyze_webpage(self, event: AstrMessageEvent):
        """æ‰‹åŠ¨åˆ†ææŒ‡å®šç½‘é¡µé“¾æ¥"""
        message_text = event.message_str
        
        # æå–æ‰€æœ‰URL
        urls = self.analyzer.extract_urls(message_text)
        if not urls:
            yield event.plain_result("è¯·æä¾›è¦åˆ†æçš„ç½‘é¡µé“¾æ¥ï¼Œä¾‹å¦‚ï¼š/ç½‘é¡µåˆ†æ https://example.com")
            return
        
        # éªŒè¯URLå¹¶è¿‡æ»¤æ‰ä¸å…è®¸è®¿é—®çš„åŸŸå
        valid_urls = [url for url in urls if self.analyzer.is_valid_url(url)]
        if not valid_urls:
            yield event.plain_result("æ— æ•ˆçš„URLé“¾æ¥ï¼Œè¯·æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®")
            return
        
        allowed_urls = [url for url in valid_urls if self._is_domain_allowed(url)]
        if not allowed_urls:
            yield event.plain_result("æ‰€æœ‰åŸŸåéƒ½ä¸åœ¨å…è®¸è®¿é—®çš„åˆ—è¡¨ä¸­ï¼Œæˆ–å·²è¢«ç¦æ­¢è®¿é—®")
            return
        
        # å‘é€å¤„ç†æç¤º
        if len(allowed_urls) == 1:
            yield event.plain_result(f"æ­£åœ¨åˆ†æç½‘é¡µ: {allowed_urls[0]}")
        else:
            yield event.plain_result(f"æ­£åœ¨åˆ†æ{len(allowed_urls)}ä¸ªç½‘é¡µé“¾æ¥...")
        
        # æ‰¹é‡å¤„ç†æ‰€æœ‰URL
        async for result in self._batch_process_urls(event, allowed_urls):
            yield result
    
    @filter.event_message_type(filter.EventMessageType.ALL)
    async def auto_detect_urls(self, event: AstrMessageEvent):
        """è‡ªåŠ¨æ£€æµ‹æ¶ˆæ¯ä¸­çš„URLé“¾æ¥å¹¶è¿›è¡Œåˆ†æ"""
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨åˆ†æ
        if not self.auto_analyze:
            logger.info("è‡ªåŠ¨åˆ†æåŠŸèƒ½å·²ç¦ç”¨")
            return
        
        # æ£€æŸ¥ç¾¤èŠæ˜¯å¦åœ¨é»‘åå•ä¸­ï¼ˆä»…ç¾¤èŠæ¶ˆæ¯ï¼‰
        # å°è¯•ä»ä¸åŒä½ç½®è·å–ç¾¤èŠID
        group_id = None
        
        # æ–¹æ³•1ï¼šä»äº‹ä»¶å¯¹è±¡ç›´æ¥è·å–
        if hasattr(event, 'group_id') and event.group_id:
            group_id = event.group_id
        # æ–¹æ³•2ï¼šä»æ¶ˆæ¯å¯¹è±¡è·å–
        elif hasattr(event, 'message_obj') and hasattr(event.message_obj, 'group_id') and event.message_obj.group_id:
            group_id = event.message_obj.group_id
        # æ–¹æ³•3ï¼šä»åŸå§‹æ¶ˆæ¯è·å–
        elif hasattr(event, 'raw_message') and hasattr(event.raw_message, 'group_id') and event.raw_message.group_id:
            group_id = event.raw_message.group_id
        
        # ç¾¤èŠåœ¨é»‘åå•ä¸­æ—¶é™é»˜å¿½ç•¥
        if group_id and self._is_group_blacklisted(group_id):
            return  # ç¾¤èŠåœ¨é»‘åå•ä¸­ï¼Œé™é»˜å¿½ç•¥
            
        message_text = event.message_str
        
        # æå–URL
        urls = self.analyzer.extract_urls(message_text)
        if not urls:
            return  # æ²¡æœ‰URLï¼Œä¸å¤„ç†
        
        valid_urls = [url for url in urls if self.analyzer.is_valid_url(url)]
        if not valid_urls:
            return
        
        # è¿‡æ»¤æ‰ä¸å…è®¸è®¿é—®çš„åŸŸå
        allowed_urls = [url for url in valid_urls if self._is_domain_allowed(url)]
        if not allowed_urls:
            return  # æ²¡æœ‰å…è®¸è®¿é—®çš„URLï¼Œä¸å¤„ç†
        
        # å‘é€å¤„ç†æç¤º
        if len(allowed_urls) == 1:
            yield event.plain_result(f"æ£€æµ‹åˆ°ç½‘é¡µé“¾æ¥ï¼Œæ­£åœ¨åˆ†æ: {allowed_urls[0]}")
        else:
            yield event.plain_result(f"æ£€æµ‹åˆ°{len(allowed_urls)}ä¸ªç½‘é¡µé“¾æ¥ï¼Œæ­£åœ¨åˆ†æ...")
        
        # æ‰¹é‡å¤„ç†æ‰€æœ‰URL
        async for result in self._batch_process_urls(event, allowed_urls):
            yield result
    
    async def _process_single_url(self, event: AstrMessageEvent, url: str, analyzer: WebAnalyzer) -> dict:
        """å¤„ç†å•ä¸ªURLï¼Œè¿”å›åˆ†æç»“æœ"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            cached_result = self._check_cache(url)
            if cached_result:
                logger.info(f"ä½¿ç”¨ç¼“å­˜ç»“æœ: {url}")
                return cached_result
            
            # æŠ“å–ç½‘é¡µå†…å®¹
            html = await analyzer.fetch_webpage(url)
            if not html:
                return {
                    'url': url,
                    'result': f"âŒ æ— æ³•æŠ“å–ç½‘é¡µå†…å®¹: {url}",
                    'screenshot': None
                }
            
            # æå–å†…å®¹
            content_data = analyzer.extract_content(html, url)
            if not content_data:
                return {
                    'url': url,
                    'result': f"âŒ æ— æ³•è§£æç½‘é¡µå†…å®¹: {url}",
                    'screenshot': None
                }
            
            # ç¿»è¯‘å†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.enable_translation:
                translated_content = await self._translate_content(event, content_data['content'])
                # åˆ›å»ºç¿»è¯‘åçš„å†…å®¹æ•°æ®å‰¯æœ¬
                translated_content_data = content_data.copy()
                translated_content_data['content'] = translated_content
                # è°ƒç”¨LLMè¿›è¡Œåˆ†æï¼ˆä½¿ç”¨ç¿»è¯‘åçš„å†…å®¹ï¼‰
                analysis_result = await self.analyze_with_llm(event, translated_content_data)
            else:
                # ç›´æ¥è°ƒç”¨LLMè¿›è¡Œåˆ†æ
                analysis_result = await self.analyze_with_llm(event, content_data)
            
            # æå–ç‰¹å®šç±»å‹å†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            specific_content = self._extract_specific_content(html, url)
            if specific_content:
                # åœ¨åˆ†æç»“æœä¸­æ·»åŠ ç‰¹å®šå†…å®¹
                specific_content_str = "\n\n**ç‰¹å®šå†…å®¹æå–**\n"
                
                if 'images' in specific_content and specific_content['images']:
                    specific_content_str += f"\nğŸ“· å›¾ç‰‡é“¾æ¥ ({len(specific_content['images'])}):\n"
                    for img_url in specific_content['images']:
                        specific_content_str += f"- {img_url}\n"
                
                if 'links' in specific_content and specific_content['links']:
                    specific_content_str += f"\nğŸ”— ç›¸å…³é“¾æ¥ ({len(specific_content['links'])}):\n"
                    for link in specific_content['links'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé“¾æ¥
                        specific_content_str += f"- [{link['text']}]({link['url']})\n"
                
                if 'code_blocks' in specific_content and specific_content['code_blocks']:
                    specific_content_str += f"\nğŸ’» ä»£ç å— ({len(specific_content['code_blocks'])}):\n"
                    for i, code in enumerate(specific_content['code_blocks'][:2]):  # åªæ˜¾ç¤ºå‰2ä¸ªä»£ç å—
                        specific_content_str += f"```\n{code}\n```\n"
                
                # æ·»åŠ å…ƒä¿¡æ¯
                if 'meta' in specific_content and specific_content['meta']:
                    meta_info = specific_content['meta']
                    specific_content_str += "\nğŸ“‹ å…ƒä¿¡æ¯:\n"
                    for key, value in meta_info.items():
                        if value:
                            specific_content_str += f"- {key}: {value}\n"
                
                # æ·»åŠ åˆ°åˆ†æç»“æœä¸­
                analysis_result += specific_content_str
            
            # æ•è·æˆªå›¾
            screenshot = None
            if self.enable_screenshot:
                screenshot = await analyzer.capture_screenshot(
                    url,
                    quality=self.screenshot_quality,
                    width=self.screenshot_width,
                    height=self.screenshot_height,
                    full_page=self.screenshot_full_page,
                    wait_time=self.screenshot_wait_time,
                    format=self.screenshot_format
                )
            
            # å‡†å¤‡ç»“æœæ•°æ®
            result_data = {
                'url': url,
                'result': analysis_result,
                'screenshot': screenshot
            }
            
            # æ›´æ–°ç¼“å­˜
            self._update_cache(url, result_data)
            
            return result_data
        except Exception as e:
            logger.error(f"å¤„ç†URL {url} æ—¶å‡ºé”™: {e}")
            return {
                'url': url,
                'result': f"âŒ å¤„ç†URLæ—¶å‡ºé”™: {url}\né”™è¯¯ä¿¡æ¯: {str(e)}",
                'screenshot': None
            }
    
    async def _batch_process_urls(self, event: AstrMessageEvent, urls: List[str]):
        """æ‰¹é‡å¤„ç†å¤šä¸ªURLï¼Œæ”¶é›†åˆ†æç»“æœå¹¶å‘é€"""
        # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
        analysis_results = []
        
        async with WebAnalyzer(self.max_content_length, self.timeout, self.user_agent, self.proxy, self.retry_count, self.retry_delay) as analyzer:
            # ä½¿ç”¨asyncio.gatherå¹¶å‘å¤„ç†å¤šä¸ªURL
            import asyncio
            # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
            tasks = [self._process_single_url(event, url, analyzer) for url in urls]
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            analysis_results = await asyncio.gather(*tasks)
        
        # å‘é€æ‰€æœ‰åˆ†æç»“æœ
        async for result in self._send_analysis_result(event, analysis_results):
            yield result
    
    async def analyze_with_llm(self, event: AstrMessageEvent, content_data: dict) -> str:
        """è°ƒç”¨LLMè¿›è¡Œå†…å®¹åˆ†æå’Œæ€»ç»“"""
        try:
            title = content_data['title']
            content = content_data['content']
            url = content_data['url']
            
            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨å’Œå¯ç”¨
            if not hasattr(self.context, 'llm_generate') or not self.llm_enabled:
                return self.get_enhanced_analysis(content_data)
            
            # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„LLMæä¾›å•†ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨å½“å‰ä¼šè¯çš„æ¨¡å‹
            provider_id = self.llm_provider
            if not provider_id:
                umo = event.unified_msg_origin
                provider_id = await self.context.get_current_chat_provider_id(umo=umo)
            
            if not provider_id:
                return self.get_enhanced_analysis(content_data)
            
            # æ„å»ºä¼˜åŒ–çš„LLMæç¤ºè¯
            emoji_prefix = "æ¯ä¸ªè¦ç‚¹ç”¨emojiå›¾æ ‡æ ‡è®°" if self.enable_emoji else ""
            
            # ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯æˆ–é»˜è®¤æç¤ºè¯
            if self.custom_prompt:
                # æ›¿æ¢è‡ªå®šä¹‰æç¤ºè¯ä¸­çš„å˜é‡
                prompt = self.custom_prompt.format(
                    title=title,
                    url=url,
                    content=content,
                    max_length=self.max_summary_length
                )
            else:
                # é»˜è®¤æç¤ºè¯
                prompt = f"""è¯·å¯¹ä»¥ä¸‹ç½‘é¡µå†…å®¹è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{title}
- é“¾æ¥ï¼š{url}

**ç½‘é¡µå†…å®¹**ï¼š
{content}

**åˆ†æè¦æ±‚**ï¼š
1. **æ ¸å¿ƒæ‘˜è¦**ï¼šç”¨50-100å­—æ¦‚æ‹¬ç½‘é¡µçš„æ ¸å¿ƒå†…å®¹å’Œä¸»æ—¨
2. **å…³é”®è¦ç‚¹**ï¼šæå–2-3ä¸ªæœ€é‡è¦çš„ä¿¡æ¯ç‚¹æˆ–è§‚ç‚¹
3. **å†…å®¹ç±»å‹**ï¼šåˆ¤æ–­ç½‘é¡µå±äºä»€ä¹ˆç±»å‹ï¼ˆæ–°é—»ã€æ•™ç¨‹ã€åšå®¢ã€äº§å“ä»‹ç»ç­‰ï¼‰
4. **ä»·å€¼è¯„ä¼°**ï¼šç®€è¦è¯„ä»·å†…å®¹çš„ä»·å€¼å’Œå®ç”¨æ€§
5. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜é€‚åˆå“ªäº›äººç¾¤é˜…è¯»

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{self.max_summary_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚"""
            
            # ä½¿ç”¨å½“å‰ä¼šè¯çš„èŠå¤©æ¨¡å‹IDè°ƒç”¨å¤§æ¨¡å‹
            llm_resp = await self.context.llm_generate(
                chat_provider_id=provider_id,  # ä½¿ç”¨å½“å‰ä¼šè¯çš„èŠå¤©æ¨¡å‹
                prompt=prompt
            )
            
            if llm_resp and llm_resp.completion_text:
                # ç¾åŒ–LLMè¿”å›çš„ç»“æœ
                analysis_text = llm_resp.completion_text.strip()
                
                # é™åˆ¶æ‘˜è¦é•¿åº¦
                if len(analysis_text) > self.max_summary_length:
                    analysis_text = analysis_text[:self.max_summary_length] + "..."
                
                # æ·»åŠ æ ‡é¢˜å’Œæ ¼å¼ç¾åŒ–
                link_emoji = "ğŸ”—" if self.enable_emoji else ""
                title_emoji = "ğŸ“" if self.enable_emoji else ""
                
                formatted_result = "**AIæ™ºèƒ½ç½‘é¡µåˆ†ææŠ¥å‘Š**\n\n"
                formatted_result += f"{link_emoji} **åˆ†æé“¾æ¥**: {url}\n"
                formatted_result += f"{title_emoji} **ç½‘é¡µæ ‡é¢˜**: {title}\n\n"
                formatted_result += "---\n\n"
                formatted_result += analysis_text
                formatted_result += "\n\n---\n"
                formatted_result += "*åˆ†æå®Œæˆï¼Œå¸Œæœ›å¯¹æ‚¨æœ‰å¸®åŠ©ï¼*"
                
                return formatted_result
            else:
                return self.get_enhanced_analysis(content_data)
                
        except Exception as e:
            logger.error(f"LLMåˆ†æå¤±è´¥: {e}")
            # å¦‚æœLLMåˆ†æå¤±è´¥ï¼Œç›´æ¥è¿”å›é”™è¯¯ä¿¡æ¯
            return f"âŒ LLMåˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
    
    def get_enhanced_analysis(self, content_data: dict) -> str:
        """å¢å¼ºç‰ˆåŸºç¡€åˆ†æï¼ˆLLMä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰"""
        title = content_data['title']
        content = content_data['content']
        url = content_data['url']
        
        # è¯¦ç»†çš„å†…å®¹ç»Ÿè®¡
        char_count = len(content)
        word_count = len(content.split())
        
        # æ™ºèƒ½å†…å®¹ç±»å‹æ£€æµ‹
        content_lower = content.lower()
        content_type = "æ–‡ç« "
        if any(keyword in content_lower for keyword in ['æ–°é—»', 'æŠ¥é“', 'æ¶ˆæ¯', 'æ—¶äº‹']):
            content_type = "æ–°é—»èµ„è®¯"
        elif any(keyword in content_lower for keyword in ['æ•™ç¨‹', 'æŒ‡å—', 'æ•™å­¦', 'æ­¥éª¤', 'æ–¹æ³•']):
            content_type = "æ•™ç¨‹æŒ‡å—"
        elif any(keyword in content_lower for keyword in ['åšå®¢', 'éšç¬”', 'æ—¥è®°', 'ä¸ªäºº', 'è§‚ç‚¹']):
            content_type = "ä¸ªäººåšå®¢"
        elif any(keyword in content_lower for keyword in ['äº§å“', 'æœåŠ¡', 'è´­ä¹°', 'ä»·æ ¼', 'ä¼˜æƒ ']):
            content_type = "äº§å“ä»‹ç»"
        elif any(keyword in content_lower for keyword in ['æŠ€æœ¯', 'å¼€å‘', 'ç¼–ç¨‹', 'ä»£ç ', 'API']):
            content_type = "æŠ€æœ¯æ–‡æ¡£"
        
        # æå–å…³é”®å¥å­ï¼ˆå‰3ä¸ªéç©ºæ®µè½ï¼‰
        paragraphs = [p.strip() for p in content.split('\n') if p.strip()]
        key_sentences = paragraphs[:3]
        
        # æ£€æµ‹å†…å®¹è´¨é‡
        quality_indicator = "å†…å®¹ä¸°å¯Œ" if char_count > 1000 else "å†…å®¹ç®€æ´"
        if char_count > 5000:
            quality_indicator = "å†…å®¹è¯¦å®"
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨emoji
        robot_emoji = "ğŸ¤–" if self.enable_emoji else ""
        page_emoji = "ğŸ“„" if self.enable_emoji else ""
        info_emoji = "ğŸ“" if self.enable_emoji else ""
        stats_emoji = "ğŸ“Š" if self.enable_emoji else ""
        search_emoji = "ğŸ”" if self.enable_emoji else ""
        light_emoji = "ğŸ’¡" if self.enable_emoji else ""
        
        result = f"{robot_emoji} **æ™ºèƒ½ç½‘é¡µåˆ†æ** {page_emoji}\n\n"
        
        if self.enable_emoji:
            result += f"**{info_emoji} åŸºæœ¬ä¿¡æ¯**\n"
        else:
            result += "**åŸºæœ¬ä¿¡æ¯**\n"
        result += f"- **æ ‡é¢˜**: {title}\n"
        result += f"- **é“¾æ¥**: {url}\n"
        result += f"- **å†…å®¹ç±»å‹**: {content_type}\n"
        result += f"- **è´¨é‡è¯„ä¼°**: {quality_indicator}\n\n"
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if self.enable_statistics:
            if self.enable_emoji:
                result += f"**{stats_emoji} å†…å®¹ç»Ÿè®¡**\n"
            else:
                result += "**å†…å®¹ç»Ÿè®¡**\n"
            result += f"- å­—ç¬¦æ•°: {char_count:,}\n"
            result += f"- æ®µè½æ•°: {len(paragraphs)}\n"
            result += f"- è¯æ•°: {word_count:,}\n\n"
        
        if self.enable_emoji:
            result += f"**{search_emoji} å†…å®¹æ‘˜è¦**\n"
        else:
            result += "**å†…å®¹æ‘˜è¦**\n"
        result += f"{chr(10).join(['â€¢ ' + sentence[:100] + ('...' if len(sentence) > 100 else '') for sentence in key_sentences])}\n\n"
        
        if self.enable_emoji:
            result += f"**{light_emoji} åˆ†æè¯´æ˜**\n"
        else:
            result += "**åˆ†æè¯´æ˜**\n"
        result += "æ­¤åˆ†æåŸºäºç½‘é¡µå†…å®¹æå–ï¼Œå¦‚éœ€æ›´æ·±å…¥çš„AIæ™ºèƒ½åˆ†æï¼Œè¯·ç¡®ä¿AstrBotå·²æ­£ç¡®é…ç½®LLMåŠŸèƒ½ã€‚\n\n"
        result += "*æç¤ºï¼šå®Œæ•´å†…å®¹é¢„è§ˆè¯·æŸ¥çœ‹åŸå§‹ç½‘é¡µ*"
        
        return result
    
    @filter.command("web_config", alias={'ç½‘é¡µåˆ†æé…ç½®', 'ç½‘é¡µåˆ†æè®¾ç½®'})
    async def show_config(self, event: AstrMessageEvent):
        """æ˜¾ç¤ºå½“å‰æ’ä»¶é…ç½®"""
        config_info = f"""**ç½‘é¡µåˆ†ææ’ä»¶é…ç½®ä¿¡æ¯**

**åŸºæœ¬è®¾ç½®**
- æœ€å¤§å†…å®¹é•¿åº¦: {self.max_content_length} å­—ç¬¦
- è¯·æ±‚è¶…æ—¶æ—¶é—´: {self.timeout} ç§’
- LLMæ™ºèƒ½åˆ†æ: {'âœ… å·²å¯ç”¨' if self.llm_enabled else 'âŒ å·²ç¦ç”¨'}
- è‡ªåŠ¨åˆ†æé“¾æ¥: {'âœ… å·²å¯ç”¨' if self.auto_analyze else 'âŒ å·²ç¦ç”¨'}
- åˆå¹¶è½¬å‘åŠŸèƒ½: {'âœ… å·²å¯ç”¨' if self.merge_forward_enabled else 'âŒ å·²ç¦ç”¨'}

**åŸŸåæ§åˆ¶**
- å…è®¸åŸŸå: {len(self.allowed_domains)} ä¸ª
- ç¦æ­¢åŸŸå: {len(self.blocked_domains)} ä¸ª

**ç¾¤èŠæ§åˆ¶**
- ç¾¤èŠé»‘åå•: {len(self.group_blacklist)} ä¸ªç¾¤èŠ

**åˆ†æè®¾ç½®**
- å¯ç”¨emoji: {'âœ… å·²å¯ç”¨' if self.enable_emoji else 'âŒ å·²ç¦ç”¨'}
- æ˜¾ç¤ºç»Ÿè®¡: {'âœ… å·²å¯ç”¨' if self.enable_statistics else 'âŒ å·²ç¦ç”¨'}
- æœ€å¤§æ‘˜è¦é•¿åº¦: {self.max_summary_length} å­—ç¬¦
- å¯ç”¨æˆªå›¾: {'âœ… å·²å¯ç”¨' if self.enable_screenshot else 'âŒ å·²ç¦ç”¨'}
- æˆªå›¾è´¨é‡: {self.screenshot_quality}
- æˆªå›¾å®½åº¦: {self.screenshot_width}px
- æˆªå›¾é«˜åº¦: {self.screenshot_height}px
- æˆªå›¾æ ¼å¼: {self.screenshot_format}
- æˆªå–æ•´é¡µ: {'âœ… å·²å¯ç”¨' if self.screenshot_full_page else 'âŒ å·²ç¦ç”¨'}
- æˆªå›¾ç­‰å¾…æ—¶é—´: {self.screenshot_wait_time}ms

**LLMé…ç½®**
- æŒ‡å®šæä¾›å•†: {self.llm_provider if self.llm_provider else 'ä½¿ç”¨ä¼šè¯é»˜è®¤'}
- è‡ªå®šä¹‰æç¤ºè¯: {'âœ… å·²å¯ç”¨' if self.custom_prompt else 'âŒ æœªè®¾ç½®'}

**ç¿»è¯‘è®¾ç½®**
- å¯ç”¨ç½‘é¡µç¿»è¯‘: {'âœ… å·²å¯ç”¨' if self.enable_translation else 'âŒ å·²ç¦ç”¨'}
- ç›®æ ‡è¯­è¨€: {self.target_language}
- ç¿»è¯‘æä¾›å•†: {self.translation_provider}
- è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯: {'âœ… å·²å¯ç”¨' if self.custom_translation_prompt else 'âŒ æœªè®¾ç½®'}

**ç¼“å­˜è®¾ç½®**
- å¯ç”¨ç»“æœç¼“å­˜: {'âœ… å·²å¯ç”¨' if self.enable_cache else 'âŒ å·²ç¦ç”¨'}
- ç¼“å­˜è¿‡æœŸæ—¶é—´: {self.cache_expire_time} åˆ†é’Ÿ
- æœ€å¤§ç¼“å­˜æ•°é‡: {self.max_cache_size} ä¸ª

**å†…å®¹æå–è®¾ç½®**
- å¯ç”¨ç‰¹å®šå†…å®¹æå–: {'âœ… å·²å¯ç”¨' if self.enable_specific_extraction else 'âŒ å·²ç¦ç”¨'}
- æå–å†…å®¹ç±»å‹: {', '.join(self.extract_types)}

*æç¤º: å¦‚éœ€ä¿®æ”¹é…ç½®ï¼Œè¯·åœ¨AstrBotç®¡ç†é¢æ¿ä¸­ç¼–è¾‘æ’ä»¶é…ç½®*"""
        
        yield event.plain_result(config_info)
    
    @filter.command("test_merge", alias={'æµ‹è¯•åˆå¹¶è½¬å‘', 'æµ‹è¯•è½¬å‘'})
    async def test_merge_forward(self, event: AstrMessageEvent):
        '''æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½'''
        from astrbot.api.message_components import Node, Plain, Nodes
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç¾¤èŠæ¶ˆæ¯
        group_id = None
        if hasattr(event, 'group_id') and event.group_id:
            group_id = event.group_id
        elif hasattr(event, 'message_obj') and hasattr(event.message_obj, 'group_id') and event.message_obj.group_id:
            group_id = event.message_obj.group_id
        
        if group_id:
            # åˆ›å»ºæµ‹è¯•ç”¨çš„åˆå¹¶è½¬å‘èŠ‚ç‚¹
            nodes = []
            
            # æ ‡é¢˜èŠ‚ç‚¹
            title_node = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•åˆå¹¶è½¬å‘",
                content=[
                    Plain("è¿™æ˜¯åˆå¹¶è½¬å‘æµ‹è¯•æ¶ˆæ¯")
                ]
            )
            nodes.append(title_node)
            
            # å†…å®¹èŠ‚ç‚¹1
            content_node1 = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•èŠ‚ç‚¹1",
                content=[
                    Plain("è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•èŠ‚ç‚¹å†…å®¹")
                ]
            )
            nodes.append(content_node1)
            
            # å†…å®¹èŠ‚ç‚¹2
            content_node2 = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•èŠ‚ç‚¹2",
                content=[
                    Plain("è¿™æ˜¯ç¬¬äºŒä¸ªæµ‹è¯•èŠ‚ç‚¹å†…å®¹")
                ]
            )
            nodes.append(content_node2)
            
            # ä½¿ç”¨NodesåŒ…è£…æ‰€æœ‰èŠ‚ç‚¹ï¼Œåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
            merge_forward_message = Nodes(nodes)
            yield event.chain_result([merge_forward_message])
            logger.info(f"æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½ï¼Œç¾¤èŠ {group_id}")
        else:
            yield event.plain_result("åˆå¹¶è½¬å‘åŠŸèƒ½ä»…æ”¯æŒç¾¤èŠæ¶ˆæ¯æµ‹è¯•")
            logger.info("ç§èŠæ¶ˆæ¯æ— æ³•æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½")
    
    @filter.command("group_blacklist", alias={'ç¾¤é»‘åå•', 'é»‘åå•'})
    async def manage_group_blacklist(self, event: AstrMessageEvent):
        """ç®¡ç†ç¾¤èŠé»‘åå•"""
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()
        
        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰é»‘åå•
        if len(message_parts) <= 1:
            if not self.group_blacklist:
                yield event.plain_result("å½“å‰ç¾¤èŠé»‘åå•ä¸ºç©º")
                return
            
            blacklist_info = "**å½“å‰ç¾¤èŠé»‘åå•**\n\n"
            for i, group_id in enumerate(self.group_blacklist, 1):
                blacklist_info += f"{i}. {group_id}\n"
            
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist add <ç¾¤å·>` æ·»åŠ ç¾¤èŠåˆ°é»‘åå•"
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist remove <ç¾¤å·>` ä»é»‘åå•ç§»é™¤ç¾¤èŠ"
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist clear` æ¸…ç©ºé»‘åå•"
            
            yield event.plain_result(blacklist_info)
            return
        
        action = message_parts[1].lower() if len(message_parts) > 1 else ""
        group_id = message_parts[2] if len(message_parts) > 2 else ""
        
        if action == "add" and group_id:
            # æ·»åŠ ç¾¤èŠåˆ°é»‘åå•
            if group_id in self.group_blacklist:
                yield event.plain_result(f"ç¾¤èŠ {group_id} å·²åœ¨é»‘åå•ä¸­")
                return
            
            self.group_blacklist.append(group_id)
            self._save_group_blacklist()
            yield event.plain_result(f"âœ… å·²æ·»åŠ ç¾¤èŠ {group_id} åˆ°é»‘åå•")
            
        elif action == "remove" and group_id:
            # ä»é»‘åå•ç§»é™¤ç¾¤èŠ
            if group_id not in self.group_blacklist:
                yield event.plain_result(f"ç¾¤èŠ {group_id} ä¸åœ¨é»‘åå•ä¸­")
                return
            
            self.group_blacklist.remove(group_id)
            self._save_group_blacklist()
            yield event.plain_result(f"âœ… å·²ä»é»‘åå•ç§»é™¤ç¾¤èŠ {group_id}")
            
        elif action == "clear":
            # æ¸…ç©ºé»‘åå•
            if not self.group_blacklist:
                yield event.plain_result("é»‘åå•å·²ä¸ºç©º")
                return
            
            self.group_blacklist.clear()
            self._save_group_blacklist()
            yield event.plain_result("âœ… å·²æ¸…ç©ºç¾¤èŠé»‘åå•")
            
        else:
            yield event.plain_result("æ— æ•ˆçš„æ“ä½œï¼Œè¯·ä½¿ç”¨: add <ç¾¤å·>, remove <ç¾¤å·>, clear")
    
    @filter.command("web_cache", alias={'ç½‘é¡µç¼“å­˜', 'æ¸…ç†ç¼“å­˜'})
    async def manage_cache(self, event: AstrMessageEvent):
        """ç®¡ç†ç¼“å­˜"""
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()
        
        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰ç¼“å­˜çŠ¶æ€
        if len(message_parts) <= 1:
            cache_stats = self.cache_manager.get_stats()
            cache_info = "**å½“å‰ç¼“å­˜çŠ¶æ€**\n\n"
            cache_info += f"- ç¼“å­˜æ€»æ•°: {cache_stats['total']} ä¸ª\n"
            cache_info += f"- æœ‰æ•ˆç¼“å­˜: {cache_stats['valid']} ä¸ª\n"
            cache_info += f"- è¿‡æœŸç¼“å­˜: {cache_stats['expired']} ä¸ª\n"
            cache_info += f"- ç¼“å­˜è¿‡æœŸæ—¶é—´: {self.cache_expire_time} åˆ†é’Ÿ\n"
            cache_info += f"- æœ€å¤§ç¼“å­˜æ•°é‡: {self.max_cache_size} ä¸ª\n"
            cache_info += f"- ç¼“å­˜åŠŸèƒ½: {'âœ… å·²å¯ç”¨' if self.enable_cache else 'âŒ å·²ç¦ç”¨'}\n"
            
            cache_info += "\nä½¿ç”¨ `/web_cache clear` æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"
            
            yield event.plain_result(cache_info)
            return
        
        action = message_parts[1].lower() if len(message_parts) > 1 else ""
        
        if action == "clear":
            # æ¸…ç©ºç¼“å­˜
            self.cache_manager.clear()
            cache_stats = self.cache_manager.get_stats()
            yield event.plain_result(f"âœ… å·²æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ï¼Œå½“å‰ç¼“å­˜æ•°é‡: {cache_stats['total']} ä¸ª")
            
        else:
            yield event.plain_result("æ— æ•ˆçš„æ“ä½œï¼Œè¯·ä½¿ç”¨: clear")
    
    @filter.command("web_export", alias={'å¯¼å‡ºåˆ†æç»“æœ', 'ç½‘é¡µå¯¼å‡º'})
    async def export_analysis_result(self, event: AstrMessageEvent):
        """å¯¼å‡ºåˆ†æç»“æœï¼Œæ”¯æŒMarkdownã€JSONç­‰æ ¼å¼"""
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()
        
        # æ£€æŸ¥å‚æ•°
        if len(message_parts) < 2:
            yield event.plain_result("è¯·æä¾›è¦å¯¼å‡ºçš„URLé“¾æ¥å’Œæ ¼å¼ï¼Œä¾‹å¦‚ï¼š/web_export https://example.com md æˆ– /web_export all json")
            return
        
        url_or_all = message_parts[1]
        format_type = message_parts[2] if len(message_parts) > 2 else "md"
        
        # éªŒè¯æ ¼å¼ç±»å‹
        supported_formats = ["md", "markdown", "json", "txt"]
        if format_type.lower() not in supported_formats:
            yield event.plain_result(f"ä¸æ”¯æŒçš„æ ¼å¼ç±»å‹ï¼Œè¯·ä½¿ç”¨ï¼š{', '.join(supported_formats)}")
            return
        
        # å‡†å¤‡å¯¼å‡ºæ•°æ®
        export_results = []
        
        if url_or_all.lower() == "all":
            # å¯¼å‡ºæ‰€æœ‰ç¼“å­˜çš„åˆ†æç»“æœ
            if not self.cache:
                yield event.plain_result("å½“å‰æ²¡æœ‰ç¼“å­˜çš„åˆ†æç»“æœ")
                return
            
            for url, cache_data in self.cache.items():
                export_results.append({
                    "url": url,
                    "result": cache_data["result"]
                })
        else:
            # å¯¼å‡ºæŒ‡å®šURLçš„åˆ†æç»“æœ
            url = url_or_all
            
            # æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ
            if not self.analyzer.is_valid_url(url):
                yield event.plain_result("æ— æ•ˆçš„URLé“¾æ¥")
                return
            
            # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦æœ‰è¯¥URLçš„åˆ†æç»“æœ
            cached_result = self._check_cache(url)
            if cached_result:
                export_results.append({
                    "url": url,
                    "result": cached_result
                })
            else:
                # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå…ˆè¿›è¡Œåˆ†æ
                yield event.plain_result("ç¼“å­˜ä¸­æ²¡æœ‰è¯¥URLçš„åˆ†æç»“æœï¼Œæ­£åœ¨è¿›è¡Œåˆ†æ...")
                
                # æŠ“å–å¹¶åˆ†æç½‘é¡µ
                async with WebAnalyzer(self.max_content_length, self.timeout, self.user_agent, self.proxy, self.retry_count, self.retry_delay) as analyzer:
                    html = await analyzer.fetch_webpage(url)
                    if not html:
                        yield event.plain_result(f"æ— æ³•æŠ“å–ç½‘é¡µå†…å®¹: {url}")
                        return
                    
                    content_data = analyzer.extract_content(html, url)
                    if not content_data:
                        yield event.plain_result(f"æ— æ³•è§£æç½‘é¡µå†…å®¹: {url}")
                        return
                    
                    # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
                    if self.enable_translation:
                        translated_content = await self._translate_content(event, content_data['content'])
                        translated_content_data = content_data.copy()
                        translated_content_data['content'] = translated_content
                        analysis_result = await self.analyze_with_llm(event, translated_content_data)
                    else:
                        analysis_result = await self.analyze_with_llm(event, content_data)
                    
                    # æå–ç‰¹å®šå†…å®¹
                    specific_content = self._extract_specific_content(html, url)
                    if specific_content:
                        # åœ¨åˆ†æç»“æœä¸­æ·»åŠ ç‰¹å®šå†…å®¹
                        specific_content_str = "\n\n**ç‰¹å®šå†…å®¹æå–**\n"
                        
                        if 'images' in specific_content and specific_content['images']:
                            specific_content_str += f"\nğŸ“· å›¾ç‰‡é“¾æ¥ ({len(specific_content['images'])}):\n"
                            for img_url in specific_content['images']:
                                specific_content_str += f"- {img_url}\n"
                        
                        if 'links' in specific_content and specific_content['links']:
                            specific_content_str += f"\nğŸ”— ç›¸å…³é“¾æ¥ ({len(specific_content['links'])}):\n"
                            for link in specific_content['links'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé“¾æ¥
                                specific_content_str += f"- [{link['text']}]({link['url']})\n"
                        
                        if 'code_blocks' in specific_content and specific_content['code_blocks']:
                            specific_content_str += f"\nğŸ’» ä»£ç å— ({len(specific_content['code_blocks'])}):\n"
                            for i, code in enumerate(specific_content['code_blocks'][:2]):  # åªæ˜¾ç¤ºå‰2ä¸ªä»£ç å—
                                specific_content_str += f"```\n{code}\n```\n"
                        
                        analysis_result += specific_content_str
                    
                    # å‡†å¤‡å¯¼å‡ºæ•°æ®
                    export_results.append({
                        "url": url,
                        "result": {
                            "url": url,
                            "result": analysis_result,
                            "screenshot": None
                        }
                    })
        
        # æ‰§è¡Œå¯¼å‡º
        try:
            import os
            import json
            import time
            
            # åˆ›å»ºdataç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            data_dir = os.path.join(os.path.dirname(__file__), "data")
            os.makedirs(data_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = int(time.time())
            if len(export_results) == 1:
                # å•ä¸ªURLå¯¼å‡º
                url = export_results[0]["url"]
                # ä»URLä¸­æå–åŸŸåä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†
                from urllib.parse import urlparse
                parsed = urlparse(url)
                domain = parsed.netloc.replace(".", "_")
                filename = f"web_analysis_{domain}_{timestamp}"
            else:
                # å¤šä¸ªURLå¯¼å‡º
                filename = f"web_analysis_all_{timestamp}"
            
            # æ ¹æ®æ ¼å¼ç”Ÿæˆæ–‡ä»¶å†…å®¹
            file_extension = format_type.lower()
            if file_extension == "markdown":
                file_extension = "md"
            
            file_path = os.path.join(data_dir, f"{filename}.{file_extension}")
            
            if format_type.lower() in ["md", "markdown"]:
                # ç”ŸæˆMarkdownæ ¼å¼å†…å®¹
                md_content = "# ç½‘é¡µåˆ†æç»“æœå¯¼å‡º\n\n"
                md_content += f"å¯¼å‡ºæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}\n\n"
                md_content += f"å…± {len(export_results)} ä¸ªåˆ†æç»“æœ\n\n"
                md_content += "---\n\n"
                
                for i, export_item in enumerate(export_results, 1):
                    url = export_item["url"]
                    result_data = export_item["result"]
                    
                    md_content += f"## {i}. {url}\n\n"
                    md_content += result_data["result"]
                    md_content += "\n\n"
                    md_content += "---\n\n"
                
                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(md_content)
            
            elif format_type.lower() == "json":
                # ç”ŸæˆJSONæ ¼å¼å†…å®¹
                json_data = {
                    "export_time": timestamp,
                    "export_time_str": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp)),
                    "total_results": len(export_results),
                    "results": []
                }
                
                for export_item in export_results:
                    url = export_item["url"]
                    result_data = export_item["result"]
                    
                    json_data["results"].append({
                        "url": url,
                        "analysis_result": result_data["result"],
                        "has_screenshot": result_data["screenshot"] is not None
                    })
                
                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            elif format_type.lower() == "txt":
                # ç”Ÿæˆçº¯æ–‡æœ¬æ ¼å¼å†…å®¹
                txt_content = "ç½‘é¡µåˆ†æç»“æœå¯¼å‡º\n"
                txt_content += f"å¯¼å‡ºæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}\n"
                txt_content += f"å…± {len(export_results)} ä¸ªåˆ†æç»“æœ\n"
                txt_content += "=" * 50 + "\n\n"
                
                for i, export_item in enumerate(export_results, 1):
                    url = export_item["url"]
                    result_data = export_item["result"]
                    
                    txt_content += f"{i}. {url}\n"
                    txt_content += "-" * 30 + "\n"
                    txt_content += result_data["result"]
                    txt_content += "\n\n"
                    txt_content += "=" * 50 + "\n\n"
                
                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(txt_content)
            
            # å‘é€å¯¼å‡ºæˆåŠŸæ¶ˆæ¯ï¼Œå¹¶é™„å¸¦å¯¼å‡ºæ–‡ä»¶
            from astrbot.api.message_components import Plain, File
            
            # æ„å»ºæ¶ˆæ¯é“¾
            message_chain = [
                    Plain("âœ… åˆ†æç»“æœå¯¼å‡ºæˆåŠŸï¼\n\n"),
                    Plain(f"å¯¼å‡ºæ ¼å¼: {format_type}\n"),
                    Plain(f"å¯¼å‡ºæ•°é‡: {len(export_results)}\n\n"),
                    Plain("ğŸ“ å¯¼å‡ºæ–‡ä»¶ï¼š\n"),
                    File(file=file_path, name=os.path.basename(file_path))
                ]
            
            yield event.chain_result(message_chain)
            
            logger.info(f"æˆåŠŸå¯¼å‡º {len(export_results)} ä¸ªåˆ†æç»“æœåˆ° {file_path}ï¼Œå¹¶å‘é€ç»™ç”¨æˆ·")
        
        except Exception as e:
            logger.error(f"å¯¼å‡ºåˆ†æç»“æœå¤±è´¥: {e}")
            yield event.plain_result(f"âŒ å¯¼å‡ºåˆ†æç»“æœå¤±è´¥: {str(e)}")
    
    def _save_group_blacklist(self):
        """ä¿å­˜ç¾¤èŠé»‘åå•åˆ°é…ç½®"""
        try:
            # å°†ç¾¤èŠåˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
            group_text = '\n'.join(self.group_blacklist)
            # æ›´æ–°é…ç½®å¹¶ä¿å­˜
            self.config['group_blacklist'] = group_text
            self.config.save_config()
        except Exception as e:
            logger.error(f"ä¿å­˜ç¾¤èŠé»‘åå•å¤±è´¥: {e}")
    
    def _check_cache(self, url: str) -> dict:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ"""
        if not self.enable_cache:
            return None
        
        return self.cache_manager.get(url)
    
    def _update_cache(self, url: str, result: dict):
        """æ›´æ–°ç¼“å­˜"""
        if not self.enable_cache:
            return
        
        self.cache_manager.set(url, result)
    
    def _clean_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        # ç¼“å­˜ç®¡ç†å™¨ä¼šè‡ªåŠ¨æ¸…ç†ï¼Œè¿™é‡Œç•™ç©ºå³å¯
        pass
    
    async def _translate_content(self, event: AstrMessageEvent, content: str) -> str:
        """ç¿»è¯‘ç½‘é¡µå†…å®¹"""
        if not self.enable_translation:
            return content
        
        try:
            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨
            if not hasattr(self.context, 'llm_generate'):
                logger.error("LLMä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘")
                return content
            
            # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„LLMæä¾›å•†ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨å½“å‰ä¼šè¯çš„æ¨¡å‹
            provider_id = self.llm_provider
            if not provider_id:
                umo = event.unified_msg_origin
                provider_id = await self.context.get_current_chat_provider_id(umo=umo)
            
            if not provider_id:
                logger.error("æ— æ³•è·å–LLMæä¾›å•†IDï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘")
                return content
            
            # ä½¿ç”¨è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯æˆ–é»˜è®¤æç¤ºè¯
            if self.custom_translation_prompt:
                # æ›¿æ¢è‡ªå®šä¹‰æç¤ºè¯ä¸­çš„å˜é‡
                prompt = self.custom_translation_prompt.format(
                    content=content,
                    target_language=self.target_language
                )
            else:
                # é»˜è®¤ç¿»è¯‘æç¤ºè¯
                prompt = f"è¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆ{self.target_language}è¯­è¨€ï¼Œä¿æŒåŸæ–‡æ„æ€ä¸å˜ï¼Œè¯­è¨€æµç•…è‡ªç„¶ï¼š\n\n{content}"
            
            # è°ƒç”¨LLMè¿›è¡Œç¿»è¯‘
            llm_resp = await self.context.llm_generate(
                chat_provider_id=provider_id,
                prompt=prompt
            )
            
            if llm_resp and llm_resp.completion_text:
                return llm_resp.completion_text.strip()
            else:
                logger.error("LLMç¿»è¯‘è¿”å›ä¸ºç©º")
                return content
        except Exception as e:
            logger.error(f"ç¿»è¯‘å†…å®¹å¤±è´¥: {e}")
            return content
    
    def _extract_specific_content(self, html: str, url: str) -> dict:
        """æå–ç‰¹å®šç±»å‹çš„å†…å®¹"""
        if not self.enable_specific_extraction:
            return {}
        
        try:
            # ä½¿ç”¨WebAnalyzerå®ä¾‹æå–ç‰¹å®šå†…å®¹
            analyzer = WebAnalyzer(self.max_content_length, self.timeout, self.user_agent)
            return analyzer.extract_specific_content(html, url, self.extract_types)
        except Exception as e:
            logger.error(f"æå–ç‰¹å®šå†…å®¹å¤±è´¥: {e}")
            return {}
    
    async def _send_analysis_result(self, event, analysis_results):
        '''å‘é€åˆ†æç»“æœï¼Œæ ¹æ®å¼€å…³å†³å®šæ˜¯å¦ä½¿ç”¨åˆå¹¶è½¬å‘'''
        try:
            from astrbot.api.message_components import Node, Plain, Nodes, Image
            import tempfile
            import os
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç¾¤èŠæ¶ˆæ¯ä¸”åˆå¹¶è½¬å‘åŠŸèƒ½å·²å¯ç”¨
            group_id = None
            if hasattr(event, 'group_id') and event.group_id:
                group_id = event.group_id
            elif hasattr(event, 'message_obj') and hasattr(event.message_obj, 'group_id') and event.message_obj.group_id:
                group_id = event.message_obj.group_id
            
            # å¦‚æœæ˜¯ç¾¤èŠæ¶ˆæ¯ä¸”åˆå¹¶è½¬å‘åŠŸèƒ½å·²å¯ç”¨ï¼Œä½¿ç”¨åˆå¹¶è½¬å‘
            if group_id and self.merge_forward_enabled:
                # ä½¿ç”¨åˆå¹¶è½¬å‘ - å°†æ‰€æœ‰åˆ†æç»“æœåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
                nodes = []
                
                # æ·»åŠ æ€»æ ‡é¢˜èŠ‚ç‚¹
                total_title_node = Node(
                    uin=event.get_sender_id(),
                    name="ç½‘é¡µåˆ†æç»“æœæ±‡æ€»",
                    content=[
                        Plain(f"å…±{len(analysis_results)}ä¸ªç½‘é¡µåˆ†æç»“æœ")
                    ]
                )
                nodes.append(total_title_node)
                
                # ä¸ºæ¯ä¸ªURLæ·»åŠ åˆ†æç»“æœèŠ‚ç‚¹
                for i, result_data in enumerate(analysis_results, 1):
                    url = result_data['url']
                    analysis_result = result_data['result']
                    screenshot = result_data.get('screenshot')
                    
                    # æ·»åŠ å½“å‰URLçš„æ ‡é¢˜èŠ‚ç‚¹
                    url_title_node = Node(
                        uin=event.get_sender_id(),
                        name=f"åˆ†æç»“æœ {i}",
                        content=[
                            Plain(f"ç¬¬{i}ä¸ªç½‘é¡µåˆ†æç»“æœ - {url}")
                        ]
                    )
                    nodes.append(url_title_node)
                    
                    # æ·»åŠ å½“å‰URLçš„å†…å®¹èŠ‚ç‚¹
                    content_node = Node(
                        uin=event.get_sender_id(),
                        name="è¯¦ç»†åˆ†æ",
                        content=[
                            Plain(analysis_result)
                        ]
                    )
                    nodes.append(content_node)
            
                # ä½¿ç”¨NodesåŒ…è£…æ‰€æœ‰èŠ‚ç‚¹ï¼Œåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
                merge_forward_message = Nodes(nodes)
                
                # å‘é€åˆå¹¶è½¬å‘æ¶ˆæ¯
                yield event.chain_result([merge_forward_message])
                
                # å¦‚æœæœ‰æˆªå›¾ï¼Œé€ä¸ªå‘é€æˆªå›¾
                for result_data in analysis_results:
                    screenshot = result_data.get('screenshot')
                    if screenshot:
                        try:
                            # æ ¹æ®æˆªå›¾æ ¼å¼è®¾ç½®æ–‡ä»¶åç¼€
                            suffix = f".{self.screenshot_format}" if self.screenshot_format else ".jpg"
                            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æˆªå›¾
                            with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as temp_file:
                                temp_file.write(screenshot)
                                temp_file_path = temp_file.name
                            
                            # ä½¿ç”¨Image.fromFileSystem()æ–¹æ³•å‘é€å›¾ç‰‡
                            image_component = Image.fromFileSystem(temp_file_path)
                            yield event.chain_result([image_component])
                            logger.info(f"ç¾¤èŠ {group_id} ä½¿ç”¨åˆå¹¶è½¬å‘å‘é€åˆ†æç»“æœï¼Œå¹¶å‘é€æˆªå›¾")
                            
                            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                            os.unlink(temp_file_path)
                        except Exception as e:
                            logger.error(f"å‘é€æˆªå›¾å¤±è´¥: {e}")
                            # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
                            if 'temp_file_path' in locals() and os.path.exists(temp_file_path):
                                os.unlink(temp_file_path)
                logger.info(f"ç¾¤èŠ {group_id} ä½¿ç”¨åˆå¹¶è½¬å‘å‘é€{len(analysis_results)}ä¸ªåˆ†æç»“æœ")
            else:
                # æ™®é€šå‘é€ - é€ä¸ªå‘é€åˆ†æç»“æœ
                for i, result_data in enumerate(analysis_results, 1):
                    url = result_data['url']
                    analysis_result = result_data['result']
                    screenshot = result_data.get('screenshot')
                    
                    # å‘é€åˆ†æç»“æœæ–‡æœ¬
                    if len(analysis_results) == 1:
                        result_text = f"ç½‘é¡µåˆ†æç»“æœï¼š\n{analysis_result}"
                    else:
                        result_text = f"ç¬¬{i}/{len(analysis_results)}ä¸ªç½‘é¡µåˆ†æç»“æœï¼š\n{analysis_result}"
                    yield event.plain_result(result_text)
                    
                    # å¦‚æœæœ‰æˆªå›¾ï¼Œå‘é€æˆªå›¾
                    if screenshot:
                        try:
                            # æ ¹æ®æˆªå›¾æ ¼å¼è®¾ç½®æ–‡ä»¶åç¼€
                            suffix = f".{self.screenshot_format}" if self.screenshot_format else ".jpg"
                            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æˆªå›¾
                            with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as temp_file:
                                temp_file.write(screenshot)
                                temp_file_path = temp_file.name
                            
                            # ä½¿ç”¨Image.fromFileSystem()æ–¹æ³•å‘é€å›¾ç‰‡
                            image_component = Image.fromFileSystem(temp_file_path)
                            yield event.chain_result([image_component])
                            logger.info("æ™®é€šå‘é€åˆ†æç»“æœï¼Œå¹¶å‘é€æˆªå›¾")
                            
                            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                            os.unlink(temp_file_path)
                        except Exception as e:
                            logger.error(f"å‘é€æˆªå›¾å¤±è´¥: {e}")
                            # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
                            if 'temp_file_path' in locals() and os.path.exists(temp_file_path):
                                os.unlink(temp_file_path)
                message_type = "ç¾¤èŠ" if group_id else "ç§èŠ"
                logger.info(f"{message_type}æ¶ˆæ¯æ™®é€šå‘é€{len(analysis_results)}ä¸ªåˆ†æç»“æœ")
        except Exception as e:
            logger.error(f"å‘é€åˆ†æç»“æœå¤±è´¥: {e}")
            yield event.plain_result(f"âŒ å‘é€åˆ†æç»“æœå¤±è´¥: {str(e)}")
    
    async def terminate(self):
        """æ’ä»¶å¸è½½æ—¶çš„æ¸…ç†å·¥ä½œ"""
        logger.info("ç½‘é¡µåˆ†ææ’ä»¶å·²å¸è½½")