"""
AstrBot ç½‘é¡µåˆ†ææ’ä»¶
è‡ªåŠ¨è¯†åˆ«ç”¨æˆ·å‘é€çš„ç½‘é¡µé“¾æ¥ï¼ŒæŠ“å–å†…å®¹å¹¶è°ƒç”¨LLMè¿›è¡Œåˆ†æå’Œæ€»ç»“
"""

import re
from typing import List, Optional
from urllib.parse import urlparse

import httpx
from bs4 import BeautifulSoup

from astrbot.api import AstrBotConfig
from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.star import Context, Star, register
from astrbot.api import logger


class WebAnalyzer:
    """ç½‘é¡µåˆ†æå™¨ç±»"""
    
    def __init__(self, max_content_length: int = 10000, timeout: int = 30, user_agent: str = None):
        self.max_content_length = max_content_length
        self.timeout = timeout
        self.user_agent = user_agent or 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        self.client = None
        self.browser = None
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.client = httpx.AsyncClient(timeout=self.timeout)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.client:
            await self.client.aclose()
        if self.browser:
            await self.browser.close()
    
    def extract_urls(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–URLé“¾æ¥"""
        # åŒ¹é…å¸¸è§çš„URLæ ¼å¼
        url_pattern = r'https?://[^\s\u4e00-\u9fff]+'
        urls = re.findall(url_pattern, text)
        return urls
    
    def is_valid_url(self, url: str) -> bool:
        """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except Exception:
            return False
    
    async def fetch_webpage(self, url: str) -> Optional[str]:
        """æŠ“å–ç½‘é¡µå†…å®¹"""
        try:
            headers = {
                'User-Agent': self.user_agent
            }
            
            response = await self.client.get(url, headers=headers, follow_redirects=True)
            response.raise_for_status()
            
            return response.text
        except Exception as e:
            logger.error(f"æŠ“å–ç½‘é¡µå¤±è´¥: {url}, é”™è¯¯: {e}")
            return None
    
    def extract_content(self, html: str, url: str) -> dict:
        """ä»HTMLä¸­æå–ä¸»è¦å†…å®¹"""
        try:
            soup = BeautifulSoup(html, 'lxml')
            
            # æå–æ ‡é¢˜
            title = soup.find('title')
            title_text = title.get_text().strip() if title else "æ— æ ‡é¢˜"
            
            # å°è¯•æå–æ–‡ç« å†…å®¹ï¼ˆä¼˜å…ˆé€‰æ‹©articleã€mainç­‰è¯­ä¹‰åŒ–æ ‡ç­¾ï¼‰
            content_selectors = [
                'article',
                'main',
                '.article-content',
                '.post-content',
                '.content',
                'body'
            ]
            
            content_text = ""
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element:
                    # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
                    for script in element(['script', 'style']):
                        script.decompose()
                    
                    text = element.get_text(separator='\n', strip=True)
                    if len(text) > len(content_text):
                        content_text = text
            
            # å¦‚æœæ²¡æ‰¾åˆ°åˆé€‚çš„å†…å®¹ï¼Œä½¿ç”¨body
            if not content_text:
                body = soup.find('body')
                if body:
                    for script in body(['script', 'style']):
                        script.decompose()
                    content_text = body.get_text(separator='\n', strip=True)
            
            # é™åˆ¶å†…å®¹é•¿åº¦
            if len(content_text) > self.max_content_length:
                content_text = content_text[:self.max_content_length] + "..."
            
            return {
                'title': title_text,
                'content': content_text,
                'url': url
            }
        except Exception as e:
            logger.error(f"è§£æç½‘é¡µå†…å®¹å¤±è´¥: {e}")
            return None
    
    async def capture_screenshot(self, url: str, quality: int = 80, width: int = 1280, full_page: bool = False, wait_time: int = 2000) -> Optional[bytes]:
        """æ•è·ç½‘é¡µæˆªå›¾"""
        try:
            from playwright.async_api import async_playwright
            import sys
            import subprocess
            
            # é¦–å…ˆå°è¯•å®‰è£…æµè§ˆå™¨ï¼ˆæ— è®ºæ˜¯å¦å·²å®‰è£…ï¼Œplaywright installéƒ½ä¼šæ£€æŸ¥å¹¶æ›´æ–°ï¼‰
            logger.info("æ­£åœ¨æ£€æŸ¥å¹¶å®‰è£…æµè§ˆå™¨...")
            result = subprocess.run([sys.executable, "-m", "playwright", "install", "chromium"], 
                                  capture_output=True, text=True)
            
            if result.returncode != 0:
                logger.error(f"æµè§ˆå™¨å®‰è£…å¤±è´¥: {result.stderr}")
                return None
            
            logger.info("æµè§ˆå™¨å®‰è£…æˆåŠŸï¼Œæ­£åœ¨å°è¯•æˆªå›¾...")
            
            # å°è¯•å¯åŠ¨playwrightå¹¶æˆªå›¾
            async with async_playwright() as p:
                # å¯åŠ¨æµè§ˆå™¨ï¼ˆæ— å¤´æ¨¡å¼ï¼‰
                self.browser = await p.chromium.launch(
                    headless=True,
                    # æ·»åŠ é¢å¤–çš„å¯åŠ¨å‚æ•°ï¼Œæé«˜å…¼å®¹æ€§
                    args=[
                        '--no-sandbox',
                        '--disable-setuid-sandbox',
                        '--disable-dev-shm-usage',
                        '--disable-gpu',
                        '--remote-debugging-port=9222'
                    ]
                )
                page = await self.browser.new_page(
                    viewport={'width': width, 'height': 720},
                    user_agent=self.user_agent
                )
                
                # å¯¼èˆªåˆ°ç›®æ ‡URLï¼Œä½¿ç”¨æ›´å®½æ¾çš„ç­‰å¾…æ¡ä»¶
                await page.goto(url, wait_until='domcontentloaded', timeout=60000)
                
                # ç­‰å¾…æŒ‡å®šæ—¶é—´ï¼Œç¡®ä¿é¡µé¢å®Œå…¨åŠ è½½
                await page.wait_for_timeout(wait_time)
                
                # æ•è·æˆªå›¾
                screenshot_bytes = await page.screenshot(
                    full_page=full_page,
                    quality=quality,
                    type='jpeg'
                )
                
                await self.browser.close()
                self.browser = None
                
                logger.info("æˆªå›¾æˆåŠŸ")
                return screenshot_bytes
        except Exception as e:
            logger.error(f"æ•è·ç½‘é¡µæˆªå›¾å¤±è´¥: {url}, é”™è¯¯: {e}")
            if self.browser:
                await self.browser.close()
                self.browser = None
            return None


@register("astrbot_plugin_web_analyzer", "Sakura520222", "è‡ªåŠ¨è¯†åˆ«ç½‘é¡µé“¾æ¥å¹¶è¿›è¡Œå†…å®¹åˆ†æå’Œæ€»ç»“", "1.0.0", "https://github.com/Sakura520222/astrbot_plugin_web_analyzer")
class WebAnalyzerPlugin(Star):
    """ç½‘é¡µåˆ†ææ’ä»¶ä¸»ç±»"""
    
    def __init__(self, context: Context, config: AstrBotConfig):
        super().__init__(context)
        self.config = config
        
        # ä»é…ç½®è·å–å‚æ•°
        self.max_content_length = config.get('max_content_length', 10000)
        self.timeout = config.get('request_timeout', 30)
        self.llm_enabled = config.get('llm_enabled', True)
        self.auto_analyze = config.get('auto_analyze', True)
        self.user_agent = config.get('user_agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        self.allowed_domains = self._parse_domain_list(config.get('allowed_domains', ''))
        self.blocked_domains = self._parse_domain_list(config.get('blocked_domains', ''))
        
        # åˆ†æè®¾ç½®
        analysis_settings = config.get('analysis_settings', {})
        self.enable_emoji = analysis_settings.get('enable_emoji', True)
        self.enable_statistics = analysis_settings.get('enable_statistics', True)
        self.max_summary_length = analysis_settings.get('max_summary_length', 2000)
        # æˆªå›¾è®¾ç½®
        self.enable_screenshot = analysis_settings.get('enable_screenshot', True)
        self.screenshot_quality = analysis_settings.get('screenshot_quality', 80)
        self.screenshot_width = analysis_settings.get('screenshot_width', 1280)
        self.screenshot_full_page = analysis_settings.get('screenshot_full_page', False)
        self.screenshot_wait_time = analysis_settings.get('screenshot_wait_time', 2000)
        
        # LLMæä¾›å•†é…ç½®
        self.llm_provider = config.get('llm_provider', '')
        
        # ç¾¤èŠé»‘åå•é…ç½®
        group_blacklist_text = config.get('group_blacklist', '')
        self.group_blacklist = self._parse_group_list(group_blacklist_text)
        
        # åˆå¹¶è½¬å‘é…ç½®
        self.merge_forward_enabled = config.get('merge_forward_enabled', False)  # æ˜¯å¦å¯ç”¨åˆå¹¶è½¬å‘
        
        # è‡ªå®šä¹‰æç¤ºè¯é…ç½®
        self.custom_prompt = config.get('custom_prompt', '')  # è‡ªå®šä¹‰åˆ†ææç¤ºè¯
        
        # ç¿»è¯‘è®¾ç½®
        translation_settings = config.get('translation_settings', {})
        self.enable_translation = translation_settings.get('enable_translation', False)
        self.target_language = translation_settings.get('target_language', 'zh')
        self.translation_provider = translation_settings.get('translation_provider', 'llm')
        self.custom_translation_prompt = translation_settings.get('custom_translation_prompt', '')
        
        # ç¼“å­˜è®¾ç½®
        cache_settings = config.get('cache_settings', {})
        self.enable_cache = cache_settings.get('enable_cache', True)
        self.cache_expire_time = cache_settings.get('cache_expire_time', 1440)  # åˆ†é’Ÿ
        self.max_cache_size = cache_settings.get('max_cache_size', 100)
        
        # åˆå§‹åŒ–ç¼“å­˜
        self.cache = {}
        
        # å†…å®¹æå–è®¾ç½®
        content_extraction_settings = config.get('content_extraction_settings', {})
        self.enable_specific_extraction = content_extraction_settings.get('enable_specific_extraction', False)
        extract_types_text = content_extraction_settings.get('extract_types', 'title\ncontent')
        self.extract_types = [t.strip() for t in extract_types_text.split('\n') if t.strip()]
        
        self.analyzer = WebAnalyzer(self.max_content_length, self.timeout, self.user_agent)
    
    def _parse_domain_list(self, domain_text: str) -> List[str]:
        """è§£æåŸŸååˆ—è¡¨æ–‡æœ¬ä¸ºåˆ—è¡¨"""
        if not domain_text:
            return []
        domains = [domain.strip() for domain in domain_text.split('\n') if domain.strip()]
        return domains
    
    def _parse_group_list(self, group_text: str) -> List[str]:
        """è§£æç¾¤èŠåˆ—è¡¨æ–‡æœ¬ä¸ºåˆ—è¡¨"""
        if not group_text:
            return []
        groups = [group.strip() for group in group_text.split('\n') if group.strip()]
        return groups
    
    def _is_group_blacklisted(self, group_id: str) -> bool:
        """æ£€æŸ¥ç¾¤èŠæ˜¯å¦åœ¨é»‘åå•ä¸­"""
        if not group_id or not self.group_blacklist:
            return False
        return group_id in self.group_blacklist
    
    def _is_domain_allowed(self, url: str) -> bool:
        """æ£€æŸ¥åŸŸåæ˜¯å¦å…è®¸è®¿é—®"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # æ£€æŸ¥æ˜¯å¦åœ¨ç¦æ­¢åˆ—è¡¨ä¸­
            if self.blocked_domains:
                for blocked_domain in self.blocked_domains:
                    if blocked_domain.lower() in domain:
                        return False
            
            # æ£€æŸ¥æ˜¯å¦åœ¨å…è®¸åˆ—è¡¨ä¸­ï¼ˆå¦‚æœå…è®¸åˆ—è¡¨ä¸ä¸ºç©ºï¼‰
            if self.allowed_domains:
                for allowed_domain in self.allowed_domains:
                    if allowed_domain.lower() in domain:
                        return True
                return False  # å¦‚æœå…è®¸åˆ—è¡¨ä¸ä¸ºç©ºä½†åŸŸåä¸åœ¨å…¶ä¸­ï¼Œåˆ™ç¦æ­¢
            
            return True  # å¦‚æœå…è®¸åˆ—è¡¨ä¸ºç©ºï¼Œåˆ™å…è®¸æ‰€æœ‰åŸŸå
        except Exception:
            return False
    
    @filter.command("ç½‘é¡µåˆ†æ", alias={'åˆ†æ', 'æ€»ç»“'})
    async def analyze_webpage(self, event: AstrMessageEvent):
        """æ‰‹åŠ¨åˆ†ææŒ‡å®šç½‘é¡µé“¾æ¥"""
        message_text = event.message_str
        
        # æå–æ‰€æœ‰URL
        urls = self.analyzer.extract_urls(message_text)
        if not urls:
            yield event.plain_result("è¯·æä¾›è¦åˆ†æçš„ç½‘é¡µé“¾æ¥ï¼Œä¾‹å¦‚ï¼š/ç½‘é¡µåˆ†æ https://example.com")
            return
        
        # éªŒè¯URLå¹¶è¿‡æ»¤æ‰ä¸å…è®¸è®¿é—®çš„åŸŸå
        valid_urls = [url for url in urls if self.analyzer.is_valid_url(url)]
        if not valid_urls:
            yield event.plain_result("æ— æ•ˆçš„URLé“¾æ¥ï¼Œè¯·æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®")
            return
        
        allowed_urls = [url for url in valid_urls if self._is_domain_allowed(url)]
        if not allowed_urls:
            yield event.plain_result("æ‰€æœ‰åŸŸåéƒ½ä¸åœ¨å…è®¸è®¿é—®çš„åˆ—è¡¨ä¸­ï¼Œæˆ–å·²è¢«ç¦æ­¢è®¿é—®")
            return
        
        # å‘é€å¤„ç†æç¤º
        if len(allowed_urls) == 1:
            yield event.plain_result(f"æ­£åœ¨åˆ†æç½‘é¡µ: {allowed_urls[0]}")
        else:
            yield event.plain_result(f"æ­£åœ¨åˆ†æ{len(allowed_urls)}ä¸ªç½‘é¡µé“¾æ¥...")
        
        # æ‰¹é‡å¤„ç†æ‰€æœ‰URL
        async for result in self._batch_process_urls(event, allowed_urls):
            yield result
    
    @filter.event_message_type(filter.EventMessageType.ALL)
    async def auto_detect_urls(self, event: AstrMessageEvent):
        """è‡ªåŠ¨æ£€æµ‹æ¶ˆæ¯ä¸­çš„URLé“¾æ¥å¹¶è¿›è¡Œåˆ†æ"""
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨åˆ†æ
        if not self.auto_analyze:
            logger.info("è‡ªåŠ¨åˆ†æåŠŸèƒ½å·²ç¦ç”¨")
            return
        
        # æ£€æŸ¥ç¾¤èŠæ˜¯å¦åœ¨é»‘åå•ä¸­ï¼ˆä»…ç¾¤èŠæ¶ˆæ¯ï¼‰
        # å°è¯•ä»ä¸åŒä½ç½®è·å–ç¾¤èŠID
        group_id = None
        
        # æ–¹æ³•1ï¼šä»äº‹ä»¶å¯¹è±¡ç›´æ¥è·å–
        if hasattr(event, 'group_id') and event.group_id:
            group_id = event.group_id
        # æ–¹æ³•2ï¼šä»æ¶ˆæ¯å¯¹è±¡è·å–
        elif hasattr(event, 'message_obj') and hasattr(event.message_obj, 'group_id') and event.message_obj.group_id:
            group_id = event.message_obj.group_id
        # æ–¹æ³•3ï¼šä»åŸå§‹æ¶ˆæ¯è·å–
        elif hasattr(event, 'raw_message') and hasattr(event.raw_message, 'group_id') and event.raw_message.group_id:
            group_id = event.raw_message.group_id
        
        # ç¾¤èŠåœ¨é»‘åå•ä¸­æ—¶é™é»˜å¿½ç•¥
        if group_id and self._is_group_blacklisted(group_id):
            return  # ç¾¤èŠåœ¨é»‘åå•ä¸­ï¼Œé™é»˜å¿½ç•¥
            
        message_text = event.message_str
        
        # æå–URL
        urls = self.analyzer.extract_urls(message_text)
        if not urls:
            return  # æ²¡æœ‰URLï¼Œä¸å¤„ç†
        
        valid_urls = [url for url in urls if self.analyzer.is_valid_url(url)]
        if not valid_urls:
            return
        
        # è¿‡æ»¤æ‰ä¸å…è®¸è®¿é—®çš„åŸŸå
        allowed_urls = [url for url in valid_urls if self._is_domain_allowed(url)]
        if not allowed_urls:
            return  # æ²¡æœ‰å…è®¸è®¿é—®çš„URLï¼Œä¸å¤„ç†
        
        # å‘é€å¤„ç†æç¤º
        if len(allowed_urls) == 1:
            yield event.plain_result(f"æ£€æµ‹åˆ°ç½‘é¡µé“¾æ¥ï¼Œæ­£åœ¨åˆ†æ: {allowed_urls[0]}")
        else:
            yield event.plain_result(f"æ£€æµ‹åˆ°{len(allowed_urls)}ä¸ªç½‘é¡µé“¾æ¥ï¼Œæ­£åœ¨åˆ†æ...")
        
        # æ‰¹é‡å¤„ç†æ‰€æœ‰URL
        async for result in self._batch_process_urls(event, allowed_urls):
            yield result
    
    async def _batch_process_urls(self, event: AstrMessageEvent, urls: List[str]):
        """æ‰¹é‡å¤„ç†å¤šä¸ªURLï¼Œæ”¶é›†åˆ†æç»“æœå¹¶å‘é€"""
        # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
        analysis_results = []
        
        async with WebAnalyzer(self.max_content_length, self.timeout, self.user_agent) as analyzer:
            for url in urls:
                try:
                    # æ£€æŸ¥ç¼“å­˜
                    cached_result = self._check_cache(url)
                    if cached_result:
                        logger.info(f"ä½¿ç”¨ç¼“å­˜ç»“æœ: {url}")
                        analysis_results.append(cached_result)
                        continue
                    
                    # æŠ“å–ç½‘é¡µå†…å®¹
                    html = await analyzer.fetch_webpage(url)
                    if not html:
                        analysis_results.append({
                            'url': url,
                            'result': f"âŒ æ— æ³•æŠ“å–ç½‘é¡µå†…å®¹: {url}",
                            'screenshot': None
                        })
                        continue
                    
                    # æå–å†…å®¹
                    content_data = analyzer.extract_content(html, url)
                    if not content_data:
                        analysis_results.append({
                            'url': url,
                            'result': f"âŒ æ— æ³•è§£æç½‘é¡µå†…å®¹: {url}",
                            'screenshot': None
                        })
                        continue
                    
                    # ç¿»è¯‘å†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if self.enable_translation:
                        translated_content = await self._translate_content(event, content_data['content'])
                        # åˆ›å»ºç¿»è¯‘åçš„å†…å®¹æ•°æ®å‰¯æœ¬
                        translated_content_data = content_data.copy()
                        translated_content_data['content'] = translated_content
                        # è°ƒç”¨LLMè¿›è¡Œåˆ†æï¼ˆä½¿ç”¨ç¿»è¯‘åçš„å†…å®¹ï¼‰
                        analysis_result = await self.analyze_with_llm(event, translated_content_data)
                    else:
                        # ç›´æ¥è°ƒç”¨LLMè¿›è¡Œåˆ†æ
                        analysis_result = await self.analyze_with_llm(event, content_data)
                    
                    # æå–ç‰¹å®šç±»å‹å†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    specific_content = self._extract_specific_content(html, url)
                    if specific_content:
                        # åœ¨åˆ†æç»“æœä¸­æ·»åŠ ç‰¹å®šå†…å®¹
                        specific_content_str = "\n\n**ç‰¹å®šå†…å®¹æå–**\n"
                        
                        if 'images' in specific_content and specific_content['images']:
                            specific_content_str += f"\nğŸ“· å›¾ç‰‡é“¾æ¥ ({len(specific_content['images'])}):\n"
                            for img_url in specific_content['images']:
                                specific_content_str += f"- {img_url}\n"
                        
                        if 'links' in specific_content and specific_content['links']:
                            specific_content_str += f"\nğŸ”— ç›¸å…³é“¾æ¥ ({len(specific_content['links'])}):\n"
                            for link in specific_content['links'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé“¾æ¥
                                specific_content_str += f"- [{link['text']}]({link['url']})\n"
                        
                        if 'code_blocks' in specific_content and specific_content['code_blocks']:
                            specific_content_str += f"\nğŸ’» ä»£ç å— ({len(specific_content['code_blocks'])}):\n"
                            for i, code in enumerate(specific_content['code_blocks'][:2]):  # åªæ˜¾ç¤ºå‰2ä¸ªä»£ç å—
                                specific_content_str += f"```\n{code}\n```\n"
                        
                        # æ·»åŠ åˆ°åˆ†æç»“æœä¸­
                        analysis_result += specific_content_str
                    
                    # æ•è·æˆªå›¾
                    screenshot = None
                    if self.enable_screenshot:
                        screenshot = await analyzer.capture_screenshot(
                            url,
                            quality=self.screenshot_quality,
                            width=self.screenshot_width,
                            full_page=self.screenshot_full_page,
                            wait_time=self.screenshot_wait_time
                        )
                    
                    # å‡†å¤‡ç»“æœæ•°æ®
                    result_data = {
                        'url': url,
                        'result': analysis_result,
                        'screenshot': screenshot
                    }
                    
                    # æ›´æ–°ç¼“å­˜
                    self._update_cache(url, result_data)
                    
                    analysis_results.append(result_data)
                except Exception as e:
                    logger.error(f"å¤„ç†URL {url} æ—¶å‡ºé”™: {e}")
                    analysis_results.append({
                        'url': url,
                        'result': f"âŒ å¤„ç†URLæ—¶å‡ºé”™: {url}\né”™è¯¯ä¿¡æ¯: {str(e)}",
                        'screenshot': None
                    })
        
        # å‘é€æ‰€æœ‰åˆ†æç»“æœ
        async for result in self._send_analysis_result(event, analysis_results):
            yield result
    
    async def analyze_with_llm(self, event: AstrMessageEvent, content_data: dict) -> str:
        """è°ƒç”¨LLMè¿›è¡Œå†…å®¹åˆ†æå’Œæ€»ç»“"""
        try:
            title = content_data['title']
            content = content_data['content']
            url = content_data['url']
            
            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨å’Œå¯ç”¨
            if not hasattr(self.context, 'llm_generate') or not self.llm_enabled:
                return self.get_enhanced_analysis(content_data)
            
            # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„LLMæä¾›å•†ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨å½“å‰ä¼šè¯çš„æ¨¡å‹
            provider_id = self.llm_provider
            if not provider_id:
                umo = event.unified_msg_origin
                provider_id = await self.context.get_current_chat_provider_id(umo=umo)
            
            if not provider_id:
                return self.get_enhanced_analysis(content_data)
            
            # æ„å»ºä¼˜åŒ–çš„LLMæç¤ºè¯
            emoji_prefix = "æ¯ä¸ªè¦ç‚¹ç”¨emojiå›¾æ ‡æ ‡è®°" if self.enable_emoji else ""
            
            # ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯æˆ–é»˜è®¤æç¤ºè¯
            if self.custom_prompt:
                # æ›¿æ¢è‡ªå®šä¹‰æç¤ºè¯ä¸­çš„å˜é‡
                prompt = self.custom_prompt.format(
                    title=title,
                    url=url,
                    content=content,
                    max_length=self.max_summary_length
                )
            else:
                # é»˜è®¤æç¤ºè¯
                prompt = f"""è¯·å¯¹ä»¥ä¸‹ç½‘é¡µå†…å®¹è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{title}
- é“¾æ¥ï¼š{url}

**ç½‘é¡µå†…å®¹**ï¼š
{content}

**åˆ†æè¦æ±‚**ï¼š
1. **æ ¸å¿ƒæ‘˜è¦**ï¼šç”¨50-100å­—æ¦‚æ‹¬ç½‘é¡µçš„æ ¸å¿ƒå†…å®¹å’Œä¸»æ—¨
2. **å…³é”®è¦ç‚¹**ï¼šæå–2-3ä¸ªæœ€é‡è¦çš„ä¿¡æ¯ç‚¹æˆ–è§‚ç‚¹
3. **å†…å®¹ç±»å‹**ï¼šåˆ¤æ–­ç½‘é¡µå±äºä»€ä¹ˆç±»å‹ï¼ˆæ–°é—»ã€æ•™ç¨‹ã€åšå®¢ã€äº§å“ä»‹ç»ç­‰ï¼‰
4. **ä»·å€¼è¯„ä¼°**ï¼šç®€è¦è¯„ä»·å†…å®¹çš„ä»·å€¼å’Œå®ç”¨æ€§
5. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜é€‚åˆå“ªäº›äººç¾¤é˜…è¯»

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{self.max_summary_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚"""
            
            # ä½¿ç”¨å½“å‰ä¼šè¯çš„èŠå¤©æ¨¡å‹IDè°ƒç”¨å¤§æ¨¡å‹
            llm_resp = await self.context.llm_generate(
                chat_provider_id=provider_id,  # ä½¿ç”¨å½“å‰ä¼šè¯çš„èŠå¤©æ¨¡å‹
                prompt=prompt
            )
            
            if llm_resp and llm_resp.completion_text:
                # ç¾åŒ–LLMè¿”å›çš„ç»“æœ
                analysis_text = llm_resp.completion_text.strip()
                
                # é™åˆ¶æ‘˜è¦é•¿åº¦
                if len(analysis_text) > self.max_summary_length:
                    analysis_text = analysis_text[:self.max_summary_length] + "..."
                
                # æ·»åŠ æ ‡é¢˜å’Œæ ¼å¼ç¾åŒ–
                link_emoji = "ğŸ”—" if self.enable_emoji else ""
                title_emoji = "ğŸ“" if self.enable_emoji else ""
                
                formatted_result = "**AIæ™ºèƒ½ç½‘é¡µåˆ†ææŠ¥å‘Š**\n\n"
                formatted_result += f"{link_emoji} **åˆ†æé“¾æ¥**: {url}\n"
                formatted_result += f"{title_emoji} **ç½‘é¡µæ ‡é¢˜**: {title}\n\n"
                formatted_result += "---\n\n"
                formatted_result += analysis_text
                formatted_result += "\n\n---\n"
                formatted_result += "*åˆ†æå®Œæˆï¼Œå¸Œæœ›å¯¹æ‚¨æœ‰å¸®åŠ©ï¼*"
                
                return formatted_result
            else:
                return self.get_enhanced_analysis(content_data)
                
        except Exception as e:
            logger.error(f"LLMåˆ†æå¤±è´¥: {e}")
            # å¦‚æœLLMåˆ†æå¤±è´¥ï¼Œç›´æ¥è¿”å›é”™è¯¯ä¿¡æ¯
            return f"âŒ LLMåˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
    
    def get_enhanced_analysis(self, content_data: dict) -> str:
        """å¢å¼ºç‰ˆåŸºç¡€åˆ†æï¼ˆLLMä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰"""
        title = content_data['title']
        content = content_data['content']
        url = content_data['url']
        
        # è¯¦ç»†çš„å†…å®¹ç»Ÿè®¡
        char_count = len(content)
        word_count = len(content.split())
        
        # æ™ºèƒ½å†…å®¹ç±»å‹æ£€æµ‹
        content_lower = content.lower()
        content_type = "æ–‡ç« "
        if any(keyword in content_lower for keyword in ['æ–°é—»', 'æŠ¥é“', 'æ¶ˆæ¯', 'æ—¶äº‹']):
            content_type = "æ–°é—»èµ„è®¯"
        elif any(keyword in content_lower for keyword in ['æ•™ç¨‹', 'æŒ‡å—', 'æ•™å­¦', 'æ­¥éª¤', 'æ–¹æ³•']):
            content_type = "æ•™ç¨‹æŒ‡å—"
        elif any(keyword in content_lower for keyword in ['åšå®¢', 'éšç¬”', 'æ—¥è®°', 'ä¸ªäºº', 'è§‚ç‚¹']):
            content_type = "ä¸ªäººåšå®¢"
        elif any(keyword in content_lower for keyword in ['äº§å“', 'æœåŠ¡', 'è´­ä¹°', 'ä»·æ ¼', 'ä¼˜æƒ ']):
            content_type = "äº§å“ä»‹ç»"
        elif any(keyword in content_lower for keyword in ['æŠ€æœ¯', 'å¼€å‘', 'ç¼–ç¨‹', 'ä»£ç ', 'API']):
            content_type = "æŠ€æœ¯æ–‡æ¡£"
        
        # æå–å…³é”®å¥å­ï¼ˆå‰3ä¸ªéç©ºæ®µè½ï¼‰
        paragraphs = [p.strip() for p in content.split('\n') if p.strip()]
        key_sentences = paragraphs[:3]
        
        # æ£€æµ‹å†…å®¹è´¨é‡
        quality_indicator = "å†…å®¹ä¸°å¯Œ" if char_count > 1000 else "å†…å®¹ç®€æ´"
        if char_count > 5000:
            quality_indicator = "å†…å®¹è¯¦å®"
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨emoji
        robot_emoji = "ğŸ¤–" if self.enable_emoji else ""
        page_emoji = "ğŸ“„" if self.enable_emoji else ""
        info_emoji = "ğŸ“" if self.enable_emoji else ""
        stats_emoji = "ğŸ“Š" if self.enable_emoji else ""
        search_emoji = "ğŸ”" if self.enable_emoji else ""
        light_emoji = "ğŸ’¡" if self.enable_emoji else ""
        
        result = f"{robot_emoji} **æ™ºèƒ½ç½‘é¡µåˆ†æ** {page_emoji}\n\n"
        
        if self.enable_emoji:
            result += f"**{info_emoji} åŸºæœ¬ä¿¡æ¯**\n"
        else:
            result += "**åŸºæœ¬ä¿¡æ¯**\n"
        result += f"- **æ ‡é¢˜**: {title}\n"
        result += f"- **é“¾æ¥**: {url}\n"
        result += f"- **å†…å®¹ç±»å‹**: {content_type}\n"
        result += f"- **è´¨é‡è¯„ä¼°**: {quality_indicator}\n\n"
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if self.enable_statistics:
            if self.enable_emoji:
                result += f"**{stats_emoji} å†…å®¹ç»Ÿè®¡**\n"
            else:
                result += "**å†…å®¹ç»Ÿè®¡**\n"
            result += f"- å­—ç¬¦æ•°: {char_count:,}\n"
            result += f"- æ®µè½æ•°: {len(paragraphs)}\n"
            result += f"- è¯æ•°: {word_count:,}\n\n"
        
        if self.enable_emoji:
            result += f"**{search_emoji} å†…å®¹æ‘˜è¦**\n"
        else:
            result += "**å†…å®¹æ‘˜è¦**\n"
        result += f"{chr(10).join(['â€¢ ' + sentence[:100] + ('...' if len(sentence) > 100 else '') for sentence in key_sentences])}\n\n"
        
        if self.enable_emoji:
            result += f"**{light_emoji} åˆ†æè¯´æ˜**\n"
        else:
            result += "**åˆ†æè¯´æ˜**\n"
        result += "æ­¤åˆ†æåŸºäºç½‘é¡µå†…å®¹æå–ï¼Œå¦‚éœ€æ›´æ·±å…¥çš„AIæ™ºèƒ½åˆ†æï¼Œè¯·ç¡®ä¿AstrBotå·²æ­£ç¡®é…ç½®LLMåŠŸèƒ½ã€‚\n\n"
        result += "*æç¤ºï¼šå®Œæ•´å†…å®¹é¢„è§ˆè¯·æŸ¥çœ‹åŸå§‹ç½‘é¡µ*"
        
        return result
    
    @filter.command("web_config", alias={'ç½‘é¡µåˆ†æé…ç½®', 'ç½‘é¡µåˆ†æè®¾ç½®'})
    async def show_config(self, event: AstrMessageEvent):
        """æ˜¾ç¤ºå½“å‰æ’ä»¶é…ç½®"""
        config_info = f"""**ç½‘é¡µåˆ†ææ’ä»¶é…ç½®ä¿¡æ¯**

**åŸºæœ¬è®¾ç½®**
- æœ€å¤§å†…å®¹é•¿åº¦: {self.max_content_length} å­—ç¬¦
- è¯·æ±‚è¶…æ—¶æ—¶é—´: {self.timeout} ç§’
- LLMæ™ºèƒ½åˆ†æ: {'âœ… å·²å¯ç”¨' if self.llm_enabled else 'âŒ å·²ç¦ç”¨'}
- è‡ªåŠ¨åˆ†æé“¾æ¥: {'âœ… å·²å¯ç”¨' if self.auto_analyze else 'âŒ å·²ç¦ç”¨'}
- åˆå¹¶è½¬å‘åŠŸèƒ½: {'âœ… å·²å¯ç”¨' if self.merge_forward_enabled else 'âŒ å·²ç¦ç”¨'}

**åŸŸåæ§åˆ¶**
- å…è®¸åŸŸå: {len(self.allowed_domains)} ä¸ª
- ç¦æ­¢åŸŸå: {len(self.blocked_domains)} ä¸ª

**ç¾¤èŠæ§åˆ¶**
- ç¾¤èŠé»‘åå•: {len(self.group_blacklist)} ä¸ªç¾¤èŠ

**åˆ†æè®¾ç½®**
- å¯ç”¨emoji: {'âœ… å·²å¯ç”¨' if self.enable_emoji else 'âŒ å·²ç¦ç”¨'}
- æ˜¾ç¤ºç»Ÿè®¡: {'âœ… å·²å¯ç”¨' if self.enable_statistics else 'âŒ å·²ç¦ç”¨'}
- æœ€å¤§æ‘˜è¦é•¿åº¦: {self.max_summary_length} å­—ç¬¦
- å¯ç”¨æˆªå›¾: {'âœ… å·²å¯ç”¨' if self.enable_screenshot else 'âŒ å·²ç¦ç”¨'}
- æˆªå›¾è´¨é‡: {self.screenshot_quality}
- æˆªå›¾å®½åº¦: {self.screenshot_width}px
- æˆªå–æ•´é¡µ: {'âœ… å·²å¯ç”¨' if self.screenshot_full_page else 'âŒ å·²ç¦ç”¨'}
- æˆªå›¾ç­‰å¾…æ—¶é—´: {self.screenshot_wait_time}ms

**LLMé…ç½®**
- æŒ‡å®šæä¾›å•†: {self.llm_provider if self.llm_provider else 'ä½¿ç”¨ä¼šè¯é»˜è®¤'}
- è‡ªå®šä¹‰æç¤ºè¯: {'âœ… å·²å¯ç”¨' if self.custom_prompt else 'âŒ æœªè®¾ç½®'}

**ç¿»è¯‘è®¾ç½®**
- å¯ç”¨ç½‘é¡µç¿»è¯‘: {'âœ… å·²å¯ç”¨' if self.enable_translation else 'âŒ å·²ç¦ç”¨'}
- ç›®æ ‡è¯­è¨€: {self.target_language}
- ç¿»è¯‘æä¾›å•†: {self.translation_provider}
- è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯: {'âœ… å·²å¯ç”¨' if self.custom_translation_prompt else 'âŒ æœªè®¾ç½®'}

**ç¼“å­˜è®¾ç½®**
- å¯ç”¨ç»“æœç¼“å­˜: {'âœ… å·²å¯ç”¨' if self.enable_cache else 'âŒ å·²ç¦ç”¨'}
- ç¼“å­˜è¿‡æœŸæ—¶é—´: {self.cache_expire_time} åˆ†é’Ÿ
- æœ€å¤§ç¼“å­˜æ•°é‡: {self.max_cache_size} ä¸ª

**å†…å®¹æå–è®¾ç½®**
- å¯ç”¨ç‰¹å®šå†…å®¹æå–: {'âœ… å·²å¯ç”¨' if self.enable_specific_extraction else 'âŒ å·²ç¦ç”¨'}
- æå–å†…å®¹ç±»å‹: {', '.join(self.extract_types)}

*æç¤º: å¦‚éœ€ä¿®æ”¹é…ç½®ï¼Œè¯·åœ¨AstrBotç®¡ç†é¢æ¿ä¸­ç¼–è¾‘æ’ä»¶é…ç½®*"""
        
        yield event.plain_result(config_info)
    
    @filter.command("test_merge", alias={'æµ‹è¯•åˆå¹¶è½¬å‘', 'æµ‹è¯•è½¬å‘'})
    async def test_merge_forward(self, event: AstrMessageEvent):
        '''æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½'''
        from astrbot.api.message_components import Node, Plain, Nodes
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç¾¤èŠæ¶ˆæ¯
        group_id = None
        if hasattr(event, 'group_id') and event.group_id:
            group_id = event.group_id
        elif hasattr(event, 'message_obj') and hasattr(event.message_obj, 'group_id') and event.message_obj.group_id:
            group_id = event.message_obj.group_id
        
        if group_id:
            # åˆ›å»ºæµ‹è¯•ç”¨çš„åˆå¹¶è½¬å‘èŠ‚ç‚¹
            nodes = []
            
            # æ ‡é¢˜èŠ‚ç‚¹
            title_node = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•åˆå¹¶è½¬å‘",
                content=[
                    Plain("è¿™æ˜¯åˆå¹¶è½¬å‘æµ‹è¯•æ¶ˆæ¯")
                ]
            )
            nodes.append(title_node)
            
            # å†…å®¹èŠ‚ç‚¹1
            content_node1 = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•èŠ‚ç‚¹1",
                content=[
                    Plain("è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•èŠ‚ç‚¹å†…å®¹")
                ]
            )
            nodes.append(content_node1)
            
            # å†…å®¹èŠ‚ç‚¹2
            content_node2 = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•èŠ‚ç‚¹2",
                content=[
                    Plain("è¿™æ˜¯ç¬¬äºŒä¸ªæµ‹è¯•èŠ‚ç‚¹å†…å®¹")
                ]
            )
            nodes.append(content_node2)
            
            # ä½¿ç”¨NodesåŒ…è£…æ‰€æœ‰èŠ‚ç‚¹ï¼Œåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
            merge_forward_message = Nodes(nodes)
            yield event.chain_result([merge_forward_message])
            logger.info(f"æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½ï¼Œç¾¤èŠ {group_id}")
        else:
            yield event.plain_result("åˆå¹¶è½¬å‘åŠŸèƒ½ä»…æ”¯æŒç¾¤èŠæ¶ˆæ¯æµ‹è¯•")
            logger.info("ç§èŠæ¶ˆæ¯æ— æ³•æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½")
    
    @filter.command("group_blacklist", alias={'ç¾¤é»‘åå•', 'é»‘åå•'})
    async def manage_group_blacklist(self, event: AstrMessageEvent):
        """ç®¡ç†ç¾¤èŠé»‘åå•"""
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()
        
        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰é»‘åå•
        if len(message_parts) <= 1:
            if not self.group_blacklist:
                yield event.plain_result("å½“å‰ç¾¤èŠé»‘åå•ä¸ºç©º")
                return
            
            blacklist_info = "**å½“å‰ç¾¤èŠé»‘åå•**\n\n"
            for i, group_id in enumerate(self.group_blacklist, 1):
                blacklist_info += f"{i}. {group_id}\n"
            
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist add <ç¾¤å·>` æ·»åŠ ç¾¤èŠåˆ°é»‘åå•"
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist remove <ç¾¤å·>` ä»é»‘åå•ç§»é™¤ç¾¤èŠ"
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist clear` æ¸…ç©ºé»‘åå•"
            
            yield event.plain_result(blacklist_info)
            return
        
        action = message_parts[1].lower() if len(message_parts) > 1 else ""
        group_id = message_parts[2] if len(message_parts) > 2 else ""
        
        if action == "add" and group_id:
            # æ·»åŠ ç¾¤èŠåˆ°é»‘åå•
            if group_id in self.group_blacklist:
                yield event.plain_result(f"ç¾¤èŠ {group_id} å·²åœ¨é»‘åå•ä¸­")
                return
            
            self.group_blacklist.append(group_id)
            self._save_group_blacklist()
            yield event.plain_result(f"âœ… å·²æ·»åŠ ç¾¤èŠ {group_id} åˆ°é»‘åå•")
            
        elif action == "remove" and group_id:
            # ä»é»‘åå•ç§»é™¤ç¾¤èŠ
            if group_id not in self.group_blacklist:
                yield event.plain_result(f"ç¾¤èŠ {group_id} ä¸åœ¨é»‘åå•ä¸­")
                return
            
            self.group_blacklist.remove(group_id)
            self._save_group_blacklist()
            yield event.plain_result(f"âœ… å·²ä»é»‘åå•ç§»é™¤ç¾¤èŠ {group_id}")
            
        elif action == "clear":
            # æ¸…ç©ºé»‘åå•
            if not self.group_blacklist:
                yield event.plain_result("é»‘åå•å·²ä¸ºç©º")
                return
            
            self.group_blacklist.clear()
            self._save_group_blacklist()
            yield event.plain_result("âœ… å·²æ¸…ç©ºç¾¤èŠé»‘åå•")
            
        else:
            yield event.plain_result("æ— æ•ˆçš„æ“ä½œï¼Œè¯·ä½¿ç”¨: add <ç¾¤å·>, remove <ç¾¤å·>, clear")
    
    @filter.command("web_cache", alias={'ç½‘é¡µç¼“å­˜', 'æ¸…ç†ç¼“å­˜'})
    async def manage_cache(self, event: AstrMessageEvent):
        """ç®¡ç†ç¼“å­˜"""
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()
        
        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰ç¼“å­˜çŠ¶æ€
        if len(message_parts) <= 1:
            cache_count = len(self.cache)
            cache_info = f"**å½“å‰ç¼“å­˜çŠ¶æ€**\n\n"
            cache_info += f"- ç¼“å­˜æ•°é‡: {cache_count} ä¸ª\n"
            cache_info += f"- ç¼“å­˜è¿‡æœŸæ—¶é—´: {self.cache_expire_time} åˆ†é’Ÿ\n"
            cache_info += f"- æœ€å¤§ç¼“å­˜æ•°é‡: {self.max_cache_size} ä¸ª\n"
            cache_info += f"- ç¼“å­˜åŠŸèƒ½: {'âœ… å·²å¯ç”¨' if self.enable_cache else 'âŒ å·²ç¦ç”¨'}\n"
            
            cache_info += "\nä½¿ç”¨ `/web_cache clear` æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"
            
            yield event.plain_result(cache_info)
            return
        
        action = message_parts[1].lower() if len(message_parts) > 1 else ""
        
        if action == "clear":
            # æ¸…ç©ºç¼“å­˜
            if not self.cache:
                yield event.plain_result("ç¼“å­˜å·²ä¸ºç©º")
                return
            
            self.cache.clear()
            yield event.plain_result(f"âœ… å·²æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ï¼Œå…±æ¸…ç†äº† {len(self.cache)} ä¸ªç¼“å­˜é¡¹")
            
        else:
            yield event.plain_result("æ— æ•ˆçš„æ“ä½œï¼Œè¯·ä½¿ç”¨: clear")
    
    def _save_group_blacklist(self):
        """ä¿å­˜ç¾¤èŠé»‘åå•åˆ°é…ç½®"""
        try:
            # å°†ç¾¤èŠåˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
            group_text = '\n'.join(self.group_blacklist)
            # æ›´æ–°é…ç½®å¹¶ä¿å­˜
            self.config['group_blacklist'] = group_text
            self.config.save_config()
        except Exception as e:
            logger.error(f"ä¿å­˜ç¾¤èŠé»‘åå•å¤±è´¥: {e}")
    
    def _check_cache(self, url: str) -> dict:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ"""
        if not self.enable_cache:
            return None
        
        import time
        current_time = time.time()
        
        if url in self.cache:
            cache_data = self.cache[url]
            if current_time - cache_data['timestamp'] < self.cache_expire_time * 60:
                return cache_data['result']
            else:
                # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                del self.cache[url]
        
        return None
    
    def _update_cache(self, url: str, result: dict):
        """æ›´æ–°ç¼“å­˜"""
        if not self.enable_cache:
            return
        
        import time
        current_time = time.time()
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self._clean_cache()
        
        # æ£€æŸ¥ç¼“å­˜å¤§å°
        if len(self.cache) >= self.max_cache_size:
            # åˆ é™¤æœ€æ—§çš„ç¼“å­˜
            oldest_url = min(self.cache, key=lambda k: self.cache[k]['timestamp'])
            del self.cache[oldest_url]
        
        # æ·»åŠ æ–°ç¼“å­˜
        self.cache[url] = {
            'timestamp': current_time,
            'result': result
        }
    
    def _clean_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        import time
        current_time = time.time()
        
        expired_urls = []
        for url, cache_data in self.cache.items():
            if current_time - cache_data['timestamp'] >= self.cache_expire_time * 60:
                expired_urls.append(url)
        
        for url in expired_urls:
            del self.cache[url]
    
    async def _translate_content(self, event: AstrMessageEvent, content: str) -> str:
        """ç¿»è¯‘ç½‘é¡µå†…å®¹"""
        if not self.enable_translation:
            return content
        
        try:
            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨
            if not hasattr(self.context, 'llm_generate'):
                logger.error("LLMä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘")
                return content
            
            # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„LLMæä¾›å•†ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨å½“å‰ä¼šè¯çš„æ¨¡å‹
            provider_id = self.llm_provider
            if not provider_id:
                umo = event.unified_msg_origin
                provider_id = await self.context.get_current_chat_provider_id(umo=umo)
            
            if not provider_id:
                logger.error("æ— æ³•è·å–LLMæä¾›å•†IDï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘")
                return content
            
            # ä½¿ç”¨è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯æˆ–é»˜è®¤æç¤ºè¯
            if self.custom_translation_prompt:
                # æ›¿æ¢è‡ªå®šä¹‰æç¤ºè¯ä¸­çš„å˜é‡
                prompt = self.custom_translation_prompt.format(
                    content=content,
                    target_language=self.target_language
                )
            else:
                # é»˜è®¤ç¿»è¯‘æç¤ºè¯
                prompt = f"è¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆ{self.target_language}è¯­è¨€ï¼Œä¿æŒåŸæ–‡æ„æ€ä¸å˜ï¼Œè¯­è¨€æµç•…è‡ªç„¶ï¼š\n\n{content}"
            
            # è°ƒç”¨LLMè¿›è¡Œç¿»è¯‘
            llm_resp = await self.context.llm_generate(
                chat_provider_id=provider_id,
                prompt=prompt
            )
            
            if llm_resp and llm_resp.completion_text:
                return llm_resp.completion_text.strip()
            else:
                logger.error("LLMç¿»è¯‘è¿”å›ä¸ºç©º")
                return content
        except Exception as e:
            logger.error(f"ç¿»è¯‘å†…å®¹å¤±è´¥: {e}")
            return content
    
    def _extract_specific_content(self, html: str, url: str) -> dict:
        """æå–ç‰¹å®šç±»å‹çš„å†…å®¹"""
        if not self.enable_specific_extraction:
            return {}
        
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'lxml')
            
            extracted_content = {}
            
            # æå–æ ‡é¢˜
            if 'title' in self.extract_types:
                title = soup.find('title')
                extracted_content['title'] = title.get_text().strip() if title else "æ— æ ‡é¢˜"
            
            # æå–æ­£æ–‡å†…å®¹
            if 'content' in self.extract_types:
                content_selectors = [
                    'article',
                    'main',
                    '.article-content',
                    '.post-content',
                    '.content',
                    'body'
                ]
                
                content_text = ""
                for selector in content_selectors:
                    element = soup.select_one(selector)
                    if element:
                        for script in element(['script', 'style']):
                            script.decompose()
                        text = element.get_text(separator='\n', strip=True)
                        if len(text) > len(content_text):
                            content_text = text
                
                if len(content_text) > self.max_content_length:
                    content_text = content_text[:self.max_content_length] + "..."
                
                extracted_content['content'] = content_text
            
            # æå–å›¾ç‰‡é“¾æ¥
            if 'images' in self.extract_types:
                images = []
                for img in soup.find_all('img'):
                    src = img.get('src')
                    if src:
                        # å¤„ç†ç›¸å¯¹è·¯å¾„
                        from urllib.parse import urljoin
                        full_url = urljoin(url, src)
                        images.append(full_url)
                extracted_content['images'] = images[:10]  # æœ€å¤šæå–10å¼ å›¾ç‰‡
            
            # æå–é“¾æ¥
            if 'links' in self.extract_types:
                links = []
                for a in soup.find_all('a', href=True):
                    href = a.get('href')
                    if href and not href.startswith('#'):
                        from urllib.parse import urljoin
                        full_url = urljoin(url, href)
                        links.append({
                            'text': a.get_text().strip() or full_url,
                            'url': full_url
                        })
                extracted_content['links'] = links[:20]  # æœ€å¤šæå–20ä¸ªé“¾æ¥
            
            # æå–è¡¨æ ¼
            if 'tables' in self.extract_types:
                tables = []
                for table in soup.find_all('table'):
                    table_data = []
                    # æå–è¡¨å¤´
                    headers = []
                    thead = table.find('thead')
                    if thead:
                        for th in thead.find_all('th'):
                            headers.append(th.get_text().strip())
                    
                    # æå–è¡¨ä½“
                    tbody = table.find('tbody') or table
                    for row in tbody.find_all('tr'):
                        row_data = []
                        for cell in row.find_all(['td', 'th']):
                            row_data.append(cell.get_text().strip())
                        if row_data:
                            table_data.append(row_data)
                    
                    if table_data:
                        tables.append({
                            'headers': headers,
                            'rows': table_data
                        })
                extracted_content['tables'] = tables[:5]  # æœ€å¤šæå–5ä¸ªè¡¨æ ¼
            
            # æå–åˆ—è¡¨
            if 'lists' in self.extract_types:
                lists = []
                # æå–æ— åºåˆ—è¡¨
                for ul in soup.find_all('ul'):
                    list_items = []
                    for li in ul.find_all('li'):
                        list_items.append(li.get_text().strip())
                    if list_items:
                        lists.append({
                            'type': 'ul',
                            'items': list_items[:20]  # æ¯ä¸ªåˆ—è¡¨æœ€å¤šæå–20é¡¹
                        })
                
                # æå–æœ‰åºåˆ—è¡¨
                for ol in soup.find_all('ol'):
                    list_items = []
                    for li in ol.find_all('li'):
                        list_items.append(li.get_text().strip())
                    if list_items:
                        lists.append({
                            'type': 'ol',
                            'items': list_items[:20]  # æ¯ä¸ªåˆ—è¡¨æœ€å¤šæå–20é¡¹
                        })
                extracted_content['lists'] = lists[:10]  # æœ€å¤šæå–10ä¸ªåˆ—è¡¨
            
            # æå–ä»£ç å—
            if 'code' in self.extract_types:
                code_blocks = []
                for code in soup.find_all(['pre', 'code']):
                    code_text = code.get_text().strip()
                    if code_text and len(code_text) > 10:
                        code_blocks.append(code_text[:1000] + "..." if len(code_text) > 1000 else code_text)
                extracted_content['code_blocks'] = code_blocks[:5]  # æœ€å¤šæå–5ä¸ªä»£ç å—
            
            return extracted_content
        except Exception as e:
            logger.error(f"æå–ç‰¹å®šå†…å®¹å¤±è´¥: {e}")
            return {}
    
    async def _send_analysis_result(self, event, analysis_results):
        '''å‘é€åˆ†æç»“æœï¼Œæ ¹æ®å¼€å…³å†³å®šæ˜¯å¦ä½¿ç”¨åˆå¹¶è½¬å‘'''
        from astrbot.api.message_components import Node, Plain, Nodes, Image
        import tempfile
        import os
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç¾¤èŠæ¶ˆæ¯ä¸”åˆå¹¶è½¬å‘åŠŸèƒ½å·²å¯ç”¨
        group_id = None
        if hasattr(event, 'group_id') and event.group_id:
            group_id = event.group_id
        elif hasattr(event, 'message_obj') and hasattr(event.message_obj, 'group_id') and event.message_obj.group_id:
            group_id = event.message_obj.group_id
        
        # å¦‚æœæ˜¯ç¾¤èŠæ¶ˆæ¯ä¸”åˆå¹¶è½¬å‘åŠŸèƒ½å·²å¯ç”¨ï¼Œä½¿ç”¨åˆå¹¶è½¬å‘
        if group_id and self.merge_forward_enabled:
            # ä½¿ç”¨åˆå¹¶è½¬å‘ - å°†æ‰€æœ‰åˆ†æç»“æœåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
            nodes = []
            
            # æ·»åŠ æ€»æ ‡é¢˜èŠ‚ç‚¹
            total_title_node = Node(
                uin=event.get_sender_id(),
                name="ç½‘é¡µåˆ†æç»“æœæ±‡æ€»",
                content=[
                    Plain(f"å…±{len(analysis_results)}ä¸ªç½‘é¡µåˆ†æç»“æœ")
                ]
            )
            nodes.append(total_title_node)
            
            # ä¸ºæ¯ä¸ªURLæ·»åŠ åˆ†æç»“æœèŠ‚ç‚¹
            for i, result_data in enumerate(analysis_results, 1):
                url = result_data['url']
                analysis_result = result_data['result']
                screenshot = result_data['screenshot']
                
                # æ·»åŠ å½“å‰URLçš„æ ‡é¢˜èŠ‚ç‚¹
                url_title_node = Node(
                    uin=event.get_sender_id(),
                    name=f"åˆ†æç»“æœ {i}",
                    content=[
                        Plain(f"ç¬¬{i}ä¸ªç½‘é¡µåˆ†æç»“æœ - {url}")
                    ]
                )
                nodes.append(url_title_node)
                
                # æ·»åŠ å½“å‰URLçš„å†…å®¹èŠ‚ç‚¹
                content_node = Node(
                    uin=event.get_sender_id(),
                    name="è¯¦ç»†åˆ†æ",
                    content=[
                        Plain(analysis_result)
                    ]
                )
                nodes.append(content_node)
            
            # ä½¿ç”¨NodesåŒ…è£…æ‰€æœ‰èŠ‚ç‚¹ï¼Œåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
            merge_forward_message = Nodes(nodes)
            
            # å‘é€åˆå¹¶è½¬å‘æ¶ˆæ¯
            yield event.chain_result([merge_forward_message])
            
            # å¦‚æœæœ‰æˆªå›¾ï¼Œé€ä¸ªå‘é€æˆªå›¾
            for result_data in analysis_results:
                screenshot = result_data['screenshot']
                if screenshot:
                    try:
                        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æˆªå›¾
                        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as temp_file:
                            temp_file.write(screenshot)
                            temp_file_path = temp_file.name
                        
                        # ä½¿ç”¨Image.fromFileSystem()æ–¹æ³•å‘é€å›¾ç‰‡
                        image_component = Image.fromFileSystem(temp_file_path)
                        yield event.chain_result([image_component])
                        logger.info(f"ç¾¤èŠ {group_id} ä½¿ç”¨åˆå¹¶è½¬å‘å‘é€åˆ†æç»“æœï¼Œå¹¶å‘é€æˆªå›¾")
                        
                        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                        os.unlink(temp_file_path)
                    except Exception as e:
                        logger.error(f"å‘é€æˆªå›¾å¤±è´¥: {e}")
                        # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
                        if 'temp_file_path' in locals() and os.path.exists(temp_file_path):
                            os.unlink(temp_file_path)
            logger.info(f"ç¾¤èŠ {group_id} ä½¿ç”¨åˆå¹¶è½¬å‘å‘é€{len(analysis_results)}ä¸ªåˆ†æç»“æœ")
        else:
            # æ™®é€šå‘é€ - é€ä¸ªå‘é€åˆ†æç»“æœ
            for i, result_data in enumerate(analysis_results, 1):
                url = result_data['url']
                analysis_result = result_data['result']
                screenshot = result_data['screenshot']
                
                # å‘é€åˆ†æç»“æœæ–‡æœ¬
                if len(analysis_results) == 1:
                    result_text = f"ç½‘é¡µåˆ†æç»“æœï¼š\n{analysis_result}"
                else:
                    result_text = f"ç¬¬{i}/{len(analysis_results)}ä¸ªç½‘é¡µåˆ†æç»“æœï¼š\n{analysis_result}"
                yield event.plain_result(result_text)
                
                # å¦‚æœæœ‰æˆªå›¾ï¼Œå‘é€æˆªå›¾
                if screenshot:
                    try:
                        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æˆªå›¾
                        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as temp_file:
                            temp_file.write(screenshot)
                            temp_file_path = temp_file.name
                        
                        # ä½¿ç”¨Image.fromFileSystem()æ–¹æ³•å‘é€å›¾ç‰‡
                        image_component = Image.fromFileSystem(temp_file_path)
                        yield event.chain_result([image_component])
                        logger.info("æ™®é€šå‘é€åˆ†æç»“æœï¼Œå¹¶å‘é€æˆªå›¾")
                        
                        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                        os.unlink(temp_file_path)
                    except Exception as e:
                        logger.error(f"å‘é€æˆªå›¾å¤±è´¥: {e}")
                        # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
                        if 'temp_file_path' in locals() and os.path.exists(temp_file_path):
                            os.unlink(temp_file_path)
            message_type = "ç¾¤èŠ" if group_id else "ç§èŠ"
            logger.info(f"{message_type}æ¶ˆæ¯æ™®é€šå‘é€{len(analysis_results)}ä¸ªåˆ†æç»“æœ")
    
    async def terminate(self):
        """æ’ä»¶å¸è½½æ—¶çš„æ¸…ç†å·¥ä½œ"""
        logger.info("ç½‘é¡µåˆ†ææ’ä»¶å·²å¸è½½")