"""
ç½‘é¡µåˆ†ææ’ä»¶ - ç½‘é¡µåˆ†æå™¨æ¨¡å—

è¿™ä¸ªæ¨¡å—æ˜¯ç½‘é¡µåˆ†æçš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ï¼š
- ç½‘é¡µå†…å®¹çš„å¼‚æ­¥æŠ“å–
- URLçš„æå–å’ŒéªŒè¯
- ç½‘é¡µå†…å®¹çš„ç»“æ„åŒ–è§£æ
- ç‰¹å®šç±»å‹å†…å®¹çš„æå–ï¼ˆå›¾ç‰‡ã€é“¾æ¥ã€è¡¨æ ¼ç­‰ï¼‰
- ç½‘é¡µæˆªå›¾çš„æ•è·

ä½¿ç”¨å¼‚æ­¥HTTPå®¢æˆ·ç«¯å’ŒBeautifulSoupè¿›è¡Œç½‘é¡µå¤„ç†ï¼Œæ”¯æŒä»£ç†ã€é‡è¯•ç­‰é«˜çº§åŠŸèƒ½ã€‚
"""

import re
from typing import List, Optional
from urllib.parse import urlparse, urljoin

import httpx
from bs4 import BeautifulSoup

from astrbot.api import logger


class WebAnalyzer:
    """ç½‘é¡µåˆ†æå™¨æ ¸å¿ƒç±»

    è¿™ä¸ªç±»æä¾›äº†å®Œæ•´çš„ç½‘é¡µåˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - ç½‘é¡µå†…å®¹çš„å¼‚æ­¥æŠ“å–
    - URLçš„æå–å’ŒéªŒè¯
    - HTMLå†…å®¹çš„è§£æå’Œç»“æ„åŒ–
    - ç‰¹å®šç±»å‹å†…å®¹çš„æå–
    - ç½‘é¡µæˆªå›¾çš„æ•è·

    æ”¯æŒå¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç¡®ä¿èµ„æºçš„æ­£ç¡®é‡Šæ”¾ã€‚
    """

    def __init__(
        self,
        max_content_length: int = 10000,
        timeout: int = 30,
        user_agent: str = None,
        proxy: str = None,
        retry_count: int = 3,
        retry_delay: int = 2,
    ):
        """åˆå§‹åŒ–ç½‘é¡µåˆ†æå™¨

        Args:
            max_content_length: æå–çš„æœ€å¤§å†…å®¹é•¿åº¦ï¼Œé˜²æ­¢å†…å®¹è¿‡å¤§
            timeout: HTTPè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œå•ä½ä¸ºç§’
            user_agent: è¯·æ±‚æ—¶ä½¿ç”¨çš„User-Agentå¤´
            proxy: HTTPä»£ç†è®¾ç½®ï¼Œæ ¼å¼ä¸º http://host:port æˆ– https://host:port
            retry_count: è¯·æ±‚å¤±è´¥æ—¶çš„é‡è¯•æ¬¡æ•°
            retry_delay: é‡è¯•é—´éš”æ—¶é—´ï¼Œå•ä½ä¸ºç§’
        """
        self.max_content_length = max_content_length
        self.timeout = timeout
        self.user_agent = (
            user_agent
            or "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )
        self.proxy = proxy
        self.retry_count = retry_count
        self.retry_delay = retry_delay
        self.client = None
        self.browser = None

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£

        åˆå§‹åŒ–å¼‚æ­¥HTTPå®¢æˆ·ç«¯ï¼Œé…ç½®ï¼š
        - è¯·æ±‚è¶…æ—¶æ—¶é—´
        - ä»£ç†è®¾ç½®ï¼ˆå¦‚æœæä¾›ï¼‰
        - å…¶ä»–HTTPå®¢æˆ·ç«¯å‚æ•°

        Returns:
            è¿”å›WebAnalyzerå®ä¾‹è‡ªèº«ï¼Œç”¨äºä¸Šä¸‹æ–‡ç®¡ç†
        """
        # é…ç½®å®¢æˆ·ç«¯å‚æ•°
        client_params = {"timeout": self.timeout}

        # æ·»åŠ ä»£ç†é…ç½®ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.proxy:
            client_params["proxies"] = {"http://": self.proxy, "https://": self.proxy}

        self.client = httpx.AsyncClient(**client_params)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£

        æ¸…ç†èµ„æºï¼Œç¡®ä¿ï¼š
        - å¼‚æ­¥HTTPå®¢æˆ·ç«¯æ­£ç¡®å…³é—­
        - æµè§ˆå™¨å®ä¾‹æ­£ç¡®å…³é—­ï¼ˆå¦‚æœä½¿ç”¨äº†ï¼‰
        - èµ„æºæ³„æ¼çš„é˜²æ­¢

        Args:
            exc_type: å¼‚å¸¸ç±»å‹ï¼ˆå¦‚æœæœ‰ï¼‰
            exc_val: å¼‚å¸¸å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
            exc_tb: å¼‚å¸¸å›æº¯ï¼ˆå¦‚æœæœ‰ï¼‰
        """
        if self.client:
            await self.client.aclose()
        if self.browser:
            await self.browser.close()

    def extract_urls(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰HTTP/HTTPS URLé“¾æ¥

        ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ–‡æœ¬ä¸­çš„URLï¼Œæ”¯æŒï¼š
        - HTTPå’ŒHTTPSåè®®
        - å„ç§å¸¸è§çš„URLæ ¼å¼
        - æ’é™¤ä¸­æ–‡ç­‰éASCIIå­—ç¬¦ä½œä¸ºURLçš„ä¸€éƒ¨åˆ†

        Args:
            text: è¦ä»ä¸­æå–URLçš„æ–‡æœ¬å†…å®¹

        Returns:
            åŒ…å«æ‰€æœ‰æå–åˆ°çš„URLçš„åˆ—è¡¨
        """
        # åŒ¹é…å¸¸è§çš„URLæ ¼å¼ï¼Œæ’é™¤ä¸­æ–‡ç­‰éASCIIå­—ç¬¦
        url_pattern = r"https?://[^\s\u4e00-\u9fff]+"
        urls = re.findall(url_pattern, text)
        return urls

    def is_valid_url(self, url: str) -> bool:
        """éªŒè¯URLæ ¼å¼æ˜¯å¦æœ‰æ•ˆ

        æ£€æŸ¥URLæ˜¯å¦ç¬¦åˆåŸºæœ¬æ ¼å¼è¦æ±‚ï¼š
        - å¿…é¡»åŒ…å«æœ‰æ•ˆçš„åè®®ï¼ˆhttp/httpsï¼‰
        - å¿…é¡»åŒ…å«æœ‰æ•ˆçš„åŸŸåæˆ–IPåœ°å€
        - å¿…é¡»èƒ½è¢«æ­£ç¡®è§£æ

        Args:
            url: è¦éªŒè¯çš„URLå­—ç¬¦ä¸²

        Returns:
            Trueè¡¨ç¤ºURLæ ¼å¼æœ‰æ•ˆï¼ŒFalseè¡¨ç¤ºæ— æ•ˆ
        """
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except Exception:
            return False
    
    def normalize_url(self, url: str) -> str:
        """è§„èŒƒåŒ–URLï¼Œç»Ÿä¸€æ ¼å¼

        å¯¹URLè¿›è¡Œè§„èŒƒåŒ–å¤„ç†ï¼š
        - ğŸ”„  è½¬æ¢ä¸ºå°å†™
        - ğŸ“  ç»Ÿä¸€å¤„ç†å°¾éƒ¨æ–œæ 
        - ğŸ§¹  å»é™¤å¤šä½™çš„æŸ¥è¯¢å‚æ•°å’Œç‰‡æ®µï¼ˆå¯é€‰ï¼‰

        Args:
            url: è¦è§„èŒƒåŒ–çš„URLå­—ç¬¦ä¸²

        Returns:
            è§„èŒƒåŒ–åçš„URLå­—ç¬¦ä¸²
        """
        try:
            parsed = urlparse(url)
            # è½¬æ¢ä¸ºå°å†™
            normalized = parsed._replace(
                scheme=parsed.scheme.lower(),
                netloc=parsed.netloc.lower(),
                path=parsed.path.rstrip('/')  # ç§»é™¤å°¾éƒ¨æ–œæ 
            )
            return normalized.geturl()
        except Exception:
            return url

    async def fetch_webpage(self, url: str) -> Optional[str]:
        """å¼‚æ­¥æŠ“å–ç½‘é¡µHTMLå†…å®¹

        ä½¿ç”¨å¼‚æ­¥HTTPå®¢æˆ·ç«¯æŠ“å–ç½‘é¡µï¼Œæ”¯æŒï¼š
        - è‡ªå®šä¹‰User-Agent
        - è‡ªåŠ¨è·Ÿéšé‡å®šå‘
        - é…ç½®çš„ä»£ç†è®¾ç½®
        - æ™ºèƒ½é‡è¯•æœºåˆ¶ï¼ˆå¤±è´¥åè‡ªåŠ¨é‡è¯•ï¼‰

        Args:
            url: è¦æŠ“å–çš„ç½‘é¡µURL

        Returns:
            ç½‘é¡µçš„HTMLæ–‡æœ¬å†…å®¹ï¼Œå¦‚æœæŠ“å–å¤±è´¥åˆ™è¿”å›None
        """
        headers = {
            "User-Agent": self.user_agent,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "DNT": "1",
            "Sec-GPC": "1"
        }

        # å®ç°é‡è¯•æœºåˆ¶ï¼Œæœ€å¤šå°è¯• retry_count + 1 æ¬¡
        for attempt in range(self.retry_count + 1):
            try:
                response = await self.client.get(
                    url, headers=headers, follow_redirects=True
                )
                response.raise_for_status()

                logger.info(
                    f"æŠ“å–ç½‘é¡µæˆåŠŸ: {url} (å°è¯• {attempt + 1}/{self.retry_count + 1})"
                )
                return response.text
            except Exception as e:
                if attempt < self.retry_count:
                    # è¿˜æœ‰é‡è¯•æ¬¡æ•°ï¼Œç­‰å¾… retry_delay ç§’åé‡è¯•
                    logger.warning(
                        f"æŠ“å–ç½‘é¡µå¤±è´¥ï¼Œå°†é‡è¯•: {url}, é”™è¯¯: {e} (å°è¯• {attempt + 1}/{self.retry_count + 1})"
                    )
                    import asyncio

                    await asyncio.sleep(self.retry_delay)
                else:
                    # é‡è¯•æ¬¡æ•°ç”¨å®Œï¼Œè®°å½•é”™è¯¯å¹¶è¿”å›None
                    logger.error(
                        f"æŠ“å–ç½‘é¡µå¤±è´¥: {url}, é”™è¯¯: {e} (å°è¯• {attempt + 1}/{self.retry_count + 1})"
                    )
                    return None

    def extract_content(self, html: str, url: str) -> dict:
        """ä»HTMLä¸­æå–ç»“æ„åŒ–çš„ç½‘é¡µå†…å®¹

        è§£æHTMLæ–‡æ¡£ï¼Œæå–å…³é”®å†…å®¹ï¼š
        - ç½‘é¡µæ ‡é¢˜
        - ä¸»è¦æ­£æ–‡å†…å®¹
        - æ”¯æŒå¤šç§å†…å®¹é€‰æ‹©ç­–ç•¥

        ä½¿ç”¨BeautifulSoupè¿›è¡ŒHTMLè§£æï¼Œä¼˜å…ˆé€‰æ‹©è¯­ä¹‰åŒ–æ ‡ç­¾
        ï¼ˆå¦‚articleã€mainç­‰ï¼‰æå–å†…å®¹ï¼Œç¡®ä¿æå–çš„å†…å®¹è´¨é‡ã€‚

        Args:
            html: ç½‘é¡µçš„HTMLæ–‡æœ¬å†…å®¹
            url: ç½‘é¡µçš„åŸå§‹URLï¼Œç”¨äºç»“æœè¿”å›

        Returns:
            åŒ…å«æ ‡é¢˜ã€å†…å®¹å’ŒURLçš„å­—å…¸ï¼Œå¦‚æœè§£æå¤±è´¥åˆ™è¿”å›None
        """
        try:
            soup = BeautifulSoup(html, "lxml")

            # æå–ç½‘é¡µæ ‡é¢˜
            title_text = self._extract_title(soup)
            
            # æå–æ–‡ç« å†…å®¹
            content_text = self._extract_main_content(soup)
            
            # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé˜²æ­¢å†…å®¹è¿‡å¤§
            content_text = self._limit_content_length(content_text)

            return {"title": title_text, "content": content_text, "url": url}
        except Exception as e:
            logger.error(f"è§£æç½‘é¡µå†…å®¹å¤±è´¥: {e}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """ä»BeautifulSoupå¯¹è±¡ä¸­æå–ç½‘é¡µæ ‡é¢˜
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            ç½‘é¡µæ ‡é¢˜æ–‡æœ¬
        """
        title = soup.find("title")
        return title.get_text().strip() if title else "æ— æ ‡é¢˜"
    
    def _extract_main_content(self, soup: BeautifulSoup) -> str:
        """ä»BeautifulSoupå¯¹è±¡ä¸­æå–ä¸»è¦å†…å®¹
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            æå–çš„ä¸»è¦å†…å®¹æ–‡æœ¬
        """
        # å°è¯•æå–æ–‡ç« å†…å®¹ï¼ˆä¼˜å…ˆé€‰æ‹©articleã€mainç­‰è¯­ä¹‰åŒ–æ ‡ç­¾ï¼‰
        content_selectors = [
            "article",  # è¯­ä¹‰åŒ–æ–‡ç« æ ‡ç­¾
            "main",  # è¯­ä¹‰åŒ–ä¸»å†…å®¹æ ‡ç­¾
            ".article-content",  # å¸¸è§æ–‡ç« å†…å®¹ç±»å
            ".post-content",  # å¸¸è§åšå®¢å†…å®¹ç±»å
            ".content",  # é€šç”¨å†…å®¹ç±»å
            "body",  # å…œåº•ï¼šä½¿ç”¨æ•´ä¸ªbody
        ]

        content_text = ""
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                # æ¸…ç†å†…å®¹ï¼Œç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
                cleaned_element = self._clean_content_element(element)
                text = cleaned_element.get_text(separator="\n", strip=True)
                if len(text) > len(content_text):
                    content_text = text

        # å¦‚æœæ²¡æ‰¾åˆ°åˆé€‚çš„å†…å®¹ï¼Œä½¿ç”¨bodyä½œä¸ºæœ€åçš„å…œåº•æ–¹æ¡ˆ
        if not content_text:
            body = soup.find("body")
            if body:
                cleaned_body = self._clean_content_element(body)
                content_text = cleaned_body.get_text(separator="\n", strip=True)
        
        return content_text
    
    def _clean_content_element(self, element: BeautifulSoup) -> BeautifulSoup:
        """æ¸…ç†å†…å®¹å…ƒç´ ï¼Œç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
        
        Args:
            element: BeautifulSoupå…ƒç´ 
            
        Returns:
            æ¸…ç†åçš„BeautifulSoupå…ƒç´ 
        """
        # ç›´æ¥å¤„ç†å…ƒç´ ï¼Œä¸éœ€è¦åˆ›å»ºå‰¯æœ¬
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾ï¼Œé¿å…å¹²æ‰°å†…å®¹æå–
        for script in element.find_all(["script", "style"]):
            script.decompose()
        return element
    
    def _limit_content_length(self, content: str) -> str:
        """é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé˜²æ­¢å†…å®¹è¿‡å¤§
        
        Args:
            content: åŸå§‹å†…å®¹æ–‡æœ¬
            
        Returns:
            é™åˆ¶é•¿åº¦åçš„å†…å®¹æ–‡æœ¬
        """
        if len(content) > self.max_content_length:
            return content[: self.max_content_length] + "..."
        return content

    async def capture_screenshot(
        self,
        url: str,
        quality: int = 80,
        width: int = 1280,
        height: int = 720,
        full_page: bool = False,
        wait_time: int = 2000,
        format: str = "jpeg",
    ) -> Optional[bytes]:
        """ä½¿ç”¨Playwrightæ•è·ç½‘é¡µæˆªå›¾

        è‡ªåŠ¨å¤„ç†æµè§ˆå™¨çš„å®‰è£…å’Œé…ç½®ï¼Œæ”¯æŒï¼š
        - è‡ªå®šä¹‰åˆ†è¾¨ç‡å’Œè´¨é‡
        - å…¨å±æˆªå›¾æˆ–å¯è§†åŒºåŸŸæˆªå›¾
        - è‡ªå®šä¹‰ç­‰å¾…æ—¶é—´ï¼Œç¡®ä¿é¡µé¢åŠ è½½å®Œæˆ
        - æ”¯æŒJPEGå’ŒPNGæ ¼å¼

        Args:
            url: è¦æˆªå›¾çš„ç½‘é¡µURL
            quality: æˆªå›¾è´¨é‡ï¼ŒèŒƒå›´1-100
            width: æˆªå›¾å®½åº¦ï¼ˆåƒç´ ï¼‰
            height: æˆªå›¾é«˜åº¦ï¼ˆåƒç´ ï¼‰
            full_page: æ˜¯å¦æˆªå–æ•´ä¸ªé¡µé¢ï¼ŒFalseä»…æˆªå–å¯è§†åŒºåŸŸ
            wait_time: é¡µé¢åŠ è½½åç­‰å¾…çš„æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½
            format: æˆªå›¾æ ¼å¼ï¼Œæ”¯æŒ"jpeg"å’Œ"png"

        Returns:
            æˆªå›¾çš„äºŒè¿›åˆ¶æ•°æ®ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            from playwright.async_api import async_playwright
            import sys
            import subprocess
            import os

            # åªåœ¨ç¬¬ä¸€æ¬¡æ‰§è¡Œæ—¶æ£€æŸ¥æµè§ˆå™¨å®‰è£…
            if not hasattr(self, '_playwright_browser_checked'):
                logger.info("æ­£åœ¨æ£€æŸ¥æµè§ˆå™¨...")
                # æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦å·²å®‰è£…
                browser_path = os.path.join(os.path.expanduser('~'), '.cache', 'ms-playwright', 'chromium')
                if os.path.exists(browser_path):
                    logger.info("æµè§ˆå™¨å·²å®‰è£…ï¼Œè·³è¿‡å®‰è£…æ­¥éª¤")
                else:
                    # å®‰è£…æµè§ˆå™¨
                    logger.info("æ­£åœ¨å®‰è£…æµè§ˆå™¨...")
                    result = subprocess.run(
                        [sys.executable, "-m", "playwright", "install", "chromium"],
                        capture_output=True,
                        text=True,
                    )

                    if result.returncode != 0:
                        logger.error(f"æµè§ˆå™¨å®‰è£…å¤±è´¥: {result.stderr}")
                        return None
                    logger.info("æµè§ˆå™¨å®‰è£…æˆåŠŸ")
            
            # æ ‡è®°å·²æ£€æŸ¥æµè§ˆå™¨
            self._playwright_browser_checked = True

            logger.info("æ­£åœ¨å°è¯•æˆªå›¾...")

            # å°è¯•å¯åŠ¨playwrightå¹¶æˆªå›¾
            async with async_playwright() as p:
                # å¯åŠ¨æµè§ˆå™¨ï¼ˆæ— å¤´æ¨¡å¼ï¼Œä¸æ˜¾ç¤ºGUIï¼‰
                browser = await p.chromium.launch(
                    headless=True,
                    # å¢åŠ æµè§ˆå™¨å¯åŠ¨è¶…æ—¶æ—¶é—´åˆ°60ç§’
                    timeout=20000,
                    # æ·»åŠ é¢å¤–çš„å¯åŠ¨å‚æ•°ï¼Œæé«˜å…¼å®¹æ€§å’Œç¨³å®šæ€§
                    args=[
                        "--no-sandbox",  # ç¦ç”¨æ²™ç®±ï¼Œæé«˜å…¼å®¹æ€§
                        "--disable-setuid-sandbox",  # ç¦ç”¨setuidæ²™ç®±
                        "--disable-dev-shm-usage",  # ç¦ç”¨/dev/shmä½¿ç”¨
                        "--disable-gpu",  # ç¦ç”¨GPUåŠ é€Ÿ
                        # ç§»é™¤å›ºå®šç«¯å£ï¼Œè®©playwrightè‡ªåŠ¨åˆ†é…å¯ç”¨ç«¯å£
                    ],
                )

                # åˆ›å»ºæ–°çš„é¡µé¢ï¼Œè®¾ç½®è§†å£å’ŒUser-Agent
                page = await browser.new_page(
                    viewport={"width": width, "height": height},
                    user_agent=self.user_agent,
                )

                # å¯¼èˆªåˆ°ç›®æ ‡URLï¼Œä½¿ç”¨æ›´å®½æ¾çš„ç­‰å¾…æ¡ä»¶
                await page.goto(url, wait_until="domcontentloaded", timeout=60000)

                # ç­‰å¾…æŒ‡å®šæ—¶é—´ï¼Œç¡®ä¿é¡µé¢å®Œå…¨åŠ è½½ï¼ˆå°¤å…¶æ˜¯åŠ¨æ€å†…å®¹ï¼‰
                await page.wait_for_timeout(wait_time)

                # æ•è·æˆªå›¾
                screenshot_bytes = await page.screenshot(
                    full_page=full_page,  # æ˜¯å¦æˆªå–æ•´ä¸ªé¡µé¢
                    quality=quality,  # æˆªå›¾è´¨é‡
                    type=format,  # æˆªå›¾æ ¼å¼
                )

                await browser.close()

                logger.info("æˆªå›¾æˆåŠŸ")
                return screenshot_bytes
        except Exception as e:
            logger.error(f"æ•è·ç½‘é¡µæˆªå›¾å¤±è´¥: {url}, é”™è¯¯: {e}")
            return None

    def extract_specific_content(
        self, html: str, url: str, extract_types: List[str]
    ) -> dict:
        """ä»HTMLä¸­æå–ç‰¹å®šç±»å‹çš„å†…å®¹

        æ ¹æ®æŒ‡å®šçš„æå–ç±»å‹ï¼Œä»HTMLæ–‡æ¡£ä¸­æå–ç»“æ„åŒ–æ•°æ®ï¼š
        - æ ‡é¢˜ï¼ˆtitleï¼‰
        - æ­£æ–‡å†…å®¹ï¼ˆcontentï¼‰
        - å›¾ç‰‡é“¾æ¥ï¼ˆimagesï¼‰
        - è¶…é“¾æ¥ï¼ˆlinksï¼‰
        - è¡¨æ ¼ï¼ˆtablesï¼‰
        - åˆ—è¡¨ï¼ˆlistsï¼‰
        - ä»£ç å—ï¼ˆcodeï¼‰
        - å…ƒä¿¡æ¯ï¼ˆmetaï¼‰

        Args:
            html: ç½‘é¡µçš„HTMLæ–‡æœ¬å†…å®¹
            url: ç½‘é¡µçš„åŸå§‹URLï¼Œç”¨äºå¤„ç†ç›¸å¯¹è·¯å¾„
            extract_types: è¦æå–çš„å†…å®¹ç±»å‹åˆ—è¡¨

        Returns:
            åŒ…å«æå–å†…å®¹çš„å­—å…¸ï¼Œé”®ä¸ºæå–ç±»å‹ï¼Œå€¼ä¸ºå¯¹åº”å†…å®¹
        """
        try:
            soup = BeautifulSoup(html, "lxml")
            extracted_content = {}

            # æå–æ ‡é¢˜
            if "title" in extract_types:
                title = soup.find("title")
                extracted_content["title"] = (
                    title.get_text().strip() if title else "æ— æ ‡é¢˜"
                )

            # æå–æ­£æ–‡å†…å®¹
            if "content" in extract_types:
                content_selectors = [
                    "article",  # è¯­ä¹‰åŒ–æ–‡ç« æ ‡ç­¾
                    "main",  # è¯­ä¹‰åŒ–ä¸»å†…å®¹æ ‡ç­¾
                    ".article-content",  # å¸¸è§æ–‡ç« å†…å®¹ç±»å
                    ".post-content",  # å¸¸è§åšå®¢å†…å®¹ç±»å
                    ".content",  # é€šç”¨å†…å®¹ç±»å
                    "body",  # å…œåº•æ–¹æ¡ˆ
                ]

                content_text = ""
                for selector in content_selectors:
                    element = soup.select_one(selector)
                    if element:
                        # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾ï¼Œé¿å…å¹²æ‰°å†…å®¹æå–
                        for script in element.find_all(["script", "style"]):
                            script.decompose()
                        text = element.get_text(separator="\n", strip=True)
                        if len(text) > len(content_text):
                            content_text = text

                # é™åˆ¶å†…å®¹é•¿åº¦
                if len(content_text) > self.max_content_length:
                    content_text = content_text[: self.max_content_length] + "..."

                extracted_content["content"] = content_text

            # æå–å›¾ç‰‡é“¾æ¥ï¼Œæœ€å¤šæå–10å¼ 
            if "images" in extract_types:
                images = []
                for img in soup.find_all("img"):
                    src = img.get("src")
                    if src:
                        # å¤„ç†ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹URL
                        full_url = urljoin(url, src)
                        images.append(full_url)
                extracted_content["images"] = images[:10]  # é™åˆ¶æœ€å¤š10å¼ å›¾ç‰‡

            # æå–è¶…é“¾æ¥ï¼Œæœ€å¤šæå–20ä¸ª
            if "links" in extract_types:
                links = []
                for a in soup.find_all("a", href=True):
                    href = a.get("href")
                    if href and not href.startswith("#"):  # è·³è¿‡é”šç‚¹é“¾æ¥
                        full_url = urljoin(url, href)
                        text = a.get_text().strip() or full_url  # é“¾æ¥æ–‡æœ¬ä¸ºç©ºæ—¶ä½¿ç”¨URL
                        links.append({"text": text, "url": full_url})
                extracted_content["links"] = links[:20]  # é™åˆ¶æœ€å¤š20ä¸ªé“¾æ¥

            # æå–è¡¨æ ¼ï¼Œæœ€å¤šæå–5ä¸ª
            if "tables" in extract_types:
                tables = []
                for table in soup.find_all("table"):
                    table_data = []
                    # æå–è¡¨å¤´
                    headers = []
                    thead = table.find("thead")
                    if thead:
                        for th in thead.find_all("th"):
                            headers.append(th.get_text().strip())

                    # æå–è¡¨ä½“
                    tbody = table.find("tbody") or table  # æ²¡æœ‰tbodyæ—¶ä½¿ç”¨tableæœ¬èº«
                    for row in tbody.find_all("tr"):
                        row_data = []
                        for cell in row.find_all(["td", "th"]):  # åŒæ—¶å¤„ç†tdå’Œth
                            row_data.append(cell.get_text().strip())
                        if row_data:  # è·³è¿‡ç©ºè¡Œ
                            table_data.append(row_data)

                    if table_data:  # åªæ·»åŠ æœ‰æ•°æ®çš„è¡¨æ ¼
                        tables.append({"headers": headers, "rows": table_data})
                extracted_content["tables"] = tables[:5]  # é™åˆ¶æœ€å¤š5ä¸ªè¡¨æ ¼

            # æå–åˆ—è¡¨ï¼Œæœ€å¤šæå–10ä¸ª
            if "lists" in extract_types:
                lists = []
                # æå–æ— åºåˆ—è¡¨
                for ul in soup.find_all("ul"):
                    list_items = []
                    for li in ul.find_all("li"):
                        list_items.append(li.get_text().strip())
                    if list_items:  # åªæ·»åŠ æœ‰å†…å®¹çš„åˆ—è¡¨
                        lists.append(
                            {
                                "type": "ul",  # åˆ—è¡¨ç±»å‹ï¼šæ— åºåˆ—è¡¨
                                "items": list_items[:20],  # æ¯ä¸ªåˆ—è¡¨æœ€å¤š20é¡¹
                            }
                        )

                # æå–æœ‰åºåˆ—è¡¨
                for ol in soup.find_all("ol"):
                    list_items = []
                    for li in ol.find_all("li"):
                        list_items.append(li.get_text().strip())
                    if list_items:  # åªæ·»åŠ æœ‰å†…å®¹çš„åˆ—è¡¨
                        lists.append(
                            {
                                "type": "ol",  # åˆ—è¡¨ç±»å‹ï¼šæœ‰åºåˆ—è¡¨
                                "items": list_items[:20],  # æ¯ä¸ªåˆ—è¡¨æœ€å¤š20é¡¹
                            }
                        )
                extracted_content["lists"] = lists[:10]  # é™åˆ¶æœ€å¤š10ä¸ªåˆ—è¡¨

            # æå–ä»£ç å—ï¼Œæœ€å¤šæå–5ä¸ª
            if "code" in extract_types:
                code_blocks = []
                for code in soup.find_all(["pre", "code"]):  # åŒæ—¶å¤„ç†preå’Œcodeæ ‡ç­¾
                    code_text = code.get_text().strip()
                    if code_text and len(code_text) > 10:  # è·³è¿‡çŸ­ä»£ç å—
                        # é™åˆ¶å•ä¸ªä»£ç å—é•¿åº¦
                        truncated_code = (
                            code_text[:1000] + "..."
                            if len(code_text) > 1000
                            else code_text
                        )
                        code_blocks.append(truncated_code)
                extracted_content["code_blocks"] = code_blocks[:5]  # é™åˆ¶æœ€å¤š5ä¸ªä»£ç å—

            # æå–å…ƒä¿¡æ¯
            if "meta" in extract_types:
                meta_info = {}
                # æå–æè¿°
                description = soup.find("meta", attrs={"name": "description"})
                if description:
                    meta_info["description"] = description.get("content", "").strip()

                # æå–å…³é”®è¯
                keywords = soup.find("meta", attrs={"name": "keywords"})
                if keywords:
                    meta_info["keywords"] = keywords.get("content", "").strip()

                # æå–ä½œè€…
                author = soup.find("meta", attrs={"name": "author"})
                if author:
                    meta_info["author"] = author.get("content", "").strip()

                # æå–å‘å¸ƒæ—¶é—´
                publish_time = soup.find(
                    "meta", attrs={"property": "article:published_time"}
                )
                if not publish_time:
                    publish_time = soup.find("meta", attrs={"name": "publish_date"})
                if publish_time:
                    meta_info["publish_time"] = publish_time.get("content", "").strip()

                extracted_content["meta"] = meta_info

            return extracted_content
        except Exception as e:
            logger.error(f"æå–ç‰¹å®šå†…å®¹å¤±è´¥: {e}")
            return {}
