"""
AstrBot ç½‘é¡µåˆ†ææ’ä»¶

è‡ªåŠ¨è¯†åˆ«ç½‘é¡µé“¾æ¥ï¼Œæ™ºèƒ½æŠ“å–è§£æå†…å®¹ï¼Œé›†æˆå¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æå’Œæ€»ç»“ï¼Œæ”¯æŒç½‘é¡µæˆªå›¾ã€ç¼“å­˜æœºåˆ¶å’Œå¤šç§ç®¡ç†å‘½ä»¤ã€‚
"""

from typing import Any

from astrbot.api import AstrBotConfig, logger
from astrbot.api.event import AstrMessageEvent, filter
from astrbot.api.star import Context, Star, register

from .analyzer import WebAnalyzer
from .cache import CacheManager
from .utils import WebAnalyzerUtils


# é”™è¯¯ç±»å‹æšä¸¾
class ErrorType:
    """é”™è¯¯ç±»å‹æšä¸¾"""
    # ç½‘ç»œç›¸å…³
    NETWORK_ERROR = "network_error"  # ç½‘ç»œé”™è¯¯
    NETWORK_TIMEOUT = "network_timeout"  # ç½‘ç»œè¶…æ—¶
    NETWORK_CONNECTION = "network_connection"  # è¿æ¥å¤±è´¥

    # è§£æç›¸å…³
    PARSING_ERROR = "parsing_error"  # è§£æé”™è¯¯
    CONTENT_EMPTY = "content_empty"  # å†…å®¹ä¸ºç©º
    HTML_PARSING = "html_parsing"  # HTMLè§£æé”™è¯¯

    # LLMç›¸å…³
    LLM_ERROR = "llm_error"  # LLMé”™è¯¯
    LLM_TIMEOUT = "llm_timeout"  # LLMè¶…æ—¶
    LLM_INVALID_RESPONSE = "llm_invalid_response"  # LLMæ— æ•ˆå“åº”
    LLM_PERMISSION = "llm_permission"  # LLMæƒé™é”™è¯¯

    # æˆªå›¾ç›¸å…³
    SCREENSHOT_ERROR = "screenshot_error"  # æˆªå›¾é”™è¯¯
    BROWSER_ERROR = "browser_error"  # æµè§ˆå™¨é”™è¯¯

    # ç¼“å­˜ç›¸å…³
    CACHE_ERROR = "cache_error"  # ç¼“å­˜é”™è¯¯
    CACHE_WRITE = "cache_write"  # ç¼“å­˜å†™å…¥é”™è¯¯
    CACHE_READ = "cache_read"  # ç¼“å­˜è¯»å–é”™è¯¯

    # é…ç½®ç›¸å…³
    CONFIG_ERROR = "config_error"  # é…ç½®é”™è¯¯
    CONFIG_INVALID = "config_invalid"  # é…ç½®æ— æ•ˆ

    # æƒé™ç›¸å…³
    PERMISSION_ERROR = "permission_error"  # æƒé™é”™è¯¯
    DOMAIN_BLOCKED = "domain_blocked"  # åŸŸåè¢«é˜»æ­¢

    # å…¶ä»–é”™è¯¯
    UNKNOWN_ERROR = "unknown_error"  # æœªçŸ¥é”™è¯¯
    INTERNAL_ERROR = "internal_error"  # å†…éƒ¨é”™è¯¯


# é”™è¯¯ä¸¥é‡ç¨‹åº¦æšä¸¾
class ErrorSeverity:
    """é”™è¯¯ä¸¥é‡ç¨‹åº¦æšä¸¾"""
    INFO = "info"  # ä¿¡æ¯çº§
    WARNING = "warning"  # è­¦å‘Šçº§
    ERROR = "error"  # é”™è¯¯çº§
    CRITICAL = "critical"  # ä¸¥é‡é”™è¯¯


# é”™è¯¯å¤„ç†é…ç½®
ERROR_MESSAGES: dict[str, dict[str, Any]] = {
    "network_error": {
        "message": "ç½‘ç»œè¯·æ±‚å¤±è´¥",
        "solution": "è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–URLæ˜¯å¦æ­£ç¡®ï¼Œæˆ–å°è¯•è°ƒæ•´è¯·æ±‚è¶…æ—¶è®¾ç½®",
        "severity": ErrorSeverity.ERROR
    },
    "network_timeout": {
        "message": "ç½‘ç»œè¯·æ±‚è¶…æ—¶",
        "solution": "ç›®æ ‡ç½‘ç«™å“åº”ç¼“æ…¢ï¼Œè¯·ç¨åé‡è¯•æˆ–è°ƒæ•´è¯·æ±‚è¶…æ—¶è®¾ç½®",
        "severity": ErrorSeverity.ERROR
    },
    "network_connection": {
        "message": "ç½‘ç»œè¿æ¥å¤±è´¥",
        "solution": "æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç›®æ ‡ç½‘ç«™æ˜¯å¦å¯è®¿é—®",
        "severity": ErrorSeverity.ERROR
    },
    "parsing_error": {
        "message": "ç½‘é¡µå†…å®¹è§£æå¤±è´¥",
        "solution": "è¯¥ç½‘é¡µç»“æ„å¯èƒ½è¾ƒä¸ºç‰¹æ®Šï¼Œå»ºè®®å°è¯•å…¶ä»–åˆ†ææ–¹å¼",
        "severity": ErrorSeverity.WARNING
    },
    "content_empty": {
        "message": "æå–çš„å†…å®¹ä¸ºç©º",
        "solution": "ç›®æ ‡ç½‘é¡µå¯èƒ½æ²¡æœ‰å¯æå–çš„å†…å®¹ï¼Œæˆ–å†…å®¹æ ¼å¼ä¸æ”¯æŒ",
        "severity": ErrorSeverity.WARNING
    },
    "html_parsing": {
        "message": "HTMLè§£æé”™è¯¯",
        "solution": "ç½‘é¡µHTMLæ ¼å¼å¼‚å¸¸ï¼Œæ— æ³•æ­£ç¡®è§£æ",
        "severity": ErrorSeverity.ERROR
    },
    "llm_error": {
        "message": "å¤§è¯­è¨€æ¨¡å‹åˆ†æå¤±è´¥",
        "solution": "è¯·æ£€æŸ¥LLMé…ç½®æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å°è¯•è°ƒæ•´åˆ†æå‚æ•°",
        "severity": ErrorSeverity.ERROR
    },
    "llm_timeout": {
        "message": "å¤§è¯­è¨€æ¨¡å‹å“åº”è¶…æ—¶",
        "solution": "LLMå“åº”ç¼“æ…¢ï¼Œè¯·ç¨åé‡è¯•æˆ–è°ƒæ•´LLMè¶…æ—¶è®¾ç½®",
        "severity": ErrorSeverity.ERROR
    },
    "llm_invalid_response": {
        "message": "å¤§è¯­è¨€æ¨¡å‹è¿”å›æ— æ•ˆå“åº”",
        "solution": "LLMè¿”å›æ ¼å¼å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥LLMé…ç½®æˆ–ç¨åé‡è¯•",
        "severity": ErrorSeverity.ERROR
    },
    "llm_permission": {
        "message": "å¤§è¯­è¨€æ¨¡å‹æƒé™ä¸è¶³",
        "solution": "è¯·æ£€æŸ¥LLM APIå¯†é’¥æˆ–æƒé™é…ç½®",
        "severity": ErrorSeverity.ERROR
    },
    "screenshot_error": {
        "message": "ç½‘é¡µæˆªå›¾å¤±è´¥",
        "solution": "è¯·æ£€æŸ¥æµè§ˆå™¨é…ç½®æˆ–ç½‘ç»œè¿æ¥ï¼Œæˆ–å°è¯•è°ƒæ•´æˆªå›¾å‚æ•°",
        "severity": ErrorSeverity.WARNING
    },
    "browser_error": {
        "message": "æµè§ˆå™¨æ“ä½œå¤±è´¥",
        "solution": "æµè§ˆå™¨åˆå§‹åŒ–æˆ–æ“ä½œå¤±è´¥ï¼Œè¯·æ£€æŸ¥æµè§ˆå™¨é…ç½®æˆ–é‡å¯æ’ä»¶",
        "severity": ErrorSeverity.ERROR
    },
    "cache_error": {
        "message": "ç¼“å­˜æ“ä½œå¤±è´¥",
        "solution": "è¯·æ£€æŸ¥ç¼“å­˜ç›®å½•æƒé™æˆ–å­˜å‚¨ç©ºé—´",
        "severity": ErrorSeverity.WARNING
    },
    "cache_write": {
        "message": "ç¼“å­˜å†™å…¥å¤±è´¥",
        "solution": "æ— æ³•å†™å…¥ç¼“å­˜æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ç¼“å­˜ç›®å½•æƒé™æˆ–å­˜å‚¨ç©ºé—´",
        "severity": ErrorSeverity.WARNING
    },
    "cache_read": {
        "message": "ç¼“å­˜è¯»å–å¤±è´¥",
        "solution": "æ— æ³•è¯»å–ç¼“å­˜æ–‡ä»¶ï¼Œç¼“å­˜å¯èƒ½å·²æŸå",
        "severity": ErrorSeverity.WARNING
    },
    "config_error": {
        "message": "é…ç½®é”™è¯¯",
        "solution": "è¯·æ£€æŸ¥æ’ä»¶é…ç½®æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å°è¯•é‡ç½®é…ç½®",
        "severity": ErrorSeverity.ERROR
    },
    "config_invalid": {
        "message": "é…ç½®æ— æ•ˆ",
        "solution": "æ’ä»¶é…ç½®æ ¼å¼æ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®é¡¹æ˜¯å¦æ­£ç¡®",
        "severity": ErrorSeverity.ERROR
    },
    "permission_error": {
        "message": "æƒé™ä¸è¶³",
        "solution": "è¯·æ£€æŸ¥æ’ä»¶æƒé™é…ç½®ï¼Œæˆ–è”ç³»ç®¡ç†å‘˜è·å–æƒé™",
        "severity": ErrorSeverity.ERROR
    },
    "domain_blocked": {
        "message": "åŸŸåè¢«é˜»æ­¢",
        "solution": "è¯¥åŸŸåå·²è¢«åŠ å…¥é»‘åå•ï¼Œæ— æ³•è®¿é—®",
        "severity": ErrorSeverity.ERROR
    },
    "unknown_error": {
        "message": "æœªçŸ¥é”™è¯¯",
        "solution": "è¯·æ£€æŸ¥æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ï¼Œæˆ–å°è¯•é‡å¯æ’ä»¶",
        "severity": ErrorSeverity.CRITICAL
    },
    "internal_error": {
        "message": "å†…éƒ¨é”™è¯¯",
        "solution": "æ’ä»¶å†…éƒ¨å‘ç”Ÿé”™è¯¯ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æˆ–è”ç³»å¼€å‘è€…",
        "severity": ErrorSeverity.CRITICAL
    }
}


@register(
    "astrbot_plugin_web_analyzer",
    "Sakura520222",
    "è‡ªåŠ¨è¯†åˆ«ç½‘é¡µé“¾æ¥ï¼Œæ™ºèƒ½æŠ“å–è§£æå†…å®¹ï¼Œé›†æˆå¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æå’Œæ€»ç»“ï¼Œæ”¯æŒç½‘é¡µæˆªå›¾ã€ç¼“å­˜æœºåˆ¶å’Œå¤šç§ç®¡ç†å‘½ä»¤",
    "1.3.6",
    "https://github.com/Sakura520222/astrbot_plugin_web_analyzer",
)
class WebAnalyzerPlugin(Star):
    """ç½‘é¡µåˆ†ææ’ä»¶ä¸»ç±»ï¼Œè´Ÿè´£ç®¡ç†å’Œè°ƒåº¦æ‰€æœ‰åŠŸèƒ½æ¨¡å—"""

    def __init__(self, context: Context, config: AstrBotConfig):
        """æ’ä»¶åˆå§‹åŒ–æ–¹æ³•ï¼Œè´Ÿè´£åŠ è½½ã€éªŒè¯å’Œåˆå§‹åŒ–æ‰€æœ‰é…ç½®é¡¹"""
        super().__init__(context)
        self.config = config

        # åˆå§‹åŒ–é…ç½®
        self._load_network_settings()
        self._load_domain_settings()
        self._load_analysis_settings()
        self._load_screenshot_settings()
        self._load_llm_settings()
        self._load_group_settings()
        self._load_translation_settings()
        self._load_cache_settings()
        self._load_content_extraction_settings()
        self._load_recall_settings()
        self._load_command_settings()
        self._load_resource_settings()

        # URLå¤„ç†æ ‡å¿—é›†åˆï¼šç”¨äºé¿å…é‡å¤å¤„ç†åŒä¸€URL
        self.processing_urls = set()

        # åˆå§‹åŒ–ç»„ä»¶
        self._init_cache_manager()
        self._init_web_analyzer()

        # æ’¤å›ä»»åŠ¡åˆ—è¡¨ï¼šç”¨äºç®¡ç†æ‰€æœ‰æ’¤å›ä»»åŠ¡
        self.recall_tasks = []

        # è®°å½•é…ç½®åˆå§‹åŒ–å®Œæˆ
        logger.info("æ’ä»¶é…ç½®åˆå§‹åŒ–å®Œæˆ")

    def _load_network_settings(self):
        """åŠ è½½å’ŒéªŒè¯ç½‘ç»œè®¾ç½®"""
        network_settings = self.config.get("network_settings", {})
        # æœ€å¤§å†…å®¹é•¿åº¦
        self.max_content_length = max(1000, network_settings.get("max_content_length", 10000))
        # è¯·æ±‚è¶…æ—¶æ—¶é—´
        self.timeout = max(5, min(300, network_settings.get("request_timeout", 30)))
        # é‡è¯•æ¬¡æ•°
        self.retry_count = max(0, min(10, network_settings.get("retry_count", 3)))
        # é‡è¯•å»¶è¿Ÿ
        self.retry_delay = max(0, min(10, network_settings.get("retry_delay", 2)))
        # ç”¨æˆ·ä»£ç†
        self.user_agent = network_settings.get(
            "user_agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )
        # ä»£ç†è®¾ç½®
        self.proxy = network_settings.get("proxy", "")
        # å¹¶å‘å¤„ç†è®¾ç½®
        self.max_concurrency = max(1, min(20, network_settings.get("max_concurrency", 5)))
        self.dynamic_concurrency = bool(network_settings.get("dynamic_concurrency", True))
        # ä¼˜å…ˆçº§è®¾ç½®
        self.enable_priority_scheduling = bool(network_settings.get("enable_priority_scheduling", False))
        # URLå¤„ç†è®¾ç½®
        self.enable_unified_domain = bool(network_settings.get("enable_unified_domain", True))

        # éªŒè¯ä»£ç†æ ¼å¼æ˜¯å¦æ­£ç¡®
        if self.proxy:
            try:
                from urllib.parse import urlparse

                parsed = urlparse(self.proxy)
                if not all([parsed.scheme, parsed.netloc]):
                    logger.warning(f"æ— æ•ˆçš„ä»£ç†æ ¼å¼: {self.proxy}ï¼Œå°†å¿½ç•¥ä»£ç†è®¾ç½®")
                    self.proxy = ""
            except Exception as e:
                logger.warning(f"è§£æä»£ç†å¤±è´¥: {self.proxy}ï¼Œå°†å¿½ç•¥ä»£ç†è®¾ç½®ï¼Œé”™è¯¯: {e}")
                self.proxy = ""

    def _load_domain_settings(self):
        """åŠ è½½å’ŒéªŒè¯åŸŸåè®¾ç½®"""
        domain_settings = self.config.get("domain_settings", {})
        self.allowed_domains = self._parse_domain_list(domain_settings.get("allowed_domains", ""))
        self.blocked_domains = self._parse_domain_list(domain_settings.get("blocked_domains", ""))

    def _load_analysis_settings(self):
        """åŠ è½½å’ŒéªŒè¯åˆ†æè®¾ç½®"""
        analysis_settings = self.config.get("analysis_settings", {})
        # åˆ†ææ¨¡å¼
        self.analysis_mode = analysis_settings.get("analysis_mode", "auto")
        # éªŒè¯åˆ†ææ¨¡å¼æ˜¯å¦æœ‰æ•ˆ
        valid_modes = ["auto", "manual", "hybrid", "LLMTOOL"]
        if self.analysis_mode not in valid_modes:
            logger.warning(f"æ— æ•ˆçš„åˆ†ææ¨¡å¼: {self.analysis_mode}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ auto")
            self.analysis_mode = "auto"
        # å…¼å®¹æ—§çš„auto_analyzeé…ç½®
        self.auto_analyze = bool(analysis_settings.get("auto_analyze", True))
        # å¦‚æœè®¾ç½®äº†analysis_modeï¼Œä¼˜å…ˆä½¿ç”¨æ–°é…ç½®
        if "analysis_mode" in analysis_settings:
            self.auto_analyze = (self.analysis_mode == "auto")
        # æ˜¯å¦åœ¨ç»“æœä¸­ä½¿ç”¨emoji
        self.enable_emoji = bool(analysis_settings.get("enable_emoji", True))
        # æ˜¯å¦æ˜¾ç¤ºå†…å®¹ç»Ÿè®¡ä¿¡æ¯
        self.enable_statistics = bool(analysis_settings.get("enable_statistics", True))
        # æœ€å¤§æ‘˜è¦é•¿åº¦
        self.max_summary_length = max(500, min(10000, analysis_settings.get("max_summary_length", 2000)))

        # å‘é€å†…å®¹ç±»å‹è®¾ç½®
        self.send_content_type = analysis_settings.get("send_content_type", "both")
        # éªŒè¯å‘é€å†…å®¹ç±»å‹æ˜¯å¦æœ‰æ•ˆ
        if self.send_content_type not in ["both", "analysis_only", "screenshot_only"]:
            logger.warning(
                f"æ— æ•ˆçš„å‘é€å†…å®¹ç±»å‹: {self.send_content_type}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ both"
            )
            self.send_content_type = "both"

        # ç»“æœå±•ç¤ºè®¾ç½®
        self.result_template = analysis_settings.get("result_template", "default")
        # éªŒè¯ç»“æœæ¨¡æ¿æ˜¯å¦æœ‰æ•ˆ
        valid_templates = ["default", "detailed", "compact", "markdown", "simple"]
        if self.result_template not in valid_templates:
            logger.warning(
                f"æ— æ•ˆçš„ç»“æœæ¨¡æ¿: {self.result_template}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ default"
            )
            self.result_template = "default"

        # ç»“æœæŠ˜å è®¾ç½®
        self.enable_collapsible = bool(analysis_settings.get("enable_collapsible", False))
        self.collapse_threshold = max(500, min(5000, analysis_settings.get("collapse_threshold", 1500)))

        # æ— åè®®å¤´URLè¯†åˆ«è®¾ç½®
        self.enable_no_protocol_url = bool(analysis_settings.get("enable_no_protocol_url", False))
        self.default_protocol = analysis_settings.get("default_protocol", "https")
        # éªŒè¯é»˜è®¤åè®®æ˜¯å¦æœ‰æ•ˆ
        if self.default_protocol not in ["http", "https"]:
            logger.warning(f"æ— æ•ˆçš„é»˜è®¤åè®®: {self.default_protocol}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ https")
            self.default_protocol = "https"
        # æ˜¯å¦å…è®¸LLMè‡ªä¸»å†³ç­–ï¼šå…è®¸LLMå†³å®šæ˜¯å‘é€åˆ†æç»“æœè¿˜æ˜¯æˆªå›¾
        self.enable_llm_decision = bool(analysis_settings.get("enable_llm_decision", False))

    def _load_screenshot_settings(self):
        """åŠ è½½å’ŒéªŒè¯æˆªå›¾è®¾ç½®"""
        screenshot_settings = self.config.get("screenshot_settings", {})
        # æ˜¯å¦å¯ç”¨ç½‘é¡µæˆªå›¾
        self.enable_screenshot = bool(screenshot_settings.get("enable_screenshot", True))
        # æˆªå›¾è´¨é‡ï¼šæ§åˆ¶æˆªå›¾çš„æ¸…æ™°åº¦å’Œæ–‡ä»¶å¤§å°
        self.screenshot_quality = max(
            10, min(100, screenshot_settings.get("screenshot_quality", 80))
        )
        # æˆªå›¾å®½åº¦å’Œé«˜åº¦ï¼šæ§åˆ¶æˆªå›¾çš„åˆ†è¾¨ç‡
        self.screenshot_width = max(
            320, min(4096, screenshot_settings.get("screenshot_width", 1280))
        )
        self.screenshot_height = max(
            240, min(4096, screenshot_settings.get("screenshot_height", 720))
        )
        # æ˜¯å¦æˆªå–æ•´é¡µï¼šæ§åˆ¶æ˜¯å¦æˆªå–å®Œæ•´çš„ç½‘é¡µå†…å®¹
        self.screenshot_full_page = bool(
            screenshot_settings.get("screenshot_full_page", False)
        )
        # æˆªå›¾ç­‰å¾…æ—¶é—´ï¼šé¡µé¢åŠ è½½å®Œæˆåç­‰å¾…çš„æ—¶é—´ï¼Œç¡®ä¿å†…å®¹å®Œæ•´æ˜¾ç¤º
        self.screenshot_wait_time = max(
            0, min(10000, screenshot_settings.get("screenshot_wait_time", 2000))
        )

        # éªŒè¯æˆªå›¾æ ¼å¼æ˜¯å¦æ”¯æŒ
        screenshot_format = screenshot_settings.get("screenshot_format", "jpeg").lower()
        if screenshot_format not in ["jpeg", "png"]:
            logger.warning(f"æ— æ•ˆçš„æˆªå›¾æ ¼å¼: {screenshot_format}ï¼Œå°†ä½¿ç”¨é»˜è®¤æ ¼å¼ jpeg")
            self.screenshot_format = "jpeg"
        else:
            self.screenshot_format = screenshot_format

        # æˆªå›¾è£å‰ªè®¾ç½®
        self.enable_crop = bool(screenshot_settings.get("enable_crop", False))
        # è£å‰ªåŒºåŸŸï¼Œæ ¼å¼ä¸º [left, top, right, bottom]
        crop_area = screenshot_settings.get("crop_area", [0, 0, self.screenshot_width, self.screenshot_height])

        # å¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„è£å‰ªåŒºåŸŸ
        if isinstance(crop_area, str):
            try:
                # å°è¯•å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºåˆ—è¡¨
                crop_area = eval(crop_area)
            except Exception as e:
                logger.warning(f"è§£æè£å‰ªåŒºåŸŸå¤±è´¥: {crop_area}ï¼Œé”™è¯¯: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
                crop_area = [0, 0, self.screenshot_width, self.screenshot_height]

        # éªŒè¯è£å‰ªåŒºåŸŸæ˜¯å¦æœ‰æ•ˆ
        if not isinstance(crop_area, list) or len(crop_area) != 4:
            logger.warning(f"æ— æ•ˆçš„è£å‰ªåŒºåŸŸ: {crop_area}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
            crop_area = [0, 0, self.screenshot_width, self.screenshot_height]

        self.crop_area = crop_area



    def _load_llm_settings(self):
        """åŠ è½½å’ŒéªŒè¯LLMè®¾ç½®"""
        llm_settings = self.config.get("llm_settings", {})
        # æ˜¯å¦å¯ç”¨LLMæ™ºèƒ½åˆ†æ
        self.llm_enabled = bool(llm_settings.get("llm_enabled", True))
        # LLMæä¾›å•†é…ç½®ï¼šæŒ‡å®šä½¿ç”¨çš„å¤§è¯­è¨€æ¨¡å‹æä¾›å•†
        self.llm_provider = llm_settings.get("llm_provider", "")
        # è‡ªå®šä¹‰æç¤ºè¯é…ç½®ï¼šå…è®¸ç”¨æˆ·è‡ªå®šä¹‰LLMåˆ†æçš„æç¤ºè¯
        self.custom_prompt = llm_settings.get("custom_prompt", "")

    def _load_group_settings(self):
        """åŠ è½½å’ŒéªŒè¯ç¾¤èŠè®¾ç½®"""
        group_settings = self.config.get("group_settings", {})
        # ç¾¤èŠé»‘åå•é…ç½®ï¼šç”¨äºæ§åˆ¶å“ªäº›ç¾¤èŠä¸å…è®¸ä½¿ç”¨æ’ä»¶
        group_blacklist_text = group_settings.get("group_blacklist", "")
        self.group_blacklist = self._parse_group_list(group_blacklist_text)

        # åˆå¹¶è½¬å‘é…ç½®ï¼šæ§åˆ¶æ˜¯å¦ä½¿ç”¨åˆå¹¶è½¬å‘åŠŸèƒ½å‘é€åˆ†æç»“æœ
        merge_forward_config = self.config.get("merge_forward_settings", {})
        self.merge_forward_enabled = {
            "group": bool(merge_forward_config.get("group", False)),
            "private": bool(merge_forward_config.get("private", False)),
            "include_screenshot": bool(
                merge_forward_config.get("include_screenshot", False)
            ),
        }

    def _load_translation_settings(self):
        """åŠ è½½å’ŒéªŒè¯ç¿»è¯‘è®¾ç½®"""
        translation_settings = self.config.get("translation_settings", {})
        self.enable_translation = bool(
            translation_settings.get("enable_translation", False)
        )

        # éªŒè¯ç›®æ ‡è¯­è¨€æ˜¯å¦æ”¯æŒ
        self.target_language = translation_settings.get("target_language", "zh").lower()
        valid_languages = ["zh", "en", "ja", "ko", "fr", "de", "es", "ru", "ar", "pt"]
        if self.target_language not in valid_languages:
            logger.warning(f"æ— æ•ˆçš„ç›®æ ‡è¯­è¨€: {self.target_language}ï¼Œå°†ä½¿ç”¨é»˜è®¤è¯­è¨€ zh")
            self.target_language = "zh"

        # ç¿»è¯‘æä¾›å•†é…ç½®
        self.translation_provider = translation_settings.get(
            "translation_provider", "llm"
        )
        # è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯ï¼šå…è®¸ç”¨æˆ·è‡ªå®šä¹‰ç¿»è¯‘çš„æç¤ºè¯
        self.custom_translation_prompt = translation_settings.get(
            "custom_translation_prompt", ""
        )

    def _load_cache_settings(self):
        """åŠ è½½å’ŒéªŒè¯ç¼“å­˜è®¾ç½®"""
        cache_settings = self.config.get("cache_settings", {})
        self.enable_cache = bool(cache_settings.get("enable_cache", True))
        # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼šæ§åˆ¶ç¼“å­˜ç»“æœçš„æœ‰æ•ˆæœŸ
        self.cache_expire_time = max(
            5, min(10080, cache_settings.get("cache_expire_time", 1440))
        )
        # æœ€å¤§ç¼“å­˜æ•°é‡ï¼šæ§åˆ¶ç¼“å­˜çš„æœ€å¤§æ¡ç›®æ•°ï¼Œé¿å…å†…å­˜å ç”¨è¿‡é«˜
        self.max_cache_size = max(
            10, min(1000, cache_settings.get("max_cache_size", 100))
        )
        # ç¼“å­˜é¢„åŠ è½½è®¾ç½®
        self.cache_preload_enabled = bool(cache_settings.get("cache_preload_enabled", False))
        self.cache_preload_count = max(
            0, min(100, cache_settings.get("cache_preload_count", 20))
        )

    def _load_content_extraction_settings(self):
        """åŠ è½½å’ŒéªŒè¯å†…å®¹æå–è®¾ç½®"""
        content_extraction_settings = self.config.get("content_extraction_settings", {})
        self.enable_specific_extraction = bool(
            content_extraction_settings.get("enable_specific_extraction", False)
        )
        # æå–ç±»å‹ï¼šæŒ‡å®šè¦æå–çš„å†…å®¹ç±»å‹
        extract_types_text = content_extraction_settings.get(
            "extract_types", "title\ncontent"
        )

        # ä½¿ç”¨è¾…åŠ©æ–¹æ³•å¤„ç†æå–ç±»å‹
        self.extract_types = WebAnalyzerUtils.parse_extract_types(extract_types_text)
        self.extract_types = WebAnalyzerUtils.validate_extract_types(self.extract_types)
        self.extract_types = WebAnalyzerUtils.ensure_minimal_extract_types(self.extract_types)
        self.extract_types = WebAnalyzerUtils.add_required_extract_types(self.extract_types)

    def _load_recall_settings(self):
        """åŠ è½½å’ŒéªŒè¯æ’¤å›è®¾ç½®"""
        recall_settings = self.config.get("recall_settings", {})
        # æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ’¤å›åŠŸèƒ½
        self.enable_recall = bool(recall_settings.get("enable_recall", True))
        # æ’¤å›ç±»å‹ï¼štime_based(å®šæ—¶æ’¤å›)æˆ–smart(æ™ºèƒ½æ’¤å›)
        self.recall_type = recall_settings.get("recall_type", "smart")
        # æ’¤å›å»¶è¿Ÿæ—¶é—´ï¼šè®¾ç½®åˆç†çš„èŒƒå›´ï¼Œé¿å…è¿‡çŸ­æˆ–è¿‡é•¿
        self.recall_time = max(0, min(120, recall_settings.get("recall_time", 10)))
        # æ˜¯å¦å¯ç”¨æ™ºèƒ½æ’¤å›
        self.smart_recall_enabled = bool(recall_settings.get("smart_recall_enabled", True))

    def _load_command_settings(self):
        """åŠ è½½å’ŒéªŒè¯å‘½ä»¤è®¾ç½®"""
        command_settings = self.config.get("command_settings", {})
        # è‡ªå®šä¹‰å‘½ä»¤åˆ«åé…ç½®
        custom_aliases = command_settings.get("custom_aliases", {})

        # å¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„è‡ªå®šä¹‰åˆ«å
        if isinstance(custom_aliases, str):
            try:
                # è§£æè‡ªå®šä¹‰åˆ«åï¼Œæ ¼å¼ä¸ºï¼šåŸå‘½ä»¤=åˆ«å1,åˆ«å2
                parsed_aliases = {}
                lines = custom_aliases.strip().split("\n")
                for line in lines:
                    line = line.strip()
                    if not line or "=" not in line:
                        continue
                    command, aliases = line.split("=", 1)
                    command = command.strip()
                    alias_list = [alias.strip() for alias in aliases.split(",") if alias.strip()]
                    if command and alias_list:
                        parsed_aliases[command] = alias_list
                self.custom_command_aliases = parsed_aliases
            except Exception as e:
                logger.warning(f"è§£æè‡ªå®šä¹‰å‘½ä»¤åˆ«åå¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
                self.custom_command_aliases = {}
        else:
            self.custom_command_aliases = custom_aliases

        # å‘½ä»¤è¡¥å…¨è®¾ç½®
        self.enable_command_completion = bool(command_settings.get("enable_completion", True))
        # å‘½ä»¤å¸®åŠ©è®¾ç½®
        self.enable_command_help = bool(command_settings.get("enable_help", True))
        # å‘½ä»¤å‚æ•°æç¤ºè®¾ç½®
        self.enable_param_hints = bool(command_settings.get("enable_param_hints", True))

    def _load_resource_settings(self):
        """åŠ è½½å’ŒéªŒè¯èµ„æºç®¡ç†è®¾ç½®"""
        resource_settings = self.config.get("resource_settings", {})
        # å†…å­˜ç›‘æ§è®¾ç½®
        self.enable_memory_monitor = bool(resource_settings.get("enable_memory_monitor", True))
        # å†…å­˜ä½¿ç”¨é˜ˆå€¼ï¼Œç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
        self.memory_threshold = max(0.0, min(100.0, resource_settings.get("memory_threshold", 80.0)))


    def _init_cache_manager(self):
        """åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨"""
        self.cache_manager = CacheManager(
            max_size=self.max_cache_size,
            expire_time=self.cache_expire_time,
            preload_enabled=self.cache_preload_enabled,
            preload_count=self.cache_preload_count
        )

    def _init_web_analyzer(self):
        """åˆå§‹åŒ–ç½‘é¡µåˆ†æå™¨"""
        self.analyzer = WebAnalyzer(
            max_content_length=self.max_content_length,
            timeout=self.timeout,
            user_agent=self.user_agent,
            proxy=self.proxy,
            retry_count=self.retry_count,
            retry_delay=self.retry_delay,
            enable_memory_monitor=self.enable_memory_monitor,  # å¯ç”¨å†…å­˜ç›‘æ§
            memory_threshold=self.memory_threshold,  # å†…å­˜ä½¿ç”¨é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤é˜ˆå€¼æ—¶è‡ªåŠ¨é‡Šæ”¾å†…å­˜
            enable_unified_domain=self.enable_unified_domain,  # æ˜¯å¦å¯ç”¨åŸŸåç»Ÿä¸€å¤„ç†
        )

    def _parse_domain_list(self, domain_text: str) -> list[str]:
        """å°†å¤šè¡ŒåŸŸåæ–‡æœ¬è½¬æ¢ä¸ºPythonåˆ—è¡¨"""
        return WebAnalyzerUtils.parse_domain_list(domain_text)

    def _parse_group_list(self, group_text: str) -> list[str]:
        """å°†å¤šè¡Œç¾¤èŠIDæ–‡æœ¬è½¬æ¢ä¸ºPythonåˆ—è¡¨"""
        return WebAnalyzerUtils.parse_group_list(group_text)

    def _is_group_blacklisted(self, group_id: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šç¾¤èŠæ˜¯å¦åœ¨é»‘åå•ä¸­"""
        if not group_id or not self.group_blacklist:
            return False
        return group_id in self.group_blacklist

    def _is_domain_allowed(self, url: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šURLçš„åŸŸåæ˜¯å¦å…è®¸è®¿é—®"""
        return WebAnalyzerUtils.is_domain_allowed(url, self.allowed_domains, self.blocked_domains)

    @filter.command("ç½‘é¡µåˆ†æ", alias={"åˆ†æ", "æ€»ç»“", "web", "analyze"})
    async def analyze_webpage(self, event: AstrMessageEvent):
        """æ‰‹åŠ¨è§¦å‘ç½‘é¡µåˆ†æå‘½ä»¤"""
        message_text = event.message_str

        # ä»æ¶ˆæ¯ä¸­æå–æ‰€æœ‰URL
        urls = self.analyzer.extract_urls(message_text, self.enable_no_protocol_url, self.default_protocol)
        if not urls:
            yield event.plain_result(
                "è¯·æä¾›è¦åˆ†æçš„ç½‘é¡µé“¾æ¥ï¼Œä¾‹å¦‚ï¼š/ç½‘é¡µåˆ†æ https://example.com"
            )
            return

        # éªŒè¯URLæ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œå¹¶è§„èŒƒåŒ–URL
        valid_urls = [self.analyzer.normalize_url(url) for url in urls if self.analyzer.is_valid_url(url)]
        # å»é‡ï¼Œé¿å…é‡å¤åˆ†æç›¸åŒURL
        valid_urls = list(set(valid_urls))
        if not valid_urls:
            yield event.plain_result("æ— æ•ˆçš„URLé“¾æ¥ï¼Œè¯·æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®")
            return

        # è¿‡æ»¤æ‰ä¸å…è®¸è®¿é—®çš„åŸŸå
        allowed_urls = [url for url in valid_urls if self._is_domain_allowed(url)]
        if not allowed_urls:
            yield event.plain_result("æ‰€æœ‰åŸŸåéƒ½ä¸åœ¨å…è®¸è®¿é—®çš„åˆ—è¡¨ä¸­ï¼Œæˆ–å·²è¢«ç¦æ­¢è®¿é—®")
            return

        # å‘é€å¤„ç†æç¤ºæ¶ˆæ¯ï¼Œå‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨åˆ†æ
        if len(allowed_urls) == 1:
            message = f"æ­£åœ¨åˆ†æç½‘é¡µ: {allowed_urls[0]}"
        else:
            message = f"æ­£åœ¨åˆ†æ{len(allowed_urls)}ä¸ªç½‘é¡µé“¾æ¥..."

        # ç›´æ¥è°ƒç”¨å‘é€æ–¹æ³•ï¼Œä¸ä½¿ç”¨yieldï¼Œè·å–message_idå’Œbotå®ä¾‹
        processing_message_id, bot = await self._send_processing_message(event, message)

        # æ‰¹é‡å¤„ç†æ‰€æœ‰å…è®¸è®¿é—®çš„URL
        async for result in self._batch_process_urls(event, allowed_urls, processing_message_id, bot):
            yield result

    @filter.llm_tool(name="analyze_webpage")
    async def analyze_webpage_tool(self, event: AstrMessageEvent, url: str) -> Any:
        """æ™ºèƒ½ç½‘é¡µåˆ†æå·¥å…·"""
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†LLMTOOLæ¨¡å¼ï¼Œæœªå¯ç”¨åˆ™ä¸æ‰§è¡Œ
        if self.analysis_mode != "LLMTOOL":
            logger.info(f"å½“å‰æœªå¯ç”¨LLMTOOLæ¨¡å¼ï¼Œæ‹’ç»analyze_webpage_toolè°ƒç”¨: {url}")
            yield event.plain_result("å½“å‰æœªå¯ç”¨ç½‘é¡µåˆ†æå·¥å…·æ¨¡å¼")
            return

        logger.info(f"æ”¶åˆ°analyze_webpage_toolè°ƒç”¨ï¼ŒåŸå§‹URL: {url}")

        # é¢„å¤„ç†URLï¼šå»é™¤å¯èƒ½çš„åå¼•å·ã€ç©ºæ ¼ç­‰
        processed_url = url.strip().strip("`")
        logger.info(f"é¢„å¤„ç†åçš„URL: {processed_url}")

        # è¡¥å…¨URLåè®®å¤´ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not processed_url.startswith(("http://", "https://")):
            processed_url = f"{self.default_protocol}://{processed_url}"
            logger.info(f"è¡¥å…¨åè®®å¤´åçš„URL: {processed_url}")

        # è§„èŒƒåŒ–URL
        normalized_url = self.analyzer.normalize_url(processed_url)
        logger.info(f"è§„èŒƒåŒ–åçš„URL: {normalized_url}")

        if not self.analyzer.is_valid_url(normalized_url):
            error_msg = f"æ— æ•ˆçš„URLé“¾æ¥ï¼Œè¯·æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®: {normalized_url}"
            logger.warning(error_msg)
            yield event.plain_result(error_msg)
            return

        # æ£€æŸ¥åŸŸåæ˜¯å¦å…è®¸è®¿é—®
        if not self._is_domain_allowed(normalized_url):
            error_msg = f"è¯¥åŸŸåä¸åœ¨å…è®¸è®¿é—®çš„åˆ—è¡¨ä¸­: {normalized_url}"
            logger.warning(error_msg)
            yield event.plain_result(error_msg)
            return

        # å‘é€å¤„ç†æç¤ºæ¶ˆæ¯ï¼Œå‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨åˆ†æ
        message = f"æ­£åœ¨åˆ†æç½‘é¡µ: {normalized_url}"
        processing_message_id, bot = await self._send_processing_message(event, message)

        # å¤„ç†å•ä¸ªURL
        async for result in self._batch_process_urls(event, [normalized_url], processing_message_id, bot):
            yield result

    @filter.llm_tool(name="analyze_webpage_with_decision")
    async def analyze_webpage_with_decision_tool(self, event: AstrMessageEvent, url: str, return_type: str = "both") -> Any:
        """æ™ºèƒ½ç½‘é¡µåˆ†æå·¥å…·ï¼ˆå¸¦è‡ªä¸»å†³ç­–ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†LLMTOOLæ¨¡å¼ï¼Œæœªå¯ç”¨åˆ™ä¸æ‰§è¡Œ
        if self.analysis_mode != "LLMTOOL":
            logger.info(f"å½“å‰æœªå¯ç”¨LLMTOOLæ¨¡å¼ï¼Œæ‹’ç»analyze_webpage_with_decision_toolè°ƒç”¨: {url}")
            yield event.plain_result("å½“å‰æœªå¯ç”¨ç½‘é¡µåˆ†æå·¥å…·æ¨¡å¼")
            return

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†LLMè‡ªä¸»å†³ç­–åŠŸèƒ½
        if not self.enable_llm_decision:
            logger.info(f"å½“å‰æœªå¯ç”¨LLMè‡ªä¸»å†³ç­–åŠŸèƒ½ï¼Œæ‹’ç»analyze_webpage_with_decision_toolè°ƒç”¨: {url}")
            yield event.plain_result("å½“å‰æœªå¯ç”¨LLMè‡ªä¸»å†³ç­–åŠŸèƒ½")
            return

        logger.info(f"æ”¶åˆ°analyze_webpage_with_decision_toolè°ƒç”¨ï¼ŒåŸå§‹URL: {url}ï¼Œè¿”å›ç±»å‹: {return_type}")

        # éªŒè¯è¿”å›ç±»å‹
        valid_return_types = ["analysis_only", "screenshot_only", "both"]
        if return_type not in valid_return_types:
            logger.warning(f"æ— æ•ˆçš„è¿”å›ç±»å‹: {return_type}ï¼Œä½¿ç”¨é»˜è®¤å€¼: both")
            return_type = "both"

        # é¢„å¤„ç†URLï¼šå»é™¤å¯èƒ½çš„åå¼•å·ã€ç©ºæ ¼ç­‰
        processed_url = url.strip().strip("`")
        logger.info(f"é¢„å¤„ç†åçš„URL: {processed_url}")

        # è¡¥å…¨URLåè®®å¤´ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not processed_url.startswith(("http://", "https://")):
            processed_url = f"{self.default_protocol}://{processed_url}"
            logger.info(f"è¡¥å…¨åè®®å¤´åçš„URL: {processed_url}")

        # è§„èŒƒåŒ–URL
        normalized_url = self.analyzer.normalize_url(processed_url)
        logger.info(f"è§„èŒƒåŒ–åçš„URL: {normalized_url}")

        if not self.analyzer.is_valid_url(normalized_url):
            error_msg = f"æ— æ•ˆçš„URLé“¾æ¥ï¼Œè¯·æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®: {normalized_url}"
            logger.warning(error_msg)
            yield event.plain_result(error_msg)
            return

        # æ£€æŸ¥åŸŸåæ˜¯å¦å…è®¸è®¿é—®
        if not self._is_domain_allowed(normalized_url):
            error_msg = f"è¯¥åŸŸåä¸åœ¨å…è®¸è®¿é—®çš„åˆ—è¡¨ä¸­: {normalized_url}"
            logger.warning(error_msg)
            yield event.plain_result(error_msg)
            return

        # å‘é€å¤„ç†æç¤ºæ¶ˆæ¯ï¼Œå‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨åˆ†æ
        message = f"æ­£åœ¨åˆ†æç½‘é¡µ: {normalized_url}"
        processing_message_id, bot = await self._send_processing_message(event, message)

        # åˆ›å»ºä¸´æ—¶WebAnalyzerå®ä¾‹
        async with WebAnalyzer(
            self.max_content_length,
            self.timeout,
            self.user_agent,
            self.proxy,
            self.retry_count,
            self.retry_delay,
        ) as analyzer:
            # å¤„ç†å•ä¸ªURLï¼Œè·å–åˆ†æç»“æœ
            result = await self._process_single_url(event, normalized_url, analyzer)

            # ä¿å­˜åŸå§‹send_content_typeé…ç½®
            original_send_content_type = self.send_content_type

            try:
                # æ ¹æ®LLMçš„å†³ç­–è®¾ç½®send_content_type
                self.send_content_type = return_type
                logger.info(f"ä¸´æ—¶è®¾ç½®send_content_typeä¸º: {return_type}")

                # å‘é€åˆ†æç»“æœ
                async for result_msg in self._send_analysis_result(event, [result]):
                    yield result_msg
            finally:
                # æ¢å¤åŸå§‹send_content_typeé…ç½®
                self.send_content_type = original_send_content_type
                logger.info(f"æ¢å¤send_content_typeä¸º: {original_send_content_type}")

            # æ™ºèƒ½æ’¤å›ï¼šåˆ†æå®Œæˆåç«‹å³æ’¤å›å¤„ç†ä¸­æ¶ˆæ¯
            if (self.enable_recall and
                self.recall_type == "smart" and
                self.smart_recall_enabled and
                processing_message_id and
                bot):
                try:
                    await bot.delete_msg(message_id=processing_message_id)
                    logger.info(f"å·²æ’¤å›å¤„ç†ä¸­æ¶ˆæ¯: {processing_message_id}")
                except Exception as e:
                    logger.error(f"æ’¤å›å¤„ç†ä¸­æ¶ˆæ¯å¤±è´¥: {e}")

    @filter.event_message_type(filter.EventMessageType.ALL)
    async def auto_detect_urls(self, event: AstrMessageEvent):
        """è‡ªåŠ¨æ£€æµ‹æ¶ˆæ¯ä¸­çš„URLé“¾æ¥å¹¶è¿›è¡Œåˆ†æ"""
        # æ£€æŸ¥åˆ†ææ¨¡å¼ï¼Œmanualæ¨¡å¼ä¸‹ä¸è¿›è¡Œè‡ªåŠ¨åˆ†æ
        if self.analysis_mode == "manual":
            return

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨åˆ†æåŠŸèƒ½ï¼ˆå…¼å®¹æ—§é…ç½®ï¼‰
        if not self.auto_analyze:
            return

        # æ£€æŸ¥æ˜¯å¦ä¸ºæŒ‡ä»¤è°ƒç”¨ï¼Œé¿å…é‡å¤å¤„ç†
        message_text = event.message_str.strip()

        # æ–¹æ³•1ï¼šè·³è¿‡ä»¥/å¼€å¤´çš„æŒ‡ä»¤æ¶ˆæ¯
        if message_text.startswith("/"):
            logger.info("æ£€æµ‹åˆ°æŒ‡ä»¤è°ƒç”¨ï¼Œè·³è¿‡è‡ªåŠ¨åˆ†æ")
            return

        # æ–¹æ³•2ï¼šæ£€æŸ¥äº‹ä»¶æ˜¯å¦æœ‰commandå±æ€§ï¼ˆæŒ‡ä»¤è°ƒç”¨æ—¶ä¼šæœ‰ï¼‰
        if hasattr(event, "command"):
            logger.info("æ£€æµ‹åˆ°commandå±æ€§ï¼Œè·³è¿‡è‡ªåŠ¨åˆ†æ")
            return

        # æ–¹æ³•3ï¼šæ£€æŸ¥åŸå§‹æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«ç½‘é¡µåˆ†æç›¸å…³æŒ‡ä»¤å…³é”®å­—
        raw_message = None
        if hasattr(event, "raw_message"):
            raw_message = str(event.raw_message)
        elif hasattr(event, "message_obj"):
            raw_message = str(event.message_obj)

        if raw_message:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç½‘é¡µåˆ†æç›¸å…³æŒ‡ä»¤
            command_keywords = ["ç½‘é¡µåˆ†æ", "/åˆ†æ", "/æ€»ç»“", "/web", "/analyze"]
            for keyword in command_keywords:
                if keyword in raw_message:
                    logger.info(f"æ£€æµ‹åˆ°æŒ‡ä»¤å…³é”®å­— {keyword}ï¼Œè·³è¿‡è‡ªåŠ¨åˆ†æ")
                    return

        # æ£€æŸ¥ç¾¤èŠæ˜¯å¦åœ¨é»‘åå•ä¸­ï¼ˆä»…ç¾¤èŠæ¶ˆæ¯ï¼‰
        group_id = None

        # æ–¹æ³•1ï¼šä»äº‹ä»¶å¯¹è±¡ç›´æ¥è·å–ç¾¤èŠID
        if hasattr(event, "group_id") and event.group_id:
            group_id = event.group_id
        # æ–¹æ³•2ï¼šä»æ¶ˆæ¯å¯¹è±¡è·å–ç¾¤èŠID
        elif (
            hasattr(event, "message_obj")
            and hasattr(event.message_obj, "group_id")
            and event.message_obj.group_id
        ):
            group_id = event.message_obj.group_id
        # æ–¹æ³•3ï¼šä»åŸå§‹æ¶ˆæ¯è·å–ç¾¤èŠID
        elif (
            hasattr(event, "raw_message")
            and hasattr(event.raw_message, "group_id")
            and event.raw_message.group_id
        ):
            group_id = event.raw_message.group_id

        # ç¾¤èŠåœ¨é»‘åå•ä¸­æ—¶é™é»˜å¿½ç•¥ï¼Œä¸è¿›è¡Œä»»ä½•å¤„ç†
        if group_id and self._is_group_blacklisted(group_id):
            return

        # ä»æ¶ˆæ¯ä¸­æå–æ‰€æœ‰URL
        urls = self.analyzer.extract_urls(message_text, self.enable_no_protocol_url, self.default_protocol)
        if not urls:
            return  # æ²¡æœ‰URLï¼Œä¸å¤„ç†

        # éªŒè¯URLæ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œå¹¶è§„èŒƒåŒ–URL
        valid_urls = [self.analyzer.normalize_url(url) for url in urls if self.analyzer.is_valid_url(url)]
        # å»é‡ï¼Œé¿å…é‡å¤åˆ†æç›¸åŒURL
        valid_urls = list(set(valid_urls))
        if not valid_urls:
            return  # æ²¡æœ‰æœ‰æ•ˆURLï¼Œä¸å¤„ç†

        # è¿‡æ»¤æ‰ä¸å…è®¸è®¿é—®çš„åŸŸå
        allowed_urls = [url for url in valid_urls if self._is_domain_allowed(url)]
        if not allowed_urls:
            return  # æ²¡æœ‰å…è®¸è®¿é—®çš„URLï¼Œä¸å¤„ç†

        # æ ¹æ®analysis_modeé…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨æ—§ç‰ˆç›´æ¥åˆ†ææ–¹å¼
        if self.analysis_mode == "LLMTOOL":
            # å¯ç”¨äº†LLMå‡½æ•°å·¥å…·æ¨¡å¼ï¼Œä¸ä½¿ç”¨æ—§ç‰ˆç›´æ¥åˆ†æ
            # è®©LLMè‡ªå·±å†³å®šæ˜¯å¦è°ƒç”¨analyze_webpageå·¥å…·
            logger.info(f"å¯ç”¨äº†LLMå‡½æ•°å·¥å…·æ¨¡å¼ï¼Œä¸è‡ªåŠ¨åˆ†æé“¾æ¥ï¼Œè®©LLMè‡ªå·±å†³å®š: {allowed_urls}")
            return
        else:
            # æœªå¯ç”¨LLMå‡½æ•°å·¥å…·æ¨¡å¼ï¼Œä½¿ç”¨æ—§ç‰ˆç›´æ¥åˆ†ææ–¹å¼
            # å‘é€å¤„ç†æç¤ºæ¶ˆæ¯ï¼Œå‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨åˆ†æ
            if len(allowed_urls) == 1:
                message = f"æ£€æµ‹åˆ°ç½‘é¡µé“¾æ¥ï¼Œæ­£åœ¨åˆ†æ: {allowed_urls[0]}"
            else:
                message = f"æ£€æµ‹åˆ°{len(allowed_urls)}ä¸ªç½‘é¡µé“¾æ¥ï¼Œæ­£åœ¨åˆ†æ..."

            # ç›´æ¥è°ƒç”¨å‘é€æ–¹æ³•ï¼Œä¸ä½¿ç”¨yieldï¼Œè·å–message_idå’Œbotå®ä¾‹
            processing_message_id, bot = await self._send_processing_message(event, message)

            # æ‰¹é‡å¤„ç†æ‰€æœ‰å…è®¸è®¿é—®çš„URL
            async for result in self._batch_process_urls(event, allowed_urls, processing_message_id, bot):
                yield result

    async def _use_llm_tool_mode(self, event: AstrMessageEvent, message_text: str, allowed_urls: list):
        """ä½¿ç”¨LLM Toolæ¨¡å¼å¤„ç†æ¶ˆæ¯"""
        try:
            # æ£€æŸ¥æ˜¯å¦æ”¯æŒtool_loop_agent
            if not hasattr(self.context, "tool_loop_agent"):
                logger.warning("ä¸æ”¯æŒtool_loop_agentï¼Œå›é€€åˆ°æ—§ç‰ˆè§£ææ–¹å¼")
                # å›é€€åˆ°æ—§ç‰ˆè§£ææ–¹å¼
                async for result in self._fallback_to_old_mode(event, allowed_urls):
                    yield result
                return

            # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„LLMæä¾›å•†ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨å½“å‰ä¼šè¯çš„æ¨¡å‹
            provider_id = self.llm_provider
            if not provider_id:
                umo = event.unified_msg_origin
                provider_id = await self.context.get_current_chat_provider_id(umo=umo)

            if not provider_id:
                logger.warning("æ— æ³•è·å–LLMæä¾›å•†ï¼Œå›é€€åˆ°æ—§ç‰ˆè§£ææ–¹å¼")
                # å›é€€åˆ°æ—§ç‰ˆè§£ææ–¹å¼
                async for result in self._fallback_to_old_mode(event, allowed_urls):
                    yield result
                return

            # æ„å»ºæç¤ºè¯
            prompt = f"è¯·æ ¹æ®ç”¨æˆ·æ¶ˆæ¯åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†æå…¶ä¸­çš„ç½‘é¡µé“¾æ¥ï¼Œå¦‚æœéœ€è¦åˆ†æï¼Œè¯·è°ƒç”¨analyze_webpageå·¥å…·è¿›è¡Œåˆ†æã€‚\n\nç”¨æˆ·æ¶ˆæ¯ï¼š{message_text}"

            # è°ƒç”¨tool_loop_agentè®©LLMå†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·
            logger.info(f"è°ƒç”¨tool_loop_agentå¤„ç†æ¶ˆæ¯ï¼ŒURLs: {allowed_urls}")

            try:
                # å°è¯•å¯¼å…¥ToolSet
                from astrbot.core.agent.tool import ToolSet
                # è°ƒç”¨tool_loop_agent
                llm_resp = await self.context.tool_loop_agent(
                    event=event,
                    chat_provider_id=provider_id,
                    prompt=prompt,
                    tools=ToolSet([]),  # ç©ºToolSetï¼Œä¾èµ–è£…é¥°å™¨æ³¨å†Œçš„å·¥å…·
                    max_steps=30,  # Agentæœ€å¤§æ‰§è¡Œæ­¥éª¤
                    tool_call_timeout=60,  # å·¥å…·è°ƒç”¨è¶…æ—¶æ—¶é—´
                )
                logger.info(f"tool_loop_agentæ‰§è¡Œå®Œæˆï¼Œç»“æœ: {llm_resp.completion_text if hasattr(llm_resp, 'completion_text') else 'æ— æ–‡æœ¬ç»“æœ'}")
            except Exception as e_inner:
                logger.error(f"è°ƒç”¨tool_loop_agentå¤±è´¥: {e_inner}")
                # å°è¯•ä¸ä½¿ç”¨ToolSetå‚æ•°è°ƒç”¨ï¼ˆå…¼å®¹ä¸åŒç‰ˆæœ¬ï¼‰
                try:
                    llm_resp = await self.context.tool_loop_agent(
                        event=event,
                        chat_provider_id=provider_id,
                        prompt=prompt,
                        max_steps=30,
                        tool_call_timeout=60,
                    )
                    logger.info(f"tool_loop_agentæ‰§è¡Œå®Œæˆï¼ˆæ— ToolSetï¼‰ï¼Œç»“æœ: {llm_resp.completion_text if hasattr(llm_resp, 'completion_text') else 'æ— æ–‡æœ¬ç»“æœ'}")
                except Exception as e_inner2:
                    logger.error(f"è°ƒç”¨tool_loop_agentï¼ˆæ— ToolSetï¼‰å¤±è´¥: {e_inner2}")
                    # å›é€€åˆ°æ—§ç‰ˆè§£ææ–¹å¼
                    logger.warning("tool_loop_agentè°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°æ—§ç‰ˆè§£ææ–¹å¼")
                    async for result in self._fallback_to_old_mode(event, allowed_urls):
                        yield result
                    return
        except Exception as e:
            logger.error(f"LLM Toolæ¨¡å¼å¤„ç†å¤±è´¥: {e}")
            # å‡ºé”™æ—¶å›é€€åˆ°æ—§ç‰ˆè§£ææ–¹å¼
            async for result in self._fallback_to_old_mode(event, allowed_urls):
                yield result

    async def _fallback_to_old_mode(self, event: AstrMessageEvent, allowed_urls: list):
        """å›é€€åˆ°æ—§ç‰ˆè§£ææ–¹å¼"""
        # å‘é€å¤„ç†æç¤ºæ¶ˆæ¯ï¼Œå‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨åˆ†æ
        if len(allowed_urls) == 1:
            message = f"æ£€æµ‹åˆ°ç½‘é¡µé“¾æ¥ï¼Œæ­£åœ¨åˆ†æ: {allowed_urls[0]}"
        else:
            message = f"æ£€æµ‹åˆ°{len(allowed_urls)}ä¸ªç½‘é¡µé“¾æ¥ï¼Œæ­£åœ¨åˆ†æ..."

        # ç›´æ¥è°ƒç”¨å‘é€æ–¹æ³•ï¼Œä¸ä½¿ç”¨yieldï¼Œè·å–message_idå’Œbotå®ä¾‹
        processing_message_id, bot = await self._send_processing_message(event, message)

        # æ‰¹é‡å¤„ç†æ‰€æœ‰å…è®¸è®¿é—®çš„URL
        async for result in self._batch_process_urls(event, allowed_urls, processing_message_id, bot):
            yield result



    async def _process_single_url(
        self, event: AstrMessageEvent, url: str, analyzer: WebAnalyzer
    ) -> dict:
        """å¤„ç†å•ä¸ªç½‘é¡µURLï¼Œç”Ÿæˆå®Œæ•´çš„åˆ†æç»“æœ"""
        try:
            # 1. æ£€æŸ¥ç¼“å­˜
            cached_result = self._check_cache(url)
            if cached_result:
                logger.info(f"ä½¿ç”¨URLç¼“å­˜ç»“æœ: {url}")
                return cached_result

            # 2. æŠ“å–ç½‘é¡µå†…å®¹
            html = await self._fetch_webpage_content(analyzer, url)
            if not html:
                error_msg = self._handle_error(ErrorType.NETWORK_ERROR, Exception("æ— æ³•è·å–ç½‘é¡µå†…å®¹"), url)
                return {
                    "url": url,
                    "result": error_msg,
                    "screenshot": None,
                }

            # 3. æå–ç»“æ„åŒ–å†…å®¹
            content_data = await self._extract_structured_content(analyzer, html, url)
            if not content_data:
                error_msg = self._handle_error(ErrorType.PARSING_ERROR, Exception("æ— æ³•è§£æç½‘é¡µå†…å®¹"), url)
                return {
                    "url": url,
                    "result": error_msg,
                    "screenshot": None,
                }

            # 4. è°ƒç”¨LLMè¿›è¡Œåˆ†æ
            analysis_result = await self._analyze_content(event, content_data)

            # 5. æå–ç‰¹å®šå†…å®¹
            analysis_result = await self._extract_and_add_specific_content(analysis_result, html, url)

            # 6. ç”Ÿæˆæˆªå›¾
            screenshot = await self._generate_screenshot(analyzer, url, analysis_result)

            # 7. åº”ç”¨ç»“æœè®¾ç½®
            final_result = self._apply_result_settings(analysis_result, url)

            # 8. å‡†å¤‡ç»“æœæ•°æ®
            result_data = {
                "url": url,
                "result": final_result,
                "screenshot": screenshot,
            }

            # 9. æ›´æ–°ç¼“å­˜
            self._update_cache(url, result_data, content_data["content"])

            return result_data
        except Exception as e:
            # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œç¡®ä¿æ–¹æ³•å§‹ç»ˆè¿”å›æœ‰æ•ˆç»“æœ
            error_type = self._get_error_type(e)
            error_msg = self._handle_error(error_type, e, url)
            return {
                "url": url,
                "result": error_msg,
                "screenshot": None,
            }

    async def _fetch_webpage_content(self, analyzer: WebAnalyzer, url: str) -> str:
        """æŠ“å–ç½‘é¡µHTMLå†…å®¹
        
        Args:
            analyzer: WebAnalyzerå®ä¾‹
            url: è¦æŠ“å–çš„URL
        
        Returns:
            ç½‘é¡µHTMLå†…å®¹ï¼Œå¦‚æœæŠ“å–å¤±è´¥åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        try:
            html = await analyzer.fetch_webpage(url)
            return html
        except Exception as e:
            logger.error(f"æŠ“å–ç½‘é¡µå¤±è´¥: {url}, é”™è¯¯: {e}")
            return ""

    async def _extract_structured_content(self, analyzer: WebAnalyzer, html: str, url: str) -> dict:
        """ä»HTMLä¸­æå–ç»“æ„åŒ–å†…å®¹
        
        Args:
            analyzer: WebAnalyzerå®ä¾‹
            html: ç½‘é¡µHTMLå†…å®¹
            url: ç½‘é¡µURL
        
        Returns:
            åŒ…å«ç»“æ„åŒ–å†…å®¹çš„å­—å…¸ï¼Œå¦‚æœæå–å¤±è´¥åˆ™è¿”å›None
        """
        try:
            content_data = analyzer.extract_content(html, url)
            return content_data
        except Exception as e:
            logger.error(f"æå–ç»“æ„åŒ–å†…å®¹å¤±è´¥: {url}, é”™è¯¯: {e}")
            return None

    async def _analyze_content(self, event: AstrMessageEvent, content_data: dict) -> str:
        """è°ƒç”¨LLMæˆ–åŸºç¡€åˆ†ææ–¹æ³•åˆ†æå†…å®¹
        
        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
            content_data: ç»“æ„åŒ–å†…å®¹æ•°æ®
        
        Returns:
            åˆ†æç»“æœæ–‡æœ¬
        """
        try:
            # å¦‚æœå¯ç”¨äº†ç¿»è¯‘åŠŸèƒ½ï¼Œå…ˆç¿»è¯‘å†…å®¹
            if self.enable_translation:
                try:
                    translated_content = await self._translate_content(
                        event, content_data["content"]
                    )
                    # åˆ›å»ºç¿»è¯‘åçš„å†…å®¹æ•°æ®å‰¯æœ¬
                    translated_content_data = content_data.copy()
                    translated_content_data["content"] = translated_content
                    # è°ƒç”¨LLMè¿›è¡Œåˆ†æï¼ˆä½¿ç”¨ç¿»è¯‘åçš„å†…å®¹ï¼‰
                    return await self.analyze_with_llm(event, translated_content_data)
                except Exception as e:
                    # ç¿»è¯‘å¤±è´¥æ—¶ï¼Œä½¿ç”¨åŸå§‹å†…å®¹è¿›è¡Œåˆ†æ
                    logger.warning(f"ç¿»è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹è¿›è¡Œåˆ†æ: {content_data['url']}, é”™è¯¯: {e}")

            # ç›´æ¥è°ƒç”¨LLMè¿›è¡Œåˆ†æ
            return await self.analyze_with_llm(event, content_data)
        except Exception as e:
            logger.error(f"åˆ†æå†…å®¹å¤±è´¥: {content_data['url']}, é”™è¯¯: {e}")
            return self.get_enhanced_analysis(content_data)

    async def _extract_and_add_specific_content(self, analysis_result: str, html: str, url: str) -> str:
        """æå–ç‰¹å®šç±»å‹å†…å®¹å¹¶æ·»åŠ åˆ°åˆ†æç»“æœä¸­
        
        Args:
            analysis_result: å½“å‰çš„åˆ†æç»“æœ
            html: ç½‘é¡µHTMLå†…å®¹
            url: ç½‘é¡µURL
        
        Returns:
            æ›´æ–°åçš„åˆ†æç»“æœ
        """
        try:
            specific_content = self._extract_specific_content(html, url)
            if specific_content:
                # åœ¨åˆ†æç»“æœä¸­æ·»åŠ ç‰¹å®šå†…å®¹
                specific_content_str = "\n\n**ç‰¹å®šå†…å®¹æå–**\n"

                # æ·»åŠ å›¾ç‰‡é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
                if "images" in specific_content and specific_content["images"]:
                    specific_content_str += (
                        f"\nğŸ“· å›¾ç‰‡é“¾æ¥ ({len(specific_content['images'])}):\n"
                    )
                    for img_url in specific_content["images"]:
                        specific_content_str += f"- {img_url}\n"

                # æ·»åŠ ç›¸å…³é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼Œæœ€å¤šæ˜¾ç¤º5ä¸ªï¼‰
                if "links" in specific_content and specific_content["links"]:
                    specific_content_str += (
                        f"\nğŸ”— ç›¸å…³é“¾æ¥ ({len(specific_content['links'])}):\n"
                    )
                    for link in specific_content["links"][:5]:
                        specific_content_str += f"- [{link['text']}]({link['url']})\n"

                # æ·»åŠ ä»£ç å—ï¼ˆå¦‚æœæœ‰ï¼Œæœ€å¤šæ˜¾ç¤º2ä¸ªï¼‰
                if (
                    "code_blocks" in specific_content
                    and specific_content["code_blocks"]
                ):
                    specific_content_str += (
                        f"\nğŸ’» ä»£ç å— ({len(specific_content['code_blocks'])}):\n"
                    )
                    for i, code in enumerate(specific_content["code_blocks"][:2]):
                        specific_content_str += f"```\n{code}\n```\n"

                # æ·»åŠ å…ƒä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                if "meta" in specific_content and specific_content["meta"]:
                    meta_info = specific_content["meta"]
                    specific_content_str += "\nğŸ“‹ å…ƒä¿¡æ¯:\n"
                    for key, value in meta_info.items():
                        if value:
                            specific_content_str += f"- {key}: {value}\n"

                    # å°†ç‰¹å®šå†…å®¹æ·»åŠ åˆ°åˆ†æç»“æœä¸­
                    analysis_result += specific_content_str
            return analysis_result
        except Exception as e:
            # ç‰¹å®šå†…å®¹æå–å¤±è´¥æ—¶ï¼Œè®°å½•è­¦å‘Šä½†ä¸å½±å“ä¸»åˆ†æç»“æœ
            logger.warning(f"ç‰¹å®šå†…å®¹æå–å¤±è´¥: {url}, é”™è¯¯: {e}")
            return analysis_result

    async def _generate_screenshot(self, analyzer: WebAnalyzer, url: str, analysis_result: str) -> bytes:
        """ç”Ÿæˆç½‘é¡µæˆªå›¾
        
        Args:
            analyzer: WebAnalyzerå®ä¾‹
            url: ç½‘é¡µURL
            analysis_result: å½“å‰çš„åˆ†æç»“æœ
        
        Returns:
            æˆªå›¾äºŒè¿›åˆ¶æ•°æ®ï¼Œå¦‚æœç”Ÿæˆå¤±è´¥åˆ™è¿”å›None
        """
        if not self.enable_screenshot or self.send_content_type == "analysis_only":
            return None

        try:
            screenshot = await analyzer.capture_screenshot(
                url,
                quality=self.screenshot_quality,
                width=self.screenshot_width,
                height=self.screenshot_height,
                full_page=self.screenshot_full_page,
                wait_time=self.screenshot_wait_time,
                format=self.screenshot_format,
            )

            # åº”ç”¨æˆªå›¾è£å‰ª
            if self.enable_crop and screenshot:
                try:
                    screenshot = analyzer.crop_screenshot(screenshot, tuple(self.crop_area))
                except Exception as e:
                    logger.warning(f"è£å‰ªæˆªå›¾å¤±è´¥: {url}, é”™è¯¯: {e}")

            return screenshot
        except Exception as e:
            # æˆªå›¾å¤±è´¥æ—¶ï¼Œè®°å½•é”™è¯¯ä½†ä¸å½±å“ä¸»åˆ†æç»“æœ
            error_msg = self._handle_error(ErrorType.SCREENSHOT_ERROR, e, url)
            # å°†æˆªå›¾é”™è¯¯ä¿¡æ¯æ·»åŠ åˆ°åˆ†æç»“æœä¸­
            analysis_result += f"\n\nâš ï¸ æˆªå›¾åŠŸèƒ½æç¤º: {error_msg.splitlines()[0]}"
            return None



    def _get_url_priority(self, url: str) -> int:
        """è¯„ä¼°URLçš„å¤„ç†ä¼˜å…ˆçº§"""
        return WebAnalyzerUtils.get_url_priority(url)

    def _render_result_template(self, result: str, url: str, template_type: str) -> str:
        """æ ¹æ®æ¨¡æ¿ç±»å‹æ¸²æŸ“åˆ†æç»“æœ"""
        # å®šä¹‰ä¸åŒæ¨¡æ¿çš„æ¸²æŸ“é€»è¾‘
        if template_type == "detailed":
            # è¯¦ç»†æ¨¡æ¿ï¼šåŒ…å«å®Œæ•´ä¿¡æ¯å’Œæ ¼å¼
            return f"ã€è¯¦ç»†åˆ†æç»“æœã€‘\n\nğŸ“Œ åˆ†æURLï¼š{url}\n\n{result}\n\n--- åˆ†æç»“æŸ ---"
        elif template_type == "compact":
            # ç´§å‡‘æ¨¡æ¿ï¼šç®€æ´å±•ç¤ºæ ¸å¿ƒä¿¡æ¯
            lines = result.splitlines()
            compact_result = []
            for line in lines:
                if line.strip() and not line.startswith("âš ï¸"):
                    compact_result.append(line)
                    if len(compact_result) >= 10:  # æœ€å¤šæ˜¾ç¤º10è¡Œ
                        break
            return f"ã€ç´§å‡‘åˆ†æç»“æœã€‘\n{url}\n\n" + "\n".join(compact_result) + "\n\n... æ›´å¤šå†…å®¹è¯·æŸ¥çœ‹å®Œæ•´åˆ†æ"
        elif template_type == "markdown":
            # Markdownæ¨¡æ¿ï¼šä½¿ç”¨Markdownæ ¼å¼
            return f"# ç½‘é¡µåˆ†æç»“æœ\n\n## URL\n{url}\n\n## åˆ†æå†…å®¹\n{result}\n\n---\n*åˆ†æå®Œæˆäº {self._get_current_time()}*"
        elif template_type == "simple":
            # ç®€å•æ¨¡æ¿ï¼šæç®€å±•ç¤º
            return f"{url}\n\n{result}"
        else:
            # é»˜è®¤æ¨¡æ¿ï¼šæ ‡å‡†æ ¼å¼
            return f"ã€ç½‘é¡µåˆ†æç»“æœã€‘\n{url}\n\n{result}"

    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    def _collapse_result(self, result: str) -> str:
        """æ ¹æ®é…ç½®æŠ˜å é•¿ç»“æœ"""
        if self.enable_collapsible and len(result) > self.collapse_threshold:
            # è®¡ç®—æŠ˜å ä½ç½®ï¼Œå¯»æ‰¾åˆé€‚çš„æ¢è¡Œç‚¹
            collapse_pos = self.collapse_threshold
            # å°½é‡åœ¨æ®µè½ç»“æŸå¤„æŠ˜å 
            while collapse_pos < len(result) and result[collapse_pos] != "\n":
                collapse_pos += 1
            if collapse_pos == len(result):
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ¢è¡Œï¼Œç›´æ¥æˆªæ–­
                collapse_pos = self.collapse_threshold

            collapsed_content = result[:collapse_pos]
            remaining_content = result[collapse_pos:]
            return f"{collapsed_content}\n\n[å±•å¼€å…¨æ–‡]\n\n{remaining_content}"
        return result

    def _apply_result_settings(self, result: str, url: str) -> str:
        """åº”ç”¨æ‰€æœ‰ç»“æœè®¾ç½®ï¼ˆæ¨¡æ¿æ¸²æŸ“å’ŒæŠ˜å ï¼‰"""
        # é¦–å…ˆåº”ç”¨æ¨¡æ¿æ¸²æŸ“
        rendered_result = self._render_result_template(result, url, self.result_template)
        # ç„¶ååº”ç”¨ç»“æœæŠ˜å 
        final_result = self._collapse_result(rendered_result)
        return final_result

    @filter.command("web_help", alias={"ç½‘é¡µåˆ†æå¸®åŠ©", "ç½‘é¡µåˆ†æå‘½ä»¤"})
    async def show_help(self, event: AstrMessageEvent):
        """æ˜¾ç¤ºæ’ä»¶çš„æ‰€æœ‰å¯ç”¨å‘½ä»¤å’Œå¸®åŠ©ä¿¡æ¯"""
        help_text = """ã€ç½‘é¡µåˆ†ææ’ä»¶å‘½ä»¤å¸®åŠ©ã€‘
        
ğŸ“‹ æ ¸å¿ƒåˆ†æå‘½ä»¤
ğŸ” /ç½‘é¡µåˆ†æ <URL1> <URL2>... - æ‰‹åŠ¨åˆ†ææŒ‡å®šç½‘é¡µé“¾æ¥
   åˆ«åï¼š/åˆ†æ, /æ€»ç»“, /web, /analyze
   ç¤ºä¾‹ï¼š/ç½‘é¡µåˆ†æ https://example.com
        
ğŸ“‹ é…ç½®ç®¡ç†å‘½ä»¤
ğŸ› ï¸ /web_config - æŸ¥çœ‹å½“å‰æ’ä»¶é…ç½®
   åˆ«åï¼š/ç½‘é¡µåˆ†æé…ç½®, /ç½‘é¡µåˆ†æè®¾ç½®
   ç¤ºä¾‹ï¼š/web_config
        
ğŸ“‹ ç¼“å­˜ç®¡ç†å‘½ä»¤
ğŸ—‘ï¸ /web_cache [clear] - ç®¡ç†åˆ†æç»“æœç¼“å­˜
   åˆ«åï¼š/ç½‘é¡µç¼“å­˜, /æ¸…ç†ç¼“å­˜
   é€‰é¡¹ï¼š
     - clear: æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
   ç¤ºä¾‹ï¼š/web_cache clear
        
ğŸ“‹ ç¾¤èŠç®¡ç†å‘½ä»¤
ğŸ‘¥ /group_blacklist [add/remove/clear] <ç¾¤å·> - ç®¡ç†ç¾¤èŠé»‘åå•
   åˆ«åï¼š/ç¾¤é»‘åå•, /é»‘åå•
   é€‰é¡¹ï¼š
     - (ç©º): æŸ¥çœ‹å½“å‰é»‘åå•
     - add <ç¾¤å·>: æ·»åŠ ç¾¤èŠåˆ°é»‘åå•
     - remove <ç¾¤å·>: ä»é»‘åå•ç§»é™¤ç¾¤èŠ
     - clear: æ¸…ç©ºé»‘åå•
   ç¤ºä¾‹ï¼š/ç¾¤é»‘åå• add 123456789
        
ğŸ“‹ å¯¼å‡ºåŠŸèƒ½å‘½ä»¤
ğŸ“¤ /web_export - å¯¼å‡ºåˆ†æç»“æœ
   åˆ«åï¼š/å¯¼å‡ºåˆ†æç»“æœ, /ç½‘é¡µå¯¼å‡º
   ç¤ºä¾‹ï¼š/web_export
        
ğŸ“‹ æµ‹è¯•åŠŸèƒ½å‘½ä»¤
ğŸ“‹ /test_merge - æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½
   åˆ«åï¼š/æµ‹è¯•åˆå¹¶è½¬å‘, /æµ‹è¯•è½¬å‘
   ç¤ºä¾‹ï¼š/test_merge
        
ğŸ“‹ å¸®åŠ©å‘½ä»¤
â“ /web_help - æ˜¾ç¤ºæœ¬å¸®åŠ©ä¿¡æ¯
   åˆ«åï¼š/ç½‘é¡µåˆ†æå¸®åŠ©, /ç½‘é¡µåˆ†æå‘½ä»¤
   ç¤ºä¾‹ï¼š/web_help
        
ğŸ’¡ ä½¿ç”¨æç¤ºï¼š
- æ‰€æœ‰å‘½ä»¤æ”¯æŒTabè¡¥å…¨ï¼ˆå¦‚æœå®¢æˆ·ç«¯æ”¯æŒï¼‰
- å‘½ä»¤å‚æ•°æ”¯æŒæç¤ºåŠŸèƒ½
- å¯ä»¥è‡ªå®šä¹‰å‘½ä»¤åˆ«å
        
ğŸ”§ é…ç½®æç¤ºï¼š
- åœ¨AstrBotç®¡ç†é¢æ¿ä¸­å¯ä»¥é…ç½®æ’ä»¶çš„å„é¡¹åŠŸèƒ½
- æ”¯æŒè‡ªå®šä¹‰å‘½ä»¤åˆ«å
- å¯ä»¥è°ƒæ•´åˆ†æç»“æœæ¨¡æ¿å’Œæ˜¾ç¤ºæ–¹å¼
"""

        yield event.plain_result(help_text)
        logger.info("æ˜¾ç¤ºå‘½ä»¤å¸®åŠ©ä¿¡æ¯")

    def _get_available_commands(self) -> dict:
        """è·å–æ‰€æœ‰å¯ç”¨å‘½ä»¤çš„ä¿¡æ¯
        
        Returns:
            åŒ…å«æ‰€æœ‰å‘½ä»¤ä¿¡æ¯çš„å­—å…¸
        """
        return {
            "ç½‘é¡µåˆ†æ": {
                "aliases": ["åˆ†æ", "æ€»ç»“", "web", "analyze"],
                "description": "æ‰‹åŠ¨åˆ†ææŒ‡å®šç½‘é¡µé“¾æ¥",
                "usage": "/ç½‘é¡µåˆ†æ <URL1> <URL2>...",
                "options": [],
                "example": "/ç½‘é¡µåˆ†æ https://example.com"
            },
            "web_config": {
                "aliases": ["ç½‘é¡µåˆ†æé…ç½®", "ç½‘é¡µåˆ†æè®¾ç½®"],
                "description": "æŸ¥çœ‹å½“å‰æ’ä»¶é…ç½®",
                "usage": "/web_config",
                "options": [],
                "example": "/web_config"
            },
            "web_cache": {
                "aliases": ["ç½‘é¡µç¼“å­˜", "æ¸…ç†ç¼“å­˜"],
                "description": "ç®¡ç†åˆ†æç»“æœç¼“å­˜",
                "usage": "/web_cache [clear]",
                "options": ["clear"],
                "example": "/web_cache clear"
            },
            "group_blacklist": {
                "aliases": ["ç¾¤é»‘åå•", "é»‘åå•"],
                "description": "ç®¡ç†ç¾¤èŠé»‘åå•",
                "usage": "/group_blacklist [add/remove/clear] <ç¾¤å·>",
                "options": ["add", "remove", "clear"],
                "example": "/ç¾¤é»‘åå• add 123456789"
            },
            "web_export": {
                "aliases": ["å¯¼å‡ºåˆ†æç»“æœ", "ç½‘é¡µå¯¼å‡º"],
                "description": "å¯¼å‡ºåˆ†æç»“æœ",
                "usage": "/web_export",
                "options": [],
                "example": "/web_export"
            },
            "test_merge": {
                "aliases": ["æµ‹è¯•åˆå¹¶è½¬å‘", "æµ‹è¯•è½¬å‘"],
                "description": "æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½",
                "usage": "/test_merge",
                "options": [],
                "example": "/test_merge"
            },
            "web_help": {
                "aliases": ["ç½‘é¡µåˆ†æå¸®åŠ©", "ç½‘é¡µåˆ†æå‘½ä»¤"],
                "description": "æ˜¾ç¤ºå‘½ä»¤å¸®åŠ©ä¿¡æ¯",
                "usage": "/web_help",
                "options": [],
                "example": "/web_help"
            }
        }

    def _get_command_completions(self, input_text: str) -> list:
        """æ ¹æ®ç”¨æˆ·è¾“å…¥è·å–å‘½ä»¤è¡¥å…¨å»ºè®®"""
        if not input_text.startswith("/"):
            return []

        # æå–å½“å‰è¾“å…¥çš„å‘½ä»¤å‰ç¼€
        input_cmd = input_text[1:].lower()
        completions = []

        # è·å–æ‰€æœ‰å¯ç”¨å‘½ä»¤
        commands = self._get_available_commands()

        # æ£€æŸ¥ä¸»å‘½ä»¤
        for cmd_name, cmd_info in commands.items():
            if cmd_name.lower().startswith(input_cmd):
                completions.append(f"/{cmd_name}")

            # æ£€æŸ¥åˆ«å
            for alias in cmd_info["aliases"]:
                if alias.lower().startswith(input_cmd):
                    completions.append(f"/{alias}")

        return completions

    def _get_param_hints(self, command: str, current_params: list) -> list:
        """è·å–å‘½ä»¤å‚æ•°æç¤º"""
        commands = self._get_available_commands()

        # æŸ¥æ‰¾å‘½ä»¤ä¿¡æ¯
        cmd_info = None
        for cmd_name, info in commands.items():
            if command == cmd_name or command in info["aliases"]:
                cmd_info = info
                break

        if not cmd_info:
            return []

        # æ ¹æ®å‘½ä»¤å’Œå·²è¾“å…¥å‚æ•°è¿”å›æç¤º
        if cmd_name == "group_blacklist":
            if len(current_params) == 0:
                return ["add", "remove", "clear"]
            elif len(current_params) == 1 and current_params[0] in ["add", "remove"]:
                return ["<ç¾¤å·>"]
        elif cmd_name == "web_cache":
            if len(current_params) == 0:
                return ["clear"]

        return []

    async def _batch_process_urls(self, event: AstrMessageEvent, urls: list[str], processing_message_id: int | None = None, bot = None):
        """æ‰¹é‡å¤„ç†å¤šä¸ªURLï¼Œå®ç°é«˜æ•ˆçš„å¹¶å‘åˆ†æ"""
        # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
        analysis_results = []

        # è¿‡æ»¤æ‰æ­£åœ¨å¤„ç†çš„URLï¼Œé¿å…é‡å¤åˆ†æ
        filtered_urls = []
        for url in urls:
            if url not in self.processing_urls:
                filtered_urls.append(url)
                # æ·»åŠ åˆ°æ­£åœ¨å¤„ç†çš„é›†åˆä¸­ï¼Œé˜²æ­¢é‡å¤å¤„ç†
                self.processing_urls.add(url)
            else:
                logger.info(f"URL {url} æ­£åœ¨å¤„ç†ä¸­ï¼Œè·³è¿‡é‡å¤åˆ†æ")

        # å¦‚æœæ‰€æœ‰URLéƒ½æ­£åœ¨å¤„ç†ä¸­ï¼Œç›´æ¥è¿”å›
        if not filtered_urls:
            return

        # æ ¹æ®ä¼˜å…ˆçº§å¯¹URLè¿›è¡Œæ’åº
        if self.enable_priority_scheduling:
            filtered_urls = sorted(filtered_urls, key=lambda url: self._get_url_priority(url), reverse=True)
            logger.info(f"URLä¼˜å…ˆçº§æ’åºå®Œæˆ: {[(url, self._get_url_priority(url)) for url in filtered_urls]}")

        try:
            # åˆ›å»ºWebAnalyzerå®ä¾‹ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºæ­£ç¡®é‡Šæ”¾
            async with WebAnalyzer(
                self.max_content_length,
                self.timeout,
                self.user_agent,
                self.proxy,
                self.retry_count,
                self.retry_delay,
            ) as analyzer:
                # ä½¿ç”¨asyncio.gatherå¹¶å‘å¤„ç†å¤šä¸ªURLï¼Œæé«˜æ•ˆç‡
                import asyncio

                # åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°
                concurrency = self.max_concurrency
                if self.dynamic_concurrency:
                    # æ ¹æ®URLæ•°é‡åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°
                    # è®¡ç®—åˆç†çš„å¹¶å‘æ•°ï¼šURLæ•°é‡çš„å¹³æ–¹æ ¹ï¼Œä¸è¶…è¿‡max_concurrency
                    dynamic_concurrency = min(self.max_concurrency, max(1, int(len(filtered_urls) ** 0.5) + 1))
                    concurrency = dynamic_concurrency

                logger.info(f"ä½¿ç”¨å¹¶å‘æ•°: {concurrency} å¤„ç† {len(filtered_urls)} ä¸ªURL")

                # åˆ†æ‰¹æ¬¡å¤„ç†URLï¼Œæ§åˆ¶å¹¶å‘æ•°
                batch_size = concurrency
                results = []

                # å¦‚æœå¹¶å‘æ•°å¤§äºç­‰äºURLæ•°é‡ï¼Œç›´æ¥å¤„ç†æ‰€æœ‰URL
                if batch_size >= len(filtered_urls):
                    tasks = [self._process_single_url(event, url, analyzer) for url in filtered_urls]
                    results = await asyncio.gather(*tasks)
                else:
                    # åˆ†æ‰¹æ¬¡å¤„ç†
                    for i in range(0, len(filtered_urls), batch_size):
                        batch_urls = filtered_urls[i:i+batch_size]
                        logger.info(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(filtered_urls) + batch_size - 1)//batch_size}: {batch_urls}")
                        tasks = [self._process_single_url(event, url, analyzer) for url in batch_urls]
                        batch_results = await asyncio.gather(*tasks)
                        results.extend(batch_results)

                analysis_results = results

            # å‘é€æ‰€æœ‰åˆ†æç»“æœ
            async for result in self._send_analysis_result(event, analysis_results):
                yield result
        finally:
            # æ— è®ºå¤„ç†æˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼Œéƒ½è¦ä»å¤„ç†é›†åˆä¸­ç§»é™¤URL
            for url in filtered_urls:
                if url in self.processing_urls:
                    self.processing_urls.remove(url)

            # æ™ºèƒ½æ’¤å›ï¼šåˆ†æå®Œæˆåç«‹å³æ’¤å›å¤„ç†ä¸­æ¶ˆæ¯
            if (self.enable_recall and
                self.recall_type == "smart" and
                self.smart_recall_enabled and
                processing_message_id and
                bot):
                try:
                    logger.info(f"æ™ºèƒ½æ’¤å›ï¼šåˆ†æå®Œæˆï¼Œç«‹å³æ’¤å›å¤„ç†ä¸­æ¶ˆæ¯ï¼Œmessage_id: {processing_message_id}")
                    await bot.delete_msg(message_id=processing_message_id)
                    logger.info(f"æ™ºèƒ½æ’¤å›æˆåŠŸï¼Œå·²æ’¤å›æ¶ˆæ¯: {processing_message_id}")
                except Exception as e:
                    logger.error(f"æ™ºèƒ½æ’¤å›æ¶ˆæ¯å¤±è´¥: {e}")

    def _get_analysis_template(self, content_type: str, emoji_prefix: str, max_length: int) -> str:
        """æ ¹æ®å†…å®¹ç±»å‹è·å–ç›¸åº”çš„åˆ†ææ¨¡æ¿"""
        # å®šä¹‰å¤šç§åˆ†ææ¨¡æ¿
        templates = {
            "æ–°é—»èµ„è®¯": f"""è¯·å¯¹ä»¥ä¸‹æ–°é—»èµ„è®¯è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**æ–°é—»å†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **æ ¸å¿ƒäº‹ä»¶**ï¼šç”¨50-100å­—æ¦‚æ‹¬æ–°é—»çš„æ ¸å¿ƒäº‹ä»¶å’ŒèƒŒæ™¯
2. **å…³é”®ä¿¡æ¯**ï¼šæå–3-5ä¸ªæœ€é‡è¦çš„äº‹å®è¦ç‚¹
3. **äº‹ä»¶å½±å“**ï¼šåˆ†æäº‹ä»¶å¯èƒ½äº§ç”Ÿçš„å½±å“å’Œæ„ä¹‰
4. **ç›¸å…³èƒŒæ™¯**ï¼šè¡¥å……å¿…è¦çš„ç›¸å…³èƒŒæ™¯ä¿¡æ¯
5. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜è¿™æ¡æ–°é—»å¯¹å“ªäº›äººç¾¤æœ€æœ‰ä»·å€¼

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚""",

            "æ•™ç¨‹æŒ‡å—": f"""è¯·å¯¹ä»¥ä¸‹æ•™ç¨‹æŒ‡å—è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**æ•™ç¨‹å†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **æ ¸å¿ƒç›®æ ‡**ï¼šç”¨50-100å­—æ¦‚æ‹¬æ•™ç¨‹çš„æ ¸å¿ƒç›®æ ‡å’Œé€‚ç”¨åœºæ™¯
2. **å­¦ä¹ ä»·å€¼**ï¼šåˆ†æè¯¥æ•™ç¨‹å¯¹å­¦ä¹ è€…çš„ä»·å€¼å’Œæ„ä¹‰
3. **å…³é”®æ­¥éª¤**ï¼šæå–æ•™ç¨‹çš„ä¸»è¦æ­¥éª¤å’Œå…³é”®ç‚¹
4. **æŠ€æœ¯è¦ç‚¹**ï¼šæ€»ç»“æ•™ç¨‹ä¸­æ¶‰åŠçš„æ ¸å¿ƒæŠ€æœ¯æˆ–çŸ¥è¯†ç‚¹
5. **æ³¨æ„äº‹é¡¹**ï¼šæ•´ç†æ•™ç¨‹ä¸­çš„é‡è¦æç¤ºå’Œæ³¨æ„äº‹é¡¹
6. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜é€‚åˆå­¦ä¹ è¯¥æ•™ç¨‹çš„äººç¾¤

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚""",

            "ä¸ªäººåšå®¢": f"""è¯·å¯¹ä»¥ä¸‹ä¸ªäººåšå®¢è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**åšå®¢å†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **æ ¸å¿ƒè§‚ç‚¹**ï¼šç”¨50-100å­—æ¦‚æ‹¬åšå®¢ä½œè€…çš„æ ¸å¿ƒè§‚ç‚¹å’Œç«‹åœº
2. **ä¸»è¦å†…å®¹**ï¼šæå–åšå®¢çš„ä¸»è¦å†…å®¹å’Œè®ºè¿°è¦ç‚¹
3. **å†™ä½œé£æ ¼**ï¼šåˆ†æåšå®¢çš„å†™ä½œé£æ ¼å’Œç‰¹ç‚¹
4. **ä»·å€¼è¯„ä¼°**ï¼šè¯„ä»·åšå®¢å†…å®¹çš„ä»·å€¼å’Œå®ç”¨æ€§
5. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜é€‚åˆé˜…è¯»è¯¥åšå®¢çš„äººç¾¤

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚""",

            "äº§å“ä»‹ç»": f"""è¯·å¯¹ä»¥ä¸‹äº§å“ä»‹ç»è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**äº§å“å†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **äº§å“å®šä½**ï¼šç”¨50-100å­—æ¦‚æ‹¬äº§å“çš„å®šä½å’Œæ ¸å¿ƒä»·å€¼
2. **æ ¸å¿ƒåŠŸèƒ½**ï¼šæå–äº§å“çš„ä¸»è¦åŠŸèƒ½å’Œç‰¹æ€§
3. **æŠ€æœ¯å‚æ•°**ï¼šæ€»ç»“äº§å“çš„å…³é”®æŠ€æœ¯å‚æ•°å’Œè§„æ ¼
4. **é€‚ç”¨åœºæ™¯**ï¼šåˆ†æäº§å“çš„é€‚ç”¨åœºæ™¯å’Œä½¿ç”¨æ–¹æ³•
5. **ç«äº‰ä¼˜åŠ¿**ï¼šåˆ†æäº§å“ç›¸æ¯”åŒç±»äº§å“çš„ä¼˜åŠ¿
6. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜é€‚åˆä½¿ç”¨è¯¥äº§å“çš„äººç¾¤

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚""",

            "æŠ€æœ¯æ–‡æ¡£": f"""è¯·å¯¹ä»¥ä¸‹æŠ€æœ¯æ–‡æ¡£è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**æ–‡æ¡£å†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **æ–‡æ¡£ç›®çš„**ï¼šç”¨50-100å­—æ¦‚æ‹¬æ–‡æ¡£çš„æ ¸å¿ƒç›®çš„å’Œé€‚ç”¨èŒƒå›´
2. **æ ¸å¿ƒæ¦‚å¿µ**ï¼šæå–æ–‡æ¡£ä¸­æ¶‰åŠçš„æ ¸å¿ƒæ¦‚å¿µå’Œæœ¯è¯­
3. **æŠ€æœ¯æ¶æ„**ï¼šåˆ†ææ–‡æ¡£ä¸­æè¿°çš„æŠ€æœ¯æ¶æ„å’Œè®¾è®¡æ€è·¯
4. **ä½¿ç”¨æ–¹æ³•**ï¼šæ€»ç»“æ–‡æ¡£ä¸­ä»‹ç»çš„ä½¿ç”¨æ–¹æ³•å’Œæœ€ä½³å®è·µ
5. **å…³é”®ç‰¹æ€§**ï¼šæ•´ç†æ–‡æ¡£ä¸­æåŠçš„å…³é”®ç‰¹æ€§å’ŒåŠŸèƒ½
6. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜é€‚åˆé˜…è¯»è¯¥æ–‡æ¡£çš„äººç¾¤

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚""",

            "å­¦æœ¯è®ºæ–‡": f"""è¯·å¯¹ä»¥ä¸‹å­¦æœ¯è®ºæ–‡è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**è®ºæ–‡å†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **ç ”ç©¶èƒŒæ™¯**ï¼šç”¨50-100å­—æ¦‚æ‹¬è®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯å’Œæ„ä¹‰
2. **æ ¸å¿ƒé—®é¢˜**ï¼šæå–è®ºæ–‡è¯•å›¾è§£å†³çš„æ ¸å¿ƒé—®é¢˜
3. **ç ”ç©¶æ–¹æ³•**ï¼šåˆ†æè®ºæ–‡é‡‡ç”¨çš„ç ”ç©¶æ–¹æ³•å’ŒæŠ€æœ¯è·¯çº¿
4. **ä¸»è¦å‘ç°**ï¼šæ€»ç»“è®ºæ–‡çš„ä¸»è¦ç ”ç©¶å‘ç°å’Œç»“è®º
5. **åˆ›æ–°ç‚¹**ï¼šåˆ†æè®ºæ–‡çš„åˆ›æ–°ç‚¹å’Œè´¡çŒ®
6. **é€‚ç”¨é¢†åŸŸ**ï¼šè¯´æ˜è¯¥ç ”ç©¶æˆæœçš„é€‚ç”¨é¢†åŸŸå’Œåº”ç”¨å‰æ™¯

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚""",

            "å•†ä¸šåˆ†æ": f"""è¯·å¯¹ä»¥ä¸‹å•†ä¸šåˆ†æè¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**åˆ†æå†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **æ ¸å¿ƒä¸»é¢˜**ï¼šç”¨50-100å­—æ¦‚æ‹¬åˆ†ææŠ¥å‘Šçš„æ ¸å¿ƒä¸»é¢˜å’Œç›®çš„
2. **å¸‚åœºè¶‹åŠ¿**ï¼šæå–æŠ¥å‘Šä¸­æŒ‡å‡ºçš„ä¸»è¦å¸‚åœºè¶‹åŠ¿å’Œå˜åŒ–
3. **å…³é”®æ•°æ®**ï¼šæ€»ç»“æŠ¥å‘Šä¸­çš„å…³é”®æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯
4. **åˆ†æç»“è®º**ï¼šåˆ†ææŠ¥å‘Šçš„ä¸»è¦ç»“è®ºå’Œé¢„æµ‹
5. **å•†ä¸šä»·å€¼**ï¼šè¯„ä»·æŠ¥å‘Šå¯¹ä¼ä¸šå’ŒæŠ•èµ„è€…çš„ä»·å€¼
6. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜é€‚åˆé˜…è¯»è¯¥æŠ¥å‘Šçš„äººç¾¤

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚""",

            "å¨±ä¹èµ„è®¯": f"""è¯·å¯¹ä»¥ä¸‹å¨±ä¹èµ„è®¯è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**å¨±ä¹å†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **æ ¸å¿ƒäº‹ä»¶**ï¼šç”¨50-100å­—æ¦‚æ‹¬å¨±ä¹èµ„è®¯çš„æ ¸å¿ƒäº‹ä»¶
2. **å…³é”®ä¿¡æ¯**ï¼šæå–3-5ä¸ªæœ€é‡è¦çš„äº‹å®è¦ç‚¹
3. **ç›¸å…³èƒŒæ™¯**ï¼šè¡¥å……å¿…è¦çš„ç›¸å…³èƒŒæ™¯ä¿¡æ¯ï¼ˆå¦‚æ˜æ˜ŸèƒŒæ™¯ã€ä½œå“ä¿¡æ¯ç­‰ï¼‰
4. **å—ä¼—ä»·å€¼**ï¼šåˆ†æè¯¥èµ„è®¯å¯¹ä¸åŒå—ä¼—ç¾¤ä½“çš„å¸å¼•åŠ›å’Œä»·å€¼
5. **è¡Œä¸šå½±å“**ï¼šç®€è¦åˆ†æè¯¥äº‹ä»¶å¯¹å¨±ä¹è¡Œä¸šçš„å¯èƒ½å½±å“
6. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜è¿™æ¡èµ„è®¯å¯¹å“ªäº›äººç¾¤æœ€æœ‰ä»·å€¼

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç”ŸåŠ¨æœ‰è¶£ï¼Œç¬¦åˆå¨±ä¹èµ„è®¯çš„ç‰¹ç‚¹
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚""",

            "ä½“è‚²æ–°é—»": f"""è¯·å¯¹ä»¥ä¸‹ä½“è‚²æ–°é—»è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**ä½“è‚²å†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **æ ¸å¿ƒäº‹ä»¶**ï¼šç”¨50-100å­—æ¦‚æ‹¬ä½“è‚²æ–°é—»çš„æ ¸å¿ƒäº‹ä»¶
2. **æ¯”èµ›æ¦‚å†µ**ï¼šæå–æ¯”èµ›çš„å…³é”®ä¿¡æ¯ï¼ˆå¦‚æ¯”åˆ†ã€å‚èµ›é˜Ÿä¼/é€‰æ‰‹ã€å…³é”®è¡¨ç°ç­‰ï¼‰
3. **æŠ€æœ¯åˆ†æ**ï¼šç®€è¦åˆ†ææ¯”èµ›ä¸­çš„æŠ€æœ¯äº®ç‚¹æˆ–æˆ˜æœ¯å®‰æ’
4. **å†å²èƒŒæ™¯**ï¼šè¡¥å……å¿…è¦çš„å†å²èƒŒæ™¯ï¼ˆå¦‚çƒé˜Ÿ/é€‰æ‰‹å†å²æˆ˜ç»©ã€èµ›äº‹é‡è¦æ€§ç­‰ï¼‰
5. **äº‹ä»¶å½±å“**ï¼šåˆ†æè¯¥äº‹ä»¶å¯¹ç›¸å…³çƒé˜Ÿã€é€‰æ‰‹æˆ–ä½“è‚²é¡¹ç›®çš„å½±å“
6. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜è¿™æ¡æ–°é—»å¯¹å“ªäº›äººç¾¤æœ€æœ‰ä»·å€¼

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€å……æ»¡æ´»åŠ›ï¼Œç¬¦åˆä½“è‚²æ–°é—»çš„ç‰¹ç‚¹
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚""",

            "æ•™è‚²èµ„è®¯": f"""è¯·å¯¹ä»¥ä¸‹æ•™è‚²èµ„è®¯è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**æ•™è‚²å†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **æ ¸å¿ƒä¸»é¢˜**ï¼šç”¨50-100å­—æ¦‚æ‹¬æ•™è‚²èµ„è®¯çš„æ ¸å¿ƒä¸»é¢˜å’Œç›®çš„
2. **å…³é”®ä¿¡æ¯**ï¼šæå–3-5ä¸ªæœ€é‡è¦çš„äº‹å®è¦ç‚¹æˆ–æ”¿ç­–å†…å®¹
3. **é€‚ç”¨èŒƒå›´**ï¼šæ˜ç¡®è¯¥èµ„è®¯é€‚ç”¨çš„äººç¾¤ã€åœ°åŒºæˆ–æ•™è‚²é˜¶æ®µ
4. **å®æ–½å½±å“**ï¼šåˆ†æè¯¥æ”¿ç­–æˆ–èµ„è®¯å¯èƒ½äº§ç”Ÿçš„æ•™è‚²å½±å“å’Œæ•ˆæœ
5. **åº”å¯¹å»ºè®®**ï¼šé’ˆå¯¹ç›¸å…³å—ä¼—æä¾›åˆç†çš„åº”å¯¹å»ºè®®æˆ–è¡ŒåŠ¨æŒ‡å—
6. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜è¿™æ¡èµ„è®¯å¯¹å“ªäº›äººç¾¤æœ€æœ‰ä»·å€¼

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´æ˜äº†ï¼Œç¬¦åˆæ•™è‚²èµ„è®¯çš„ç‰¹ç‚¹
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚""",

            # é»˜è®¤æ¨¡æ¿
            "é»˜è®¤": f"""è¯·å¯¹ä»¥ä¸‹ç½‘é¡µå†…å®¹è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**ç½‘é¡µå†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **æ ¸å¿ƒæ‘˜è¦**ï¼šç”¨50-100å­—æ¦‚æ‹¬ç½‘é¡µçš„æ ¸å¿ƒå†…å®¹å’Œä¸»æ—¨
2. **å…³é”®è¦ç‚¹**ï¼šæå–2-3ä¸ªæœ€é‡è¦çš„ä¿¡æ¯ç‚¹æˆ–è§‚ç‚¹
3. **å†…å®¹ç±»å‹**ï¼šåˆ¤æ–­ç½‘é¡µå±äºä»€ä¹ˆç±»å‹ï¼ˆæ–°é—»ã€æ•™ç¨‹ã€åšå®¢ã€äº§å“ä»‹ç»ç­‰ï¼‰
4. **ä»·å€¼è¯„ä¼°**ï¼šç®€è¦è¯„ä»·å†…å®¹çš„ä»·å€¼å’Œå®ç”¨æ€§
5. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜é€‚åˆå“ªäº›äººç¾¤é˜…è¯»

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚"""
        }

        # è¿”å›å¯¹åº”çš„æ¨¡æ¿ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤æ¨¡æ¿
        return templates.get(content_type, templates["é»˜è®¤"])

    def _check_llm_availability(self) -> bool:
        """æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨å’Œå¯ç”¨"""
        return hasattr(self.context, "llm_generate") and self.llm_enabled

    async def _get_llm_provider(self, event: AstrMessageEvent) -> str:
        """è·å–åˆé€‚çš„LLMæä¾›å•†"""
        # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„LLMæä¾›å•†
        if self.llm_provider:
            return self.llm_provider

        # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œåˆ™ä½¿ç”¨å½“å‰ä¼šè¯çš„æ¨¡å‹
        try:
            umo = event.unified_msg_origin
            return await self.context.get_current_chat_provider_id(umo=umo)
        except Exception as e:
            logger.error(f"è·å–å½“å‰ä¼šè¯çš„èŠå¤©æ¨¡å‹IDå¤±è´¥: {e}")
            return ""

    def _build_llm_prompt(self, content_data: dict, content_type: str) -> str:
        """æ„å»ºä¼˜åŒ–çš„LLMæç¤ºè¯"""
        title = content_data["title"]
        content = content_data["content"]
        url = content_data["url"]

        emoji_prefix = "æ¯ä¸ªè¦ç‚¹ç”¨emojiå›¾æ ‡æ ‡è®°" if self.enable_emoji else ""

        if self.custom_prompt:
            # ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯ï¼Œæ›¿æ¢å˜é‡
            return self.custom_prompt.format(
                title=title,
                url=url,
                content=content,
                max_length=self.max_summary_length,
                content_type=content_type
            )
        else:
            # æ ¹æ®å†…å®¹ç±»å‹è·å–ç›¸åº”çš„åˆ†ææ¨¡æ¿
            template = self._get_analysis_template(content_type, emoji_prefix, self.max_summary_length)
            # æ›¿æ¢æ¨¡æ¿ä¸­çš„å˜é‡
            return template.format(
                title=title,
                url=url,
                content=content
            )

    def _format_llm_result(self, content_data: dict, analysis_text: str, content_type: str) -> str:
        """æ ¼å¼åŒ–LLMè¿”å›çš„ç»“æœ"""
        title = content_data["title"]
        url = content_data["url"]

        # é™åˆ¶æ‘˜è¦é•¿åº¦ï¼Œé¿å…ç»“æœè¿‡é•¿
        if len(analysis_text) > self.max_summary_length:
            analysis_text = analysis_text[: self.max_summary_length] + "..."

        # æ·»åŠ æ ‡é¢˜å’Œæ ¼å¼ç¾åŒ–
        link_emoji = "ğŸ”—" if self.enable_emoji else ""
        title_emoji = "ğŸ“" if self.enable_emoji else ""
        type_emoji = "ğŸ“‹" if self.enable_emoji else ""

        formatted_result = "**AIæ™ºèƒ½ç½‘é¡µåˆ†ææŠ¥å‘Š**\n\n"
        formatted_result += f"{link_emoji} **åˆ†æé“¾æ¥**: {url}\n"
        formatted_result += f"{title_emoji} **ç½‘é¡µæ ‡é¢˜**: {title}\n"
        formatted_result += f"{type_emoji} **å†…å®¹ç±»å‹**: {content_type}\n\n"
        formatted_result += "---\n\n"
        formatted_result += analysis_text
        formatted_result += "\n\n---\n"
        formatted_result += "*åˆ†æå®Œæˆï¼Œå¸Œæœ›å¯¹æ‚¨æœ‰å¸®åŠ©ï¼*"

        return formatted_result

    async def analyze_with_llm(
        self, event: AstrMessageEvent, content_data: dict
    ) -> str:
        """è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)è¿›è¡Œæ™ºèƒ½å†…å®¹åˆ†æå’Œæ€»ç»“"""
        try:
            title = content_data["title"]
            content = content_data["content"]
            url = content_data["url"]

            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨å’Œå¯ç”¨
            if not self._check_llm_availability():
                # LLMä¸å¯ç”¨æˆ–æœªå¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
                return self.get_enhanced_analysis(content_data)

            # è·å–LLMæä¾›å•†
            provider_id = await self._get_llm_provider(event)
            if not provider_id:
                # æ— æ³•è·å–LLMæä¾›å•†ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
                return self.get_enhanced_analysis(content_data)

            # æ™ºèƒ½æ£€æµ‹å†…å®¹ç±»å‹
            content_type = self._detect_content_type(content)

            # æ„å»ºLLMæç¤ºè¯
            prompt = self._build_llm_prompt(content_data, content_type)

            # è°ƒç”¨LLMç”Ÿæˆç»“æœ
            llm_resp = await self.context.llm_generate(
                chat_provider_id=provider_id,
                prompt=prompt,
            )

            if llm_resp and llm_resp.completion_text:
                # æ ¼å¼åŒ–LLMç»“æœ
                return self._format_llm_result(
                    content_data,
                    llm_resp.completion_text.strip(),
                    content_type
                )
            else:
                # LLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
                return self.get_enhanced_analysis(content_data)

        except Exception as e:
            # ä½¿ç”¨ç»Ÿä¸€é”™è¯¯å¤„ç†
            return self._handle_error(ErrorType.LLM_ERROR, e, url)

    def get_enhanced_analysis(self, content_data: dict) -> str:
        """å¢å¼ºç‰ˆåŸºç¡€åˆ†æ - LLMä¸å¯ç”¨æ—¶çš„æ™ºèƒ½å›é€€æ–¹æ¡ˆ"""
        title = content_data["title"]
        content = content_data["content"]
        url = content_data["url"]

        # è®¡ç®—å†…å®¹ç»Ÿè®¡ä¿¡æ¯
        content_stats = self._calculate_content_statistics(content)

        # æ™ºèƒ½æ£€æµ‹å†…å®¹ç±»å‹
        content_type = self._detect_content_type(content)

        # æå–å…³é”®å¥å­ä½œä¸ºå†…å®¹æ‘˜è¦
        paragraphs = [p.strip() for p in content.split("\n") if p.strip()]
        key_sentences = self._extract_key_sentences(paragraphs)

        # è¯„ä¼°å†…å®¹è´¨é‡
        quality_indicator = self._evaluate_content_quality(content_stats["char_count"])

        # æ„å»ºåˆ†æç»“æœ
        return self._build_analysis_result(
            title, url, content_type, quality_indicator, content_stats, paragraphs, key_sentences
        )

    def _calculate_content_statistics(self, content: str) -> dict:
        """è®¡ç®—å†…å®¹ç»Ÿè®¡ä¿¡æ¯"""
        char_count = len(content)
        word_count = len(content.split())
        return {
            "char_count": char_count,
            "word_count": word_count
        }

    def _detect_content_type(self, content: str) -> str:
        """æ™ºèƒ½æ£€æµ‹å†…å®¹ç±»å‹"""
        content_lower = content.lower()

        content_type_rules = [
            ("æ–°é—»èµ„è®¯", ["æ–°é—»", "æŠ¥é“", "æ¶ˆæ¯", "æ—¶äº‹", "å¿«è®¯", "å¤´æ¡", "è¦é—»", "çƒ­ç‚¹", "äº‹ä»¶"]),
            ("æ•™ç¨‹æŒ‡å—", ["æ•™ç¨‹", "æŒ‡å—", "æ•™å­¦", "æ­¥éª¤", "æ–¹æ³•", "å¦‚ä½•", "æ€æ ·", "æ•™ç¨‹", "æŒ‡å—", "æ”»ç•¥", "æŠ€å·§"]),
            ("ä¸ªäººåšå®¢", ["åšå®¢", "éšç¬”", "æ—¥è®°", "ä¸ªäºº", "è§‚ç‚¹", "æ„Ÿæƒ³", "æ„Ÿæ‚Ÿ", "æ€è€ƒ", "åˆ†äº«"]),
            ("äº§å“ä»‹ç»", ["äº§å“", "æœåŠ¡", "è´­ä¹°", "ä»·æ ¼", "ä¼˜æƒ ", "åŠŸèƒ½", "ç‰¹æ€§", "å‚æ•°", "è§„æ ¼", "è¯„æµ‹"]),
            ("æŠ€æœ¯æ–‡æ¡£", ["æŠ€æœ¯", "å¼€å‘", "ç¼–ç¨‹", "ä»£ç ", "API", "SDK", "æ–‡æ¡£", "æ•™ç¨‹", "æŒ‡å—", "è¯´æ˜"]),
            ("å­¦æœ¯è®ºæ–‡", ["è®ºæ–‡", "ç ”ç©¶", "å®éªŒ", "ç»“è®º", "æ‘˜è¦", "å…³é”®è¯", "å¼•ç”¨", "å‚è€ƒæ–‡çŒ®"]),
            ("å•†ä¸šåˆ†æ", ["åˆ†æ", "æŠ¥å‘Š", "æ•°æ®", "ç»Ÿè®¡", "è¶‹åŠ¿", "é¢„æµ‹", "å¸‚åœº", "è¡Œä¸š"]),
            ("å¨±ä¹èµ„è®¯", ["å¨±ä¹", "æ˜æ˜Ÿ", "ç”µå½±", "éŸ³ä¹", "ç»¼è‰º", "æ¼”å”±ä¼š", "é¦–æ˜ ", "æ–°æ­Œ"]),
            ("ä½“è‚²æ–°é—»", ["ä½“è‚²", "æ¯”èµ›", "èµ›äº‹", "æ¯”åˆ†", "è¿åŠ¨å‘˜", "å† å†›", "äºšå†›", "å­£å†›"]),
            ("æ•™è‚²èµ„è®¯", ["æ•™è‚²", "å­¦æ ¡", "æ‹›ç”Ÿ", "è€ƒè¯•", "åŸ¹è®­", "å­¦ä¹ ", "è¯¾ç¨‹", "æ•™æ"])
        ]

        for type_name, keywords in content_type_rules:
            if any(keyword in content_lower for keyword in keywords):
                return type_name

        return "æ–‡ç« "

    def _extract_key_sentences(self, paragraphs: list) -> list:
        """æå–å…³é”®å¥å­ä½œä¸ºå†…å®¹æ‘˜è¦"""
        # æå–å‰3ä¸ªæ®µè½ä½œä¸ºå…³é”®å¥å­
        return paragraphs[:3]

    def _evaluate_content_quality(self, char_count: int) -> str:
        """è¯„ä¼°å†…å®¹è´¨é‡"""
        if char_count > 5000:
            return "å†…å®¹è¯¦å®"
        elif char_count > 1000:
            return "å†…å®¹ä¸°å¯Œ"
        else:
            return "å†…å®¹ç®€æ´"

    def _build_analysis_header(self) -> str:
        """æ„å»ºåˆ†æç»“æœçš„æ ‡é¢˜éƒ¨åˆ†"""
        robot_emoji = "ğŸ¤–" if self.enable_emoji else ""
        page_emoji = "ğŸ“„" if self.enable_emoji else ""
        return f"{robot_emoji} **æ™ºèƒ½ç½‘é¡µåˆ†æ** {page_emoji}\n\n"

    def _build_basic_info(self, title: str, url: str, content_type: str,
                         quality_indicator: str) -> str:
        """æ„å»ºåˆ†æç»“æœçš„åŸºæœ¬ä¿¡æ¯éƒ¨åˆ†
        
        Args:
            title: ç½‘é¡µæ ‡é¢˜
            url: ç½‘é¡µURL
            content_type: å†…å®¹ç±»å‹
            quality_indicator: è´¨é‡è¯„ä¼°
            
        Returns:
            æ ¼å¼åŒ–çš„åŸºæœ¬ä¿¡æ¯å­—ç¬¦ä¸²
        """
        info_emoji = "ğŸ“" if self.enable_emoji else ""

        basic_info = []
        if self.enable_emoji:
            basic_info.append(f"**{info_emoji} åŸºæœ¬ä¿¡æ¯**\n")
        else:
            basic_info.append("**åŸºæœ¬ä¿¡æ¯**\n")

        basic_info.append(f"- **æ ‡é¢˜**: {title}\n")
        basic_info.append(f"- **é“¾æ¥**: {url}\n")
        basic_info.append(f"- **å†…å®¹ç±»å‹**: {content_type}\n")
        basic_info.append(f"- **è´¨é‡è¯„ä¼°**: {quality_indicator}\n\n")

        return "".join(basic_info)

    def _build_statistics_info(self, content_stats: dict, paragraphs: list) -> str:
        """æ„å»ºåˆ†æç»“æœçš„ç»Ÿè®¡ä¿¡æ¯éƒ¨åˆ†"""
        if not self.enable_statistics:
            return ""

        stats_emoji = "ğŸ“Š" if self.enable_emoji else ""

        stats_info = []
        if self.enable_emoji:
            stats_info.append(f"**{stats_emoji} å†…å®¹ç»Ÿè®¡**\n")
        else:
            stats_info.append("**å†…å®¹ç»Ÿè®¡**\n")

        stats_info.append(f"- å­—ç¬¦æ•°: {content_stats['char_count']:,}\n")
        stats_info.append(f"- æ®µè½æ•°: {len(paragraphs)}\n")
        stats_info.append(f"- è¯æ•°: {content_stats['word_count']:,}\n\n")

        return "".join(stats_info)

    def _build_content_summary(self, key_sentences: list) -> str:
        """æ„å»ºåˆ†æç»“æœçš„å†…å®¹æ‘˜è¦éƒ¨åˆ†"""
        search_emoji = "ğŸ”" if self.enable_emoji else ""

        summary_info = []
        if self.enable_emoji:
            summary_info.append(f"**{search_emoji} å†…å®¹æ‘˜è¦**\n")
        else:
            summary_info.append("**å†…å®¹æ‘˜è¦**\n")

        # æ ¼å¼åŒ–å…³é”®å¥å­
        formatted_sentences = []
        for sentence in key_sentences:
            truncated = sentence[:100] + ("..." if len(sentence) > 100 else "")
            formatted_sentences.append(f"â€¢ {truncated}")

        summary_info.append(f"{chr(10).join(formatted_sentences)}\n\n")
        return "".join(summary_info)

    def _build_analysis_note(self) -> str:
        """æ„å»ºåˆ†æç»“æœçš„åˆ†æè¯´æ˜éƒ¨åˆ†"""
        light_emoji = "ğŸ’¡" if self.enable_emoji else ""

        note_info = []
        if self.enable_emoji:
            note_info.append(f"**{light_emoji} åˆ†æè¯´æ˜**\n")
        else:
            note_info.append("**åˆ†æè¯´æ˜**\n")

        note_info.append("æ­¤åˆ†æåŸºäºç½‘é¡µå†…å®¹æå–ï¼Œå¦‚éœ€æ›´æ·±å…¥çš„AIæ™ºèƒ½åˆ†æï¼Œè¯·ç¡®ä¿AstrBotå·²æ­£ç¡®é…ç½®LLMåŠŸèƒ½ã€‚\n\n")
        note_info.append("*æç¤ºï¼šå®Œæ•´å†…å®¹é¢„è§ˆè¯·æŸ¥çœ‹åŸå§‹ç½‘é¡µ*")

        return "".join(note_info)

    def _build_analysis_result(self, title: str, url: str, content_type: str,
                              quality_indicator: str, content_stats: dict,
                              paragraphs: list, key_sentences: list) -> str:
        """æ„å»ºæœ€ç»ˆçš„åˆ†æç»“æœ"""
        # æ„å»ºåˆ†æç»“æœ
        result_parts = []
        result_parts.append(self._build_analysis_header())
        result_parts.append(self._build_basic_info(title, url, content_type, quality_indicator))
        result_parts.append(self._build_statistics_info(content_stats, paragraphs))
        result_parts.append(self._build_content_summary(key_sentences))
        result_parts.append(self._build_analysis_note())

        return "".join(result_parts)



    def _handle_error(self, error_type: str, original_error: Exception, url: str | None = None, context: dict | None = None) -> str:
        """ç»Ÿä¸€é”™è¯¯å¤„ç†æ–¹æ³•"""
        # è·å–é”™è¯¯é…ç½®
        error_config = ERROR_MESSAGES.get(error_type, ERROR_MESSAGES["unknown_error"])
        error_message = error_config["message"]
        solution = error_config["solution"]
        severity = error_config["severity"]

        # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = []
        if url:
            context_info.append(f"URL: {url}")
        if context:
            for key, value in context.items():
                context_info.append(f"{key}: {value}")
        context_str = " | ".join(context_info)

        # æ„å»ºæ—¥å¿—ä¿¡æ¯
        log_message = f"{error_message}: {str(original_error)}"
        if context_str:
            log_message += f" ({context_str})"

        # æ ¹æ®ä¸¥é‡ç¨‹åº¦è®°å½•æ—¥å¿—ï¼Œæ‰€æœ‰é”™è¯¯çº§åˆ«éƒ½åŒ…å«å †æ ˆè·Ÿè¸ª
        if severity == ErrorSeverity.INFO:
            logger.info(log_message, exc_info=True)
        elif severity == ErrorSeverity.WARNING:
            logger.warning(log_message, exc_info=True)
        elif severity == ErrorSeverity.ERROR:
            logger.error(log_message, exc_info=True)
        elif severity == ErrorSeverity.CRITICAL:
            logger.critical(log_message, exc_info=True)

        # ç”Ÿæˆç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯
        user_message = []
        user_message.append(f"âŒ {error_message}")

        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        if url:
            user_message.append(f"ğŸ”— ç›¸å…³é“¾æ¥: {url}")

        # æ·»åŠ é”™è¯¯è¯¦æƒ…ï¼Œä»…æ˜¾ç¤ºå…³é”®ä¿¡æ¯ï¼Œé¿å…è¿‡é•¿
        error_detail = str(original_error)
        if len(error_detail) > 100:
            error_detail = error_detail[:100] + "..."
        user_message.append(f"ğŸ“‹ é”™è¯¯è¯¦æƒ…: {error_detail}")

        # æ·»åŠ å»ºè®®è§£å†³æ–¹æ¡ˆ
        user_message.append(f"ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ: {solution}")

        # æ·»åŠ é”™è¯¯ç±»å‹å’Œä¸¥é‡ç¨‹åº¦ï¼ˆä»…åœ¨è°ƒè¯•æ¨¡å¼æˆ–ä¸¥é‡é”™è¯¯æ—¶æ˜¾ç¤ºï¼‰
        if severity in [ErrorSeverity.ERROR, ErrorSeverity.CRITICAL]:
            user_message.append(f"âš ï¸  é”™è¯¯ç±»å‹: {error_type}")
            user_message.append(f"ğŸ”´ ä¸¥é‡ç¨‹åº¦: {severity.upper()}")

        return "\n".join(user_message)

    def _get_error_type(self, exception: Exception) -> str:
        """æ ¹æ®å¼‚å¸¸ç±»å‹è·å–å¯¹åº”çš„é”™è¯¯ç±»å‹"""

        from httpx import ConnectError, HTTPError, TimeoutException

        exception_type = type(exception).__name__
        exception_msg = str(exception).lower()

        # ç½‘ç»œç›¸å…³é”™è¯¯
        if isinstance(exception, HTTPError):
            if isinstance(exception, TimeoutException):
                return ErrorType.NETWORK_TIMEOUT
            elif isinstance(exception, ConnectError):
                return ErrorType.NETWORK_CONNECTION
            else:
                return ErrorType.NETWORK_ERROR
        elif "timeout" in exception_type.lower() or "timeout" in exception_msg:
            return ErrorType.NETWORK_TIMEOUT
        elif "connect" in exception_type.lower() or "connection" in exception_msg:
            return ErrorType.NETWORK_CONNECTION
        elif "network" in exception_type.lower() or "http" in exception_type.lower():
            return ErrorType.NETWORK_ERROR

        # è§£æç›¸å…³é”™è¯¯
        elif "parse" in exception_type.lower() or "soup" in exception_type.lower() or "lxml" in exception_type.lower():
            return ErrorType.HTML_PARSING
        elif "empty" in exception_msg or "none" in exception_msg or "null" in exception_msg:
            return ErrorType.CONTENT_EMPTY
        elif "parse" in exception_msg:
            return ErrorType.PARSING_ERROR

        # LLMç›¸å…³é”™è¯¯
        elif "llm" in exception_type.lower() or "llm" in exception_msg:
            return ErrorType.LLM_ERROR
        elif "generate" in exception_type.lower() or "generate" in exception_msg:
            return ErrorType.LLM_ERROR
        elif "timeout" in exception_msg and "llm" in exception_msg:
            return ErrorType.LLM_TIMEOUT
        elif "invalid" in exception_msg or "format" in exception_msg:
            return ErrorType.LLM_INVALID_RESPONSE
        elif "permission" in exception_msg or "auth" in exception_msg or "key" in exception_msg:
            return ErrorType.LLM_PERMISSION

        # æˆªå›¾ç›¸å…³é”™è¯¯
        elif "screenshot" in exception_type.lower() or "screenshot" in exception_msg:
            return ErrorType.SCREENSHOT_ERROR
        elif "browser" in exception_type.lower() or "playwright" in exception_type.lower():
            return ErrorType.BROWSER_ERROR

        # ç¼“å­˜ç›¸å…³é”™è¯¯
        elif "cache" in exception_type.lower() or "cache" in exception_msg:
            return ErrorType.CACHE_ERROR
        elif "write" in exception_msg or "save" in exception_msg:
            return ErrorType.CACHE_WRITE
        elif "read" in exception_msg or "load" in exception_msg:
            return ErrorType.CACHE_READ

        # é…ç½®ç›¸å…³é”™è¯¯
        elif "config" in exception_type.lower() or "setting" in exception_type.lower():
            return ErrorType.CONFIG_ERROR
        elif "invalid" in exception_msg:
            return ErrorType.CONFIG_INVALID

        # æƒé™ç›¸å…³é”™è¯¯
        elif "permission" in exception_type.lower() or "auth" in exception_type.lower():
            return ErrorType.PERMISSION_ERROR
        elif "blocked" in exception_msg or "deny" in exception_msg:
            return ErrorType.DOMAIN_BLOCKED

        # å…¶ä»–é”™è¯¯
        elif "internal" in exception_type.lower() or "internal" in exception_msg:
            return ErrorType.INTERNAL_ERROR
        else:
            return ErrorType.UNKNOWN_ERROR

    async def _auto_recall_message(self, bot, message_id: int, recall_time: int) -> None:
        """è‡ªåŠ¨æ’¤å›æ¶ˆæ¯"""
        try:
            import asyncio

            # ç­‰å¾…æŒ‡å®šæ—¶é—´
            if recall_time > 0:
                await asyncio.sleep(recall_time)

            # è°ƒç”¨botçš„delete_msgæ–¹æ³•æ’¤å›æ¶ˆæ¯
            await bot.delete_msg(message_id=message_id)
            logger.debug(f"å·²æ’¤å›æ¶ˆæ¯: {message_id}")
        except Exception as e:
            logger.error(f"æ’¤å›æ¶ˆæ¯å¤±è´¥: {e}")

    async def _send_processing_message(self, event: AstrMessageEvent, message: str) -> tuple:
        """å‘é€æ­£åœ¨åˆ†æçš„æ¶ˆæ¯å¹¶è®¾ç½®è‡ªåŠ¨æ’¤å›"""
        import asyncio

        # è·å–botå®ä¾‹
        bot = event.bot
        message_id = None

        # ç›´æ¥è°ƒç”¨botçš„å‘é€æ¶ˆæ¯æ–¹æ³•ï¼Œè·å–æ¶ˆæ¯ID
        try:
            # æ ¹æ®äº‹ä»¶ç±»å‹é€‰æ‹©å‘é€æ–¹æ³•
            send_result = None
            group_id = None
            user_id = None

            # æ–¹æ³•1ï¼šä½¿ç”¨AiocqhttpMessageEventçš„æ–¹æ³•è·å–
            if hasattr(event, "get_group_id"):
                group_id = event.get_group_id()
            if hasattr(event, "get_sender_id"):
                user_id = event.get_sender_id()

            # æ–¹æ³•2ï¼šåˆ¤æ–­æ˜¯å¦ä¸ºç§èŠ
            is_private = False
            if hasattr(event, "is_private_chat"):
                is_private = event.is_private_chat()

            # å‘é€æ¶ˆæ¯
            if group_id:
                # ç¾¤èŠæ¶ˆæ¯
                send_result = await bot.send_group_msg(
                    group_id=group_id,
                    message=message
                )
                logger.debug(f"å‘é€ç¾¤èŠå¤„ç†æ¶ˆæ¯: {message} åˆ°ç¾¤ {group_id}")
            elif user_id or is_private:
                # ç§èŠæ¶ˆæ¯
                if not user_id and hasattr(event, "get_sender_id"):
                    user_id = event.get_sender_id()

                if user_id:
                    send_result = await bot.send_private_msg(
                        user_id=user_id,
                        message=message
                    )
                    logger.debug(f"å‘é€ç§èŠå¤„ç†æ¶ˆæ¯: {message} åˆ°ç”¨æˆ· {user_id}")
                else:
                    # æ— æ³•è·å–user_idï¼Œä½¿ç”¨åŸå§‹æ–¹å¼å‘é€
                    logger.warning(f"æ— æ³•è·å–user_idï¼Œä½¿ç”¨åŸå§‹æ–¹å¼å‘é€æ¶ˆæ¯: {message}")
                    response = event.plain_result(message)
                    if hasattr(event, "send"):
                        await event.send(response)
                    return None, bot
            else:
                # æ— æ³•ç¡®å®šæ¶ˆæ¯ç±»å‹ï¼Œä½¿ç”¨åŸå§‹æ–¹å¼å‘é€å¹¶è®°å½•è¯¦ç»†ä¿¡æ¯
                logger.error(f"æ— æ³•ç¡®å®šæ¶ˆæ¯ç±»å‹ï¼Œeventç±»å‹: {type(event)}, eventæ–¹æ³•: get_group_id={hasattr(event, 'get_group_id')}, get_sender_id={hasattr(event, 'get_sender_id')}, is_private_chat={hasattr(event, 'is_private_chat')}")
                # å°è¯•ä½¿ç”¨event.plain_resultå‘é€ï¼Œè™½ç„¶æ— æ³•è·å–message_id
                response = event.plain_result(message)
                # ä½¿ç”¨eventçš„sendæ–¹æ³•å‘é€
                if hasattr(event, "send"):
                    await event.send(response)
                return None, bot

            # æ£€æŸ¥send_resultæ˜¯å¦åŒ…å«message_id
            if isinstance(send_result, dict):
                message_id = send_result.get("message_id")
            elif hasattr(send_result, "message_id"):
                message_id = send_result.message_id

            logger.debug(f"å‘é€å¤„ç†æ¶ˆæ¯æˆåŠŸï¼Œmessage_id: {message_id}")

            # å¦‚æœè·å–åˆ°message_idä¸”å¯ç”¨äº†è‡ªåŠ¨æ’¤å›
            if message_id and self.enable_recall:
                # å®šæ—¶æ’¤å›æ¨¡å¼
                if self.recall_type == "time_based":
                    logger.info(f"åˆ›å»ºå®šæ—¶æ’¤å›ä»»åŠ¡ï¼Œmessage_id: {message_id}ï¼Œå»¶è¿Ÿ: {self.recall_time}ç§’")

                    async def _recall_task():
                        try:
                            await asyncio.sleep(self.recall_time)
                            await bot.delete_msg(message_id=message_id)
                            logger.info(f"å·²å®šæ—¶æ’¤å›æ¶ˆæ¯: {message_id}")
                        except Exception as e:
                            logger.error(f"å®šæ—¶æ’¤å›æ¶ˆæ¯å¤±è´¥: {e}")

                    task = asyncio.create_task(_recall_task())

                    # å°†ä»»åŠ¡æ·»åŠ åˆ°åˆ—è¡¨ä¸­ç®¡ç†
                    self.recall_tasks.append(task)

                    # æ·»åŠ å®Œæˆå›è°ƒï¼Œä»åˆ—è¡¨ä¸­ç§»é™¤å·²å®Œæˆçš„ä»»åŠ¡
                    def _remove_task(t):
                        try:
                            self.recall_tasks.remove(t)
                        except ValueError:
                            pass

                    task.add_done_callback(_remove_task)
                # æ™ºèƒ½æ’¤å›æ¨¡å¼ - åªå‘é€æ¶ˆæ¯ï¼Œä¸åˆ›å»ºå®šæ—¶ä»»åŠ¡ï¼Œç­‰å¾…åˆ†æå®Œæˆåç«‹å³æ’¤å›
                elif self.recall_type == "smart" and self.smart_recall_enabled:
                    logger.info(f"å·²å‘é€æ™ºèƒ½æ’¤å›æ¶ˆæ¯ï¼Œmessage_id: {message_id}ï¼Œç­‰å¾…åˆ†æå®Œæˆåç«‹å³æ’¤å›")

        except Exception as e:
            logger.error(f"å‘é€å¤„ç†æ¶ˆæ¯æˆ–è®¾ç½®æ’¤å›å¤±è´¥: {e}")

        return message_id, bot

    @filter.command("web_config", alias={"ç½‘é¡µåˆ†æé…ç½®", "ç½‘é¡µåˆ†æè®¾ç½®"})
    async def show_config(self, event: AstrMessageEvent):
        """æ˜¾ç¤ºå½“å‰æ’ä»¶çš„è¯¦ç»†é…ç½®ä¿¡æ¯"""
        config_info = f"""**ç½‘é¡µåˆ†ææ’ä»¶é…ç½®ä¿¡æ¯**

**åŸºæœ¬è®¾ç½®**
- æœ€å¤§å†…å®¹é•¿åº¦: {self.max_content_length} å­—ç¬¦
- è¯·æ±‚è¶…æ—¶æ—¶é—´: {self.timeout} ç§’
- LLMæ™ºèƒ½åˆ†æ: {"âœ… å·²å¯ç”¨" if self.llm_enabled else "âŒ å·²ç¦ç”¨"}
- åˆ†ææ¨¡å¼: {self.analysis_mode}
- è‡ªåŠ¨åˆ†æé“¾æ¥: {"âœ… å·²å¯ç”¨" if self.auto_analyze else "âŒ å·²ç¦ç”¨"}
- åˆå¹¶è½¬å‘åŠŸèƒ½(ç¾¤èŠ): {"âœ… å·²å¯ç”¨" if self.merge_forward_enabled["group"] else "âŒ å·²ç¦ç”¨"}
- åˆå¹¶è½¬å‘åŠŸèƒ½(ç§èŠ): {"âœ… å·²å¯ç”¨" if self.merge_forward_enabled["private"] else "âŒ å·²ç¦ç”¨"}
- åˆå¹¶è½¬å‘åŒ…å«æˆªå›¾: {"âœ… å·²å¯ç”¨" if self.merge_forward_enabled["include_screenshot"] else "âŒ å·²ç¦ç”¨"}

**å¹¶å‘å¤„ç†è®¾ç½®**
- æœ€å¤§å¹¶å‘æ•°: {self.max_concurrency}
- åŠ¨æ€å¹¶å‘æ§åˆ¶: {"âœ… å·²å¯ç”¨" if self.dynamic_concurrency else "âŒ å·²ç¦ç”¨"}
- ä¼˜å…ˆçº§è°ƒåº¦: {"âœ… å·²å¯ç”¨" if self.enable_priority_scheduling else "âŒ å·²ç¦ç”¨"}

**åŸŸåæ§åˆ¶**
- å…è®¸åŸŸå: {len(self.allowed_domains)} ä¸ª
- ç¦æ­¢åŸŸå: {len(self.blocked_domains)} ä¸ª

**ç¾¤èŠæ§åˆ¶**
- ç¾¤èŠé»‘åå•: {len(self.group_blacklist)} ä¸ªç¾¤èŠ

**åˆ†æè®¾ç½®**
- å¯ç”¨emoji: {"âœ… å·²å¯ç”¨" if self.enable_emoji else "âŒ å·²ç¦ç”¨"}
- æ˜¾ç¤ºç»Ÿè®¡: {"âœ… å·²å¯ç”¨" if self.enable_statistics else "âŒ å·²ç¦ç”¨"}
- æœ€å¤§æ‘˜è¦é•¿åº¦: {self.max_summary_length} å­—ç¬¦
- å‘é€å†…å®¹ç±»å‹: {self.send_content_type}
- å¯ç”¨æˆªå›¾: {"âœ… å·²å¯ç”¨" if self.enable_screenshot else "âŒ å·²ç¦ç”¨"}
- æˆªå›¾è´¨é‡: {self.screenshot_quality}
- æˆªå›¾å®½åº¦: {self.screenshot_width}px
- æˆªå›¾é«˜åº¦: {self.screenshot_height}px
- æˆªå›¾æ ¼å¼: {self.screenshot_format}
- æˆªå–æ•´é¡µ: {"âœ… å·²å¯ç”¨" if self.screenshot_full_page else "âŒ å·²ç¦ç”¨"}
- æˆªå›¾ç­‰å¾…æ—¶é—´: {self.screenshot_wait_time}ms
- å¯ç”¨æˆªå›¾è£å‰ª: {"âœ… å·²å¯ç”¨" if self.enable_crop else "âŒ å·²ç¦ç”¨"}
- è£å‰ªåŒºåŸŸ: {self.crop_area}
- å¯ç”¨æ°´å°: {"âœ… å·²å¯ç”¨" if self.enable_watermark else "âŒ å·²ç¦ç”¨"}
- æ°´å°æ–‡æœ¬: {self.watermark_text}
- æ°´å°å­—ä½“å¤§å°: {self.watermark_font_size}
- æ°´å°é€æ˜åº¦: {self.watermark_opacity}%
- æ°´å°ä½ç½®: {self.watermark_position}
- å¯ç”¨LLMè‡ªä¸»å†³ç­–: {"âœ… å·²å¯ç”¨" if self.enable_llm_decision else "âŒ å·²ç¦ç”¨"}

**LLMé…ç½®**
- æŒ‡å®šæä¾›å•†: {self.llm_provider if self.llm_provider else "ä½¿ç”¨ä¼šè¯é»˜è®¤"}
- è‡ªå®šä¹‰æç¤ºè¯: {"âœ… å·²å¯ç”¨" if self.custom_prompt else "âŒ æœªè®¾ç½®"}

**ç¿»è¯‘è®¾ç½®**
- å¯ç”¨ç½‘é¡µç¿»è¯‘: {"âœ… å·²å¯ç”¨" if self.enable_translation else "âŒ å·²ç¦ç”¨"}
- ç›®æ ‡è¯­è¨€: {self.target_language}
- ç¿»è¯‘æä¾›å•†: {self.translation_provider}
- è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯: {"âœ… å·²å¯ç”¨" if self.custom_translation_prompt else "âŒ æœªè®¾ç½®"}

**ç¼“å­˜è®¾ç½®**
- å¯ç”¨ç»“æœç¼“å­˜: {"âœ… å·²å¯ç”¨" if self.enable_cache else "âŒ å·²ç¦ç”¨"}
- ç¼“å­˜è¿‡æœŸæ—¶é—´: {self.cache_expire_time} åˆ†é’Ÿ
- æœ€å¤§ç¼“å­˜æ•°é‡: {self.max_cache_size} ä¸ª
- å¯ç”¨ç¼“å­˜é¢„åŠ è½½: {"âœ… å·²å¯ç”¨" if self.cache_preload_enabled else "âŒ å·²ç¦ç”¨"}
- é¢„åŠ è½½ç¼“å­˜æ•°é‡: {self.cache_preload_count} ä¸ª

**å†…å®¹æå–è®¾ç½®**
- å¯ç”¨ç‰¹å®šå†…å®¹æå–: {"âœ… å·²å¯ç”¨" if self.enable_specific_extraction else "âŒ å·²ç¦ç”¨"}
- æå–å†…å®¹ç±»å‹: {", ".join(self.extract_types)}

*æç¤º: å¦‚éœ€ä¿®æ”¹é…ç½®ï¼Œè¯·åœ¨AstrBotç®¡ç†é¢æ¿ä¸­ç¼–è¾‘æ’ä»¶é…ç½®*"""

        yield event.plain_result(config_info)

    @filter.command("test_merge", alias={"æµ‹è¯•åˆå¹¶è½¬å‘", "æµ‹è¯•è½¬å‘"})
    async def test_merge_forward(self, event: AstrMessageEvent):
        """æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½"""
        from astrbot.api.message_components import Node, Nodes, Plain

        # æ£€æŸ¥æ˜¯å¦ä¸ºç¾¤èŠæ¶ˆæ¯ï¼Œåˆå¹¶è½¬å‘ä»…æ”¯æŒç¾¤èŠ
        group_id = None
        if hasattr(event, "group_id") and event.group_id:
            group_id = event.group_id
        elif (
            hasattr(event, "message_obj")
            and hasattr(event.message_obj, "group_id")
            and event.message_obj.group_id
        ):
            group_id = event.message_obj.group_id

        if group_id:
            # åˆ›å»ºæµ‹è¯•ç”¨çš„åˆå¹¶è½¬å‘èŠ‚ç‚¹
            nodes = []

            # æ ‡é¢˜èŠ‚ç‚¹
            title_node = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•åˆå¹¶è½¬å‘",
                content=[Plain("è¿™æ˜¯åˆå¹¶è½¬å‘æµ‹è¯•æ¶ˆæ¯")],
            )
            nodes.append(title_node)

            # å†…å®¹èŠ‚ç‚¹1
            content_node1 = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•èŠ‚ç‚¹1",
                content=[Plain("è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•èŠ‚ç‚¹å†…å®¹")],
            )
            nodes.append(content_node1)

            # å†…å®¹èŠ‚ç‚¹2
            content_node2 = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•èŠ‚ç‚¹2",
                content=[Plain("è¿™æ˜¯ç¬¬äºŒä¸ªæµ‹è¯•èŠ‚ç‚¹å†…å®¹")],
            )
            nodes.append(content_node2)

            # ä½¿ç”¨NodesåŒ…è£…æ‰€æœ‰èŠ‚ç‚¹ï¼Œåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
            merge_forward_message = Nodes(nodes)
            yield event.chain_result([merge_forward_message])
            logger.info(f"æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½ï¼Œç¾¤èŠ {group_id}")
        else:
            yield event.plain_result("åˆå¹¶è½¬å‘åŠŸèƒ½ä»…æ”¯æŒç¾¤èŠæ¶ˆæ¯æµ‹è¯•")
            logger.info("ç§èŠæ¶ˆæ¯æ— æ³•æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½")

    @filter.command("group_blacklist", alias={"ç¾¤é»‘åå•", "é»‘åå•"})
    async def manage_group_blacklist(self, event: AstrMessageEvent):
        """ç®¡ç†ç¾¤èŠé»‘åå•"""
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()

        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰é»‘åå•åˆ—è¡¨
        if len(message_parts) <= 1:
            if not self.group_blacklist:
                yield event.plain_result("å½“å‰ç¾¤èŠé»‘åå•ä¸ºç©º")
                return

            blacklist_info = "**å½“å‰ç¾¤èŠé»‘åå•**\n\n"
            for i, group_id in enumerate(self.group_blacklist, 1):
                blacklist_info += f"{i}. {group_id}\n"

            blacklist_info += "\nä½¿ç”¨ `/group_blacklist add <ç¾¤å·>` æ·»åŠ ç¾¤èŠåˆ°é»‘åå•"
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist remove <ç¾¤å·>` ä»é»‘åå•ç§»é™¤ç¾¤èŠ"
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist clear` æ¸…ç©ºé»‘åå•"

            yield event.plain_result(blacklist_info)
            return

        # è§£ææ“ä½œç±»å‹å’Œå‚æ•°
        action = message_parts[1].lower() if len(message_parts) > 1 else ""
        group_id = message_parts[2] if len(message_parts) > 2 else ""

        # æ·»åŠ ç¾¤èŠåˆ°é»‘åå•
        if action == "add" and group_id:
            if group_id in self.group_blacklist:
                yield event.plain_result(f"ç¾¤èŠ {group_id} å·²åœ¨é»‘åå•ä¸­")
                return

            self.group_blacklist.append(group_id)
            self._save_group_blacklist()
            yield event.plain_result(f"âœ… å·²æ·»åŠ ç¾¤èŠ {group_id} åˆ°é»‘åå•")

        # ä»é»‘åå•ç§»é™¤ç¾¤èŠ
        elif action == "remove" and group_id:
            if group_id not in self.group_blacklist:
                yield event.plain_result(f"ç¾¤èŠ {group_id} ä¸åœ¨é»‘åå•ä¸­")
                return

            self.group_blacklist.remove(group_id)
            self._save_group_blacklist()
            yield event.plain_result(f"âœ… å·²ä»é»‘åå•ç§»é™¤ç¾¤èŠ {group_id}")

        # æ¸…ç©ºé»‘åå•
        elif action == "clear":
            if not self.group_blacklist:
                yield event.plain_result("é»‘åå•å·²ä¸ºç©º")
                return

            self.group_blacklist.clear()
            self._save_group_blacklist()
            yield event.plain_result("âœ… å·²æ¸…ç©ºç¾¤èŠé»‘åå•")

        # æ— æ•ˆæ“ä½œ
        else:
            yield event.plain_result(
                "æ— æ•ˆçš„æ“ä½œï¼Œè¯·ä½¿ç”¨: add <ç¾¤å·>, remove <ç¾¤å·>, clear"
            )

    @filter.permission_type(filter.PermissionType.ADMIN)
    @filter.command("web_cache", alias={"ç½‘é¡µç¼“å­˜", "æ¸…ç†ç¼“å­˜"})
    async def manage_cache(self, event: AstrMessageEvent):
        """ç®¡ç†æ’ä»¶çš„ç½‘é¡µåˆ†æç»“æœç¼“å­˜"""
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()

        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰ç¼“å­˜çŠ¶æ€
        if len(message_parts) <= 1:
            cache_stats = self.cache_manager.get_stats()
            cache_info = "**å½“å‰ç¼“å­˜çŠ¶æ€**\n\n"
            cache_info += f"- ç¼“å­˜æ€»æ•°: {cache_stats['total']} ä¸ª\n"
            cache_info += f"- æœ‰æ•ˆç¼“å­˜: {cache_stats['valid']} ä¸ª\n"
            cache_info += f"- è¿‡æœŸç¼“å­˜: {cache_stats['expired']} ä¸ª\n"
            cache_info += f"- ç¼“å­˜è¿‡æœŸæ—¶é—´: {self.cache_expire_time} åˆ†é’Ÿ\n"
            cache_info += f"- æœ€å¤§ç¼“å­˜æ•°é‡: {self.max_cache_size} ä¸ª\n"
            cache_info += (
                f"- ç¼“å­˜åŠŸèƒ½: {'âœ… å·²å¯ç”¨' if self.enable_cache else 'âŒ å·²ç¦ç”¨'}\n"
            )

            cache_info += "\nä½¿ç”¨ `/web_cache clear` æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"

            yield event.plain_result(cache_info)
            return

        # è§£ææ“ä½œç±»å‹
        action = message_parts[1].lower() if len(message_parts) > 1 else ""

        # æ¸…ç©ºç¼“å­˜æ“ä½œ
        if action == "clear":
            # æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
            self.cache_manager.clear()
            cache_stats = self.cache_manager.get_stats()
            yield event.plain_result(
                f"âœ… å·²æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ï¼Œå½“å‰ç¼“å­˜æ•°é‡: {cache_stats['total']} ä¸ª"
            )

        # æ— æ•ˆæ“ä½œ
        else:
            yield event.plain_result("æ— æ•ˆçš„æ“ä½œï¼Œè¯·ä½¿ç”¨: clear")

    @filter.permission_type(filter.PermissionType.ADMIN)
    @filter.command("web_mode", alias={"åˆ†ææ¨¡å¼", "ç½‘é¡µåˆ†ææ¨¡å¼"})
    async def manage_analysis_mode(self, event: AstrMessageEvent):
        """ç®¡ç†æ’ä»¶çš„ç½‘é¡µåˆ†ææ¨¡å¼"""
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()

        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰æ¨¡å¼
        if len(message_parts) <= 1:
            mode_names = {
                "auto": "è‡ªåŠ¨åˆ†æ",
                "manual": "æ‰‹åŠ¨åˆ†æ",
                "hybrid": "æ··åˆæ¨¡å¼"
            }
            mode_info = "**å½“å‰åˆ†ææ¨¡å¼**\n\n"
            mode_info += f"- æ¨¡å¼: {mode_names.get(self.analysis_mode, self.analysis_mode)} ({self.analysis_mode})\n"
            mode_info += f"- è‡ªåŠ¨åˆ†æ: {'âœ… å·²å¯ç”¨' if self.auto_analyze else 'âŒ å·²ç¦ç”¨'}\n\n"
            mode_info += "ä½¿ç”¨ `/web_mode <æ¨¡å¼>` åˆ‡æ¢æ¨¡å¼\n"
            mode_info += "æ”¯æŒçš„æ¨¡å¼: auto, manual, hybrid"

            yield event.plain_result(mode_info)
            return

        # è§£ææ¨¡å¼å‚æ•°
        mode = message_parts[1].lower() if len(message_parts) > 1 else ""
        valid_modes = ["auto", "manual", "hybrid"]

        # éªŒè¯æ¨¡å¼æ˜¯å¦æœ‰æ•ˆ
        if mode not in valid_modes:
            yield event.plain_result(
                f"æ— æ•ˆçš„æ¨¡å¼ï¼Œè¯·ä½¿ç”¨: {', '.join(valid_modes)}"
            )
            return

        # æ›´æ–°åˆ†ææ¨¡å¼
        self.analysis_mode = mode
        self.auto_analyze = (mode == "auto")

        # ä¿å­˜é…ç½®
        analysis_settings = self.config.get("analysis_settings", {})
        analysis_settings["analysis_mode"] = mode
        self.config["analysis_settings"] = analysis_settings
        self.config.save_config()

        mode_names = {
            "auto": "è‡ªåŠ¨åˆ†æ",
            "manual": "æ‰‹åŠ¨åˆ†æ",
            "hybrid": "æ··åˆæ¨¡å¼"
        }
        yield event.plain_result(
            f"âœ… å·²åˆ‡æ¢åˆ° {mode_names.get(mode, mode)} æ¨¡å¼"
        )

    @filter.command("web_export", alias={"å¯¼å‡ºåˆ†æç»“æœ", "ç½‘é¡µå¯¼å‡º"})
    async def export_analysis_result(self, event: AstrMessageEvent):
        """å¯¼å‡ºç½‘é¡µåˆ†æç»“æœ"""
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()

        # æ£€æŸ¥å‚æ•°æ˜¯å¦è¶³å¤Ÿ
        if len(message_parts) < 2:
            yield event.plain_result(
                "è¯·æä¾›è¦å¯¼å‡ºçš„URLé“¾æ¥å’Œæ ¼å¼ï¼Œä¾‹å¦‚ï¼š/web_export https://example.com md æˆ– /web_export all json"
            )
            return

        # è·å–å¯¼å‡ºèŒƒå›´å’Œæ ¼å¼
        url_or_all = message_parts[1]
        format_type = message_parts[2] if len(message_parts) > 2 else "md"

        # éªŒè¯æ ¼å¼ç±»å‹æ˜¯å¦æ”¯æŒ
        supported_formats = ["md", "markdown", "json", "txt"]
        if format_type.lower() not in supported_formats:
            yield event.plain_result(
                f"ä¸æ”¯æŒçš„æ ¼å¼ç±»å‹ï¼Œè¯·ä½¿ç”¨ï¼š{', '.join(supported_formats)}"
            )
            return

        # å‡†å¤‡å¯¼å‡ºæ•°æ®
        export_results = []

        if url_or_all.lower() == "all":
            # å¯¼å‡ºæ‰€æœ‰ç¼“å­˜çš„åˆ†æç»“æœ
            if not self.cache_manager.memory_cache:
                yield event.plain_result("å½“å‰æ²¡æœ‰ç¼“å­˜çš„åˆ†æç»“æœ")
                return

            for url, cache_data in self.cache_manager.memory_cache.items():
                export_results.append({"url": url, "result": cache_data["result"]})
        else:
            # å¯¼å‡ºæŒ‡å®šURLçš„åˆ†æç»“æœ
            url = url_or_all

            # æ£€æŸ¥URLæ ¼å¼æ˜¯å¦æœ‰æ•ˆ
            if not self.analyzer.is_valid_url(url):
                yield event.plain_result("æ— æ•ˆçš„URLé“¾æ¥")
                return

            # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰è¯¥URLçš„åˆ†æç»“æœ
            cached_result = self._check_cache(url)
            if cached_result:
                export_results.append({"url": url, "result": cached_result})
            else:
                # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå…ˆè¿›è¡Œåˆ†æ
                yield event.plain_result("ç¼“å­˜ä¸­æ²¡æœ‰è¯¥URLçš„åˆ†æç»“æœï¼Œæ­£åœ¨è¿›è¡Œåˆ†æ...")

                # æŠ“å–å¹¶åˆ†æç½‘é¡µ
                async with WebAnalyzer(
                    self.max_content_length,
                    self.timeout,
                    self.user_agent,
                    self.proxy,
                    self.retry_count,
                    self.retry_delay,
                ) as analyzer:
                    html = await analyzer.fetch_webpage(url)
                    if not html:
                        yield event.plain_result(f"æ— æ³•æŠ“å–ç½‘é¡µå†…å®¹: {url}")
                        return

                    content_data = analyzer.extract_content(html, url)
                    if not content_data:
                        yield event.plain_result(f"æ— æ³•è§£æç½‘é¡µå†…å®¹: {url}")
                        return

                    # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
                    if self.enable_translation:
                        translated_content = await self._translate_content(
                            event, content_data["content"]
                        )
                        translated_content_data = content_data.copy()
                        translated_content_data["content"] = translated_content
                        analysis_result = await self.analyze_with_llm(
                            event, translated_content_data
                        )
                    else:
                        analysis_result = await self.analyze_with_llm(
                            event, content_data
                        )

                    # æå–ç‰¹å®šå†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    specific_content = self._extract_specific_content(html, url)
                    if specific_content:
                        # åœ¨åˆ†æç»“æœä¸­æ·»åŠ ç‰¹å®šå†…å®¹
                        specific_content_str = "\n\n**ç‰¹å®šå†…å®¹æå–**\n"

                        if "images" in specific_content and specific_content["images"]:
                            specific_content_str += (
                                f"\nğŸ“· å›¾ç‰‡é“¾æ¥ ({len(specific_content['images'])}):\n"
                            )
                            for img_url in specific_content["images"]:
                                specific_content_str += f"- {img_url}\n"

                        if "links" in specific_content and specific_content["links"]:
                            specific_content_str += (
                                f"\nğŸ”— ç›¸å…³é“¾æ¥ ({len(specific_content['links'])}):\n"
                            )
                            for link in specific_content["links"][
                                :5
                            ]:  # åªæ˜¾ç¤ºå‰5ä¸ªé“¾æ¥
                                specific_content_str += (
                                    f"- [{link['text']}]({link['url']})\n"
                                )

                        if (
                            "code_blocks" in specific_content
                            and specific_content["code_blocks"]
                        ):
                            specific_content_str += f"\nğŸ’» ä»£ç å— ({len(specific_content['code_blocks'])}):\n"
                            for i, code in enumerate(
                                specific_content["code_blocks"][:2]
                            ):  # åªæ˜¾ç¤ºå‰2ä¸ªä»£ç å—
                                specific_content_str += f"```\n{code}\n```\n"

                        analysis_result += specific_content_str

                    # å‡†å¤‡å¯¼å‡ºæ•°æ®
                    export_results.append(
                        {
                            "url": url,
                            "result": {
                                "url": url,
                                "result": analysis_result,
                                "screenshot": None,
                            },
                        }
                    )

        # æ‰§è¡Œå¯¼å‡ºæ“ä½œ
        try:
            import json
            import os
            import time

            # åˆ›å»ºdataç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            data_dir = os.path.join(os.path.dirname(__file__), "data")
            os.makedirs(data_dir, exist_ok=True)

            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = int(time.time())
            if len(export_results) == 1:
                # å•ä¸ªURLå¯¼å‡ºï¼Œä½¿ç”¨åŸŸåä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†
                url = export_results[0]["url"]
                from urllib.parse import urlparse

                parsed = urlparse(url)
                domain = parsed.netloc.replace(".", "_")
                filename = f"web_analysis_{domain}_{timestamp}"
            else:
                # å¤šä¸ªURLå¯¼å‡º
                filename = f"web_analysis_all_{timestamp}"

            # ç¡®å®šæ–‡ä»¶æ‰©å±•å
            file_extension = format_type.lower()
            if file_extension == "markdown":
                file_extension = "md"

            file_path = os.path.join(data_dir, f"{filename}.{file_extension}")

            if format_type.lower() in ["md", "markdown"]:
                # ç”ŸæˆMarkdownæ ¼å¼å†…å®¹
                md_content = "# ç½‘é¡µåˆ†æç»“æœå¯¼å‡º\n\n"
                md_content += f"å¯¼å‡ºæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}\n\n"
                md_content += f"å…± {len(export_results)} ä¸ªåˆ†æç»“æœ\n\n"
                md_content += "---\n\n"

                for i, export_item in enumerate(export_results, 1):
                    url = export_item["url"]
                    result_data = export_item["result"]

                    md_content += f"## {i}. {url}\n\n"
                    md_content += result_data["result"]
                    md_content += "\n\n"
                    md_content += "---\n\n"

                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(md_content)

            elif format_type.lower() == "json":
                # ç”ŸæˆJSONæ ¼å¼å†…å®¹
                json_data = {
                    "export_time": timestamp,
                    "export_time_str": time.strftime(
                        "%Y-%m-%d %H:%M:%S", time.localtime(timestamp)
                    ),
                    "total_results": len(export_results),
                    "results": [],
                }

                for export_item in export_results:
                    url = export_item["url"]
                    result_data = export_item["result"]

                    json_data["results"].append(
                        {
                            "url": url,
                            "analysis_result": result_data["result"],
                            "has_screenshot": result_data["screenshot"] is not None,
                        }
                    )

                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    json.dump(json_data, f, ensure_ascii=False, indent=2)

            elif format_type.lower() == "txt":
                # ç”Ÿæˆçº¯æ–‡æœ¬æ ¼å¼å†…å®¹
                txt_content = "ç½‘é¡µåˆ†æç»“æœå¯¼å‡º\n"
                txt_content += f"å¯¼å‡ºæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}\n"
                txt_content += f"å…± {len(export_results)} ä¸ªåˆ†æç»“æœ\n"
                txt_content += "=" * 50 + "\n\n"

                for i, export_item in enumerate(export_results, 1):
                    url = export_item["url"]
                    result_data = export_item["result"]

                    txt_content += f"{i}. {url}\n"
                    txt_content += "-" * 30 + "\n"
                    txt_content += result_data["result"]
                    txt_content += "\n\n"
                    txt_content += "=" * 50 + "\n\n"

                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(txt_content)

            # å‘é€å¯¼å‡ºæˆåŠŸæ¶ˆæ¯ï¼Œå¹¶é™„å¸¦å¯¼å‡ºæ–‡ä»¶
            from astrbot.api.message_components import File, Plain

            # æ„å»ºæ¶ˆæ¯é“¾
            message_chain = [
                Plain("âœ… åˆ†æç»“æœå¯¼å‡ºæˆåŠŸï¼\n\n"),
                Plain(f"å¯¼å‡ºæ ¼å¼: {format_type}\n"),
                Plain(f"å¯¼å‡ºæ•°é‡: {len(export_results)}\n\n"),
                Plain("ğŸ“ å¯¼å‡ºæ–‡ä»¶ï¼š\n"),
                File(file=file_path, name=os.path.basename(file_path)),
            ]

            yield event.chain_result(message_chain)

            logger.info(
                f"æˆåŠŸå¯¼å‡º {len(export_results)} ä¸ªåˆ†æç»“æœåˆ° {file_path}ï¼Œå¹¶å‘é€ç»™ç”¨æˆ·"
            )

        except Exception as e:
            logger.error(f"å¯¼å‡ºåˆ†æç»“æœå¤±è´¥: {e}")
            yield event.plain_result(f"âŒ å¯¼å‡ºåˆ†æç»“æœå¤±è´¥: {str(e)}")

    def _save_group_blacklist(self):
        """ä¿å­˜ç¾¤èŠé»‘åå•åˆ°é…ç½®æ–‡ä»¶"""
        try:
            # å°†ç¾¤èŠåˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªç¾¤èŠID
            group_text = "\n".join(self.group_blacklist)
            # è·å–å½“å‰group_settingsé…ç½®
            group_settings = self.config.get("group_settings", {})
            # æ›´æ–°group_blacklist
            group_settings["group_blacklist"] = group_text
            # æ›´æ–°é…ç½®å¹¶ä¿å­˜åˆ°æ–‡ä»¶
            self.config["group_settings"] = group_settings
            self.config.save_config()
        except Exception as e:
            logger.error(f"ä¿å­˜ç¾¤èŠé»‘åå•å¤±è´¥: {e}")

    def _check_cache(self, url: str) -> dict:
        """æ£€æŸ¥æŒ‡å®šURLçš„ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ"""
        if not self.enable_cache:
            return None

        # è§„èŒƒåŒ–URLï¼Œç»Ÿä¸€æ ¼å¼
        normalized_url = self.analyzer.normalize_url(url)
        return self.cache_manager.get(normalized_url)

    def _update_cache(self, url: str, result: dict, content: str = None):
        """æ›´æ–°æŒ‡å®šURLçš„ç¼“å­˜ï¼Œæ”¯æŒåŸºäºå†…å®¹å“ˆå¸Œçš„ç¼“å­˜ç­–ç•¥"""
        if not self.enable_cache:
            return

        # è§„èŒƒåŒ–URLï¼Œç»Ÿä¸€æ ¼å¼
        normalized_url = self.analyzer.normalize_url(url)

        # å¦‚æœæä¾›äº†å†…å®¹ï¼Œä½¿ç”¨åŸºäºå†…å®¹å“ˆå¸Œçš„ç¼“å­˜ç­–ç•¥
        if content:
            self.cache_manager.set_with_content_hash(normalized_url, result, content)
        else:
            # å¦åˆ™ä½¿ç”¨æ™®é€šçš„URLç¼“å­˜ç­–ç•¥
            self.cache_manager.set(normalized_url, result)

    def _clean_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        # ç¼“å­˜ç®¡ç†å™¨ä¼šè‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜ï¼Œè¿™é‡Œç•™ç©ºå³å¯
        pass

    async def _translate_content(self, event: AstrMessageEvent, content: str) -> str:
        """ç¿»è¯‘ç½‘é¡µå†…å®¹"""
        if not self.enable_translation:
            return content

        try:
            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨
            if not hasattr(self.context, "llm_generate"):
                logger.error("LLMä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘")
                return content

            # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„LLMæä¾›å•†ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨å½“å‰ä¼šè¯çš„æ¨¡å‹
            provider_id = self.llm_provider
            if not provider_id:
                umo = event.unified_msg_origin
                provider_id = await self.context.get_current_chat_provider_id(umo=umo)

            if not provider_id:
                logger.error("æ— æ³•è·å–LLMæä¾›å•†IDï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘")
                return content

            # ä½¿ç”¨è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯æˆ–é»˜è®¤æç¤ºè¯
            if self.custom_translation_prompt:
                # æ›¿æ¢è‡ªå®šä¹‰æç¤ºè¯ä¸­çš„å˜é‡
                prompt = self.custom_translation_prompt.format(
                    content=content, target_language=self.target_language
                )
            else:
                # é»˜è®¤ç¿»è¯‘æç¤ºè¯
                prompt = f"è¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆ{self.target_language}è¯­è¨€ï¼Œä¿æŒåŸæ–‡æ„æ€ä¸å˜ï¼Œè¯­è¨€æµç•…è‡ªç„¶ï¼š\n\n{content}"

            # è°ƒç”¨LLMè¿›è¡Œç¿»è¯‘
            llm_resp = await self.context.llm_generate(
                chat_provider_id=provider_id, prompt=prompt
            )

            if llm_resp and llm_resp.completion_text:
                return llm_resp.completion_text.strip()
            else:
                logger.error("LLMç¿»è¯‘è¿”å›ä¸ºç©º")
                return content
        except Exception as e:
            logger.error(f"ç¿»è¯‘å†…å®¹å¤±è´¥: {e}")
            return content

    def _extract_specific_content(self, html: str, url: str) -> dict:
        """æå–ç‰¹å®šç±»å‹çš„å†…å®¹"""
        if not self.enable_specific_extraction:
            return {}

        try:
            # ç›´æ¥ä½¿ç”¨å·²æœ‰analyzerå®ä¾‹ï¼Œé¿å…é‡å¤åˆ›å»º
            return self.analyzer.extract_specific_content(html, url, self.extract_types)
        except Exception as e:
            logger.error(f"æå–ç‰¹å®šå†…å®¹å¤±è´¥: {e}")
            return {}

    async def _send_analysis_result(self, event, analysis_results):
        """å‘é€åˆ†æç»“æœï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨åˆå¹¶è½¬å‘"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„åˆ†æç»“æœ
        if not analysis_results:
            logger.info("æ²¡æœ‰åˆ†æç»“æœï¼Œä¸å‘é€æ¶ˆæ¯")
            return

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç»“æœéƒ½æ˜¯é”™è¯¯ç»“æœï¼ˆæ²¡æœ‰æˆªå›¾ä¸”ç»“æœåŒ…å«é”™è¯¯å…³é”®è¯ï¼‰
        all_errors = True
        for result in analysis_results:
            # å¦‚æœæœ‰æˆªå›¾ï¼Œè¯´æ˜è‡³å°‘æœ‰ä¸€ä¸ªæˆåŠŸçš„ç»“æœ
            if result.get("screenshot"):
                all_errors = False
                break
            # æ£€æŸ¥ç»“æœæ˜¯å¦åŒ…å«é”™è¯¯å…³é”®è¯
            result_text = result.get("result", "")
            if not any(keyword in result_text for keyword in ["å¤±è´¥", "é”™è¯¯", "æ— æ³•", "âŒ"]):
                all_errors = False
                break

        # å¦‚æœæ‰€æœ‰ç»“æœéƒ½æ˜¯é”™è¯¯ï¼Œä¸å‘é€æ¶ˆæ¯
        if all_errors:
            logger.info("æ‰€æœ‰URLåˆ†æå¤±è´¥ï¼Œä¸å‘é€æ¶ˆæ¯")
            return

        try:
            import os
            import tempfile

            from astrbot.api.message_components import Image, Node, Nodes, Plain

            # æ£€æŸ¥æ˜¯å¦ä¸ºç¾¤èŠæ¶ˆæ¯ä¸”åˆå¹¶è½¬å‘åŠŸèƒ½å·²å¯ç”¨
            group_id = None
            if hasattr(event, "group_id") and event.group_id:
                group_id = event.group_id
            elif (
                hasattr(event, "message_obj")
                and hasattr(event.message_obj, "group_id")
                and event.message_obj.group_id
            ):
                group_id = event.message_obj.group_id

            # æ ¹æ®æ¶ˆæ¯ç±»å‹å†³å®šæ˜¯å¦ä½¿ç”¨åˆå¹¶è½¬å‘
            is_group = bool(group_id)
            is_private = not is_group

            # å¦‚æœæ˜¯ç¾¤èŠä¸”ç¾¤èŠåˆå¹¶è½¬å‘å·²å¯ç”¨ï¼Œæˆ–è€…æ˜¯ç§èŠä¸”ç§èŠåˆå¹¶è½¬å‘å·²å¯ç”¨ï¼Œä¸”ä¸æ˜¯åªå‘é€æˆªå›¾
            if (self.send_content_type != "screenshot_only") and (
                (is_group and self.merge_forward_enabled["group"])
                or (is_private and self.merge_forward_enabled["private"])
            ):
                # ä½¿ç”¨åˆå¹¶è½¬å‘ - å°†æ‰€æœ‰åˆ†æç»“æœåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
                nodes = []

                # æ·»åŠ æ€»æ ‡é¢˜èŠ‚ç‚¹
                total_title_node = Node(
                    uin=event.get_sender_id(),
                    name="ç½‘é¡µåˆ†æç»“æœæ±‡æ€»",
                    content=[Plain(f"å…±{len(analysis_results)}ä¸ªç½‘é¡µåˆ†æç»“æœ")],
                )
                nodes.append(total_title_node)

                # ä¸ºæ¯ä¸ªURLæ·»åŠ åˆ†æç»“æœèŠ‚ç‚¹
                for i, result_data in enumerate(analysis_results, 1):
                    url = result_data["url"]
                    analysis_result = result_data["result"]
                    screenshot = result_data.get("screenshot")

                    # æ·»åŠ å½“å‰URLçš„æ ‡é¢˜èŠ‚ç‚¹
                    url_title_node = Node(
                        uin=event.get_sender_id(),
                        name=f"åˆ†æç»“æœ {i}",
                        content=[Plain(f"ç¬¬{i}ä¸ªç½‘é¡µåˆ†æç»“æœ - {url}")],
                    )
                    nodes.append(url_title_node)

                    # å¤„ç†æˆªå›¾ï¼Œå‡†å¤‡åˆ›å»ºå›¾ç‰‡ç»„ä»¶
                    image_component = None
                    if (
                        self.merge_forward_enabled.get("include_screenshot", False)
                        and screenshot
                        and self.send_content_type != "analysis_only"
                    ):
                        try:
                            # æ ¹æ®æˆªå›¾æ ¼å¼è®¾ç½®æ–‡ä»¶åç¼€
                            suffix = (
                                f".{self.screenshot_format}"
                                if self.screenshot_format
                                else ".jpg"
                            )
                            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æˆªå›¾
                            with tempfile.NamedTemporaryFile(
                                suffix=suffix, delete=False
                            ) as temp_file:
                                temp_file.write(screenshot)
                                temp_file_path = temp_file.name

                            # åˆ›å»ºå›¾ç‰‡ç»„ä»¶
                            image_component = Image.fromFileSystem(temp_file_path)

                            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶è·¯å¾„ï¼Œä»¥ä¾¿åç»­æ¸…ç†
                            if "temp_files" not in locals():
                                temp_files = []
                            temp_files.append(temp_file_path)
                        except Exception as e:
                            logger.error(f"å¤„ç†æˆªå›¾å¤±è´¥: {e}")
                            # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
                            if "temp_file_path" in locals() and os.path.exists(
                                temp_file_path
                            ):
                                os.unlink(temp_file_path)

                    # æ ¹æ®å‘é€å†…å®¹ç±»å‹å†³å®šæ˜¯å¦æ·»åŠ åˆ†æç»“æœèŠ‚ç‚¹
                    if self.send_content_type != "screenshot_only":
                        content = [Plain(analysis_result)]
                        content_node = Node(
                            uin=event.get_sender_id(),
                            name="è¯¦ç»†åˆ†æ",
                            content=content,
                        )
                        nodes.append(content_node)

                    # å¦‚æœå¯ç”¨äº†åˆå¹¶è½¬å‘åŒ…å«æˆªå›¾åŠŸèƒ½ï¼Œå¹¶ä¸”æœ‰æˆªå›¾ï¼Œä¸”éœ€è¦å‘é€æˆªå›¾ï¼Œåˆ™åˆ›å»ºå•ç‹¬çš„æˆªå›¾èŠ‚ç‚¹
                    if (
                        self.merge_forward_enabled.get("include_screenshot", False)
                        and screenshot
                        and self.send_content_type != "analysis_only"
                    ):
                        try:
                            # åˆ›å»ºå•ç‹¬çš„æˆªå›¾èŠ‚ç‚¹
                            screenshot_node = Node(
                                uin=event.get_sender_id(),
                                name="ç½‘é¡µæˆªå›¾",
                                content=[image_component],
                            )
                            nodes.append(screenshot_node)
                        except Exception as e:
                            logger.error(f"åˆ›å»ºæˆªå›¾èŠ‚ç‚¹å¤±è´¥: {e}")

                # ä½¿ç”¨NodesåŒ…è£…æ‰€æœ‰èŠ‚ç‚¹ï¼Œåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
                merge_forward_message = Nodes(nodes)

                # å‘é€åˆå¹¶è½¬å‘æ¶ˆæ¯
                yield event.chain_result([merge_forward_message])

                # å¦‚æœæœªå¯ç”¨åˆå¹¶è½¬å‘åŒ…å«æˆªå›¾åŠŸèƒ½ï¼Œä¸”éœ€è¦å‘é€æˆªå›¾ï¼Œåˆ™é€ä¸ªå‘é€æˆªå›¾
                if (
                    not self.merge_forward_enabled.get("include_screenshot", False)
                    and self.send_content_type != "analysis_only"
                ):
                    for result_data in analysis_results:
                        screenshot = result_data.get("screenshot")
                        if screenshot:
                            try:
                                # æ ¹æ®æˆªå›¾æ ¼å¼è®¾ç½®æ–‡ä»¶åç¼€
                                suffix = (
                                    f".{self.screenshot_format}"
                                    if self.screenshot_format
                                    else ".jpg"
                                )
                                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æˆªå›¾
                                with tempfile.NamedTemporaryFile(
                                    suffix=suffix, delete=False
                                ) as temp_file:
                                    temp_file.write(screenshot)
                                    temp_file_path = temp_file.name

                                # ä½¿ç”¨Image.fromFileSystem()æ–¹æ³•å‘é€å›¾ç‰‡
                                image_component = Image.fromFileSystem(temp_file_path)
                                yield event.chain_result([image_component])
                                logger.info(
                                    f"ç¾¤èŠ {group_id} ä½¿ç”¨åˆå¹¶è½¬å‘å‘é€åˆ†æç»“æœï¼Œå¹¶å‘é€æˆªå›¾"
                                )

                                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                                os.unlink(temp_file_path)
                            except Exception as e:
                                logger.error(f"å‘é€æˆªå›¾å¤±è´¥: {e}")
                                # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
                                if "temp_file_path" in locals() and os.path.exists(
                                    temp_file_path
                                ):
                                    os.unlink(temp_file_path)
                            if "temp_file_path" in locals() and os.path.exists(
                                temp_file_path
                            ):
                                os.unlink(temp_file_path)
                # æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
                if "temp_files" in locals():
                    for temp_file_path in temp_files:
                        try:
                            if os.path.exists(temp_file_path):
                                os.unlink(temp_file_path)
                        except Exception as e:
                            logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
                logger.info(
                    f"ç¾¤èŠ {group_id} ä½¿ç”¨åˆå¹¶è½¬å‘å‘é€{len(analysis_results)}ä¸ªåˆ†æç»“æœ"
                )
            else:
                # æ™®é€šå‘é€
                for i, result_data in enumerate(analysis_results, 1):
                    screenshot = result_data.get("screenshot")
                    analysis_result = result_data.get("result")

                    # å¦‚æœåªå‘é€æˆªå›¾
                    if self.send_content_type == "screenshot_only":
                        if screenshot:
                            try:
                                # æ ¹æ®æˆªå›¾æ ¼å¼è®¾ç½®æ–‡ä»¶åç¼€
                                suffix = (
                                    f".{self.screenshot_format}"
                                    if self.screenshot_format
                                    else ".jpg"
                                )
                                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æˆªå›¾
                                with tempfile.NamedTemporaryFile(
                                    suffix=suffix, delete=False
                                ) as temp_file:
                                    temp_file.write(screenshot)
                                    temp_file_path = temp_file.name

                                # ä½¿ç”¨Image.fromFileSystem()æ–¹æ³•å‘é€å›¾ç‰‡
                                image_component = Image.fromFileSystem(temp_file_path)
                                yield event.chain_result([image_component])
                                logger.info("åªå‘é€æˆªå›¾")

                                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                                os.unlink(temp_file_path)
                            except Exception as e:
                                logger.error(f"å‘é€æˆªå›¾å¤±è´¥: {e}")
                                # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
                                if "temp_file_path" in locals() and os.path.exists(
                                    temp_file_path
                                ):
                                    os.unlink(temp_file_path)
                    # å‘é€åˆ†æç»“æœæˆ–ä¸¤è€…éƒ½å‘é€
                    else:
                        url = result_data["url"]
                        # æ ¹æ®å‘é€å†…å®¹ç±»å‹å†³å®šæ˜¯å¦å‘é€åˆ†æç»“æœæ–‡æœ¬
                        if self.send_content_type != "screenshot_only":
                            if len(analysis_results) == 1:
                                result_text = f"ç½‘é¡µåˆ†æç»“æœï¼š\n{analysis_result}"
                            else:
                                result_text = f"ç¬¬{i}/{len(analysis_results)}ä¸ªç½‘é¡µåˆ†æç»“æœï¼š\n{analysis_result}"
                            yield event.plain_result(result_text)

                        # æ ¹æ®å‘é€å†…å®¹ç±»å‹å†³å®šæ˜¯å¦å‘é€æˆªå›¾
                        if screenshot and self.send_content_type != "analysis_only":
                            try:
                                # æ ¹æ®æˆªå›¾æ ¼å¼è®¾ç½®æ–‡ä»¶åç¼€
                                suffix = (
                                    f".{self.screenshot_format}"
                                    if self.screenshot_format
                                    else ".jpg"
                                )
                                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æˆªå›¾
                                with tempfile.NamedTemporaryFile(
                                    suffix=suffix, delete=False
                                ) as temp_file:
                                    temp_file.write(screenshot)
                                    temp_file_path = temp_file.name

                                # ä½¿ç”¨Image.fromFileSystem()æ–¹æ³•å‘é€å›¾ç‰‡
                                image_component = Image.fromFileSystem(temp_file_path)
                                yield event.chain_result([image_component])
                                logger.info("æ™®é€šå‘é€åˆ†æç»“æœï¼Œå¹¶å‘é€æˆªå›¾")

                                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                                os.unlink(temp_file_path)
                            except Exception as e:
                                logger.error(f"å‘é€æˆªå›¾å¤±è´¥: {e}")
                                # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
                                if "temp_file_path" in locals() and os.path.exists(
                                    temp_file_path
                                ):
                                    os.unlink(temp_file_path)
                message_type = "ç¾¤èŠ" if group_id else "ç§èŠ"
                logger.info(
                    f"{message_type}æ¶ˆæ¯æ™®é€šå‘é€{len(analysis_results)}ä¸ªåˆ†æç»“æœ"
                )
        except Exception as e:
            logger.error(f"å‘é€åˆ†æç»“æœå¤±è´¥: {e}")
            yield event.plain_result(f"âŒ å‘é€åˆ†æç»“æœå¤±è´¥: {str(e)}")

    async def terminate(self):
        """æ’ä»¶å¸è½½æ—¶çš„æ¸…ç†å·¥ä½œ"""
        logger.info("ç½‘é¡µåˆ†ææ’ä»¶å·²å¸è½½")
