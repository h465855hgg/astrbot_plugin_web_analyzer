"""
AstrBot ç½‘é¡µåˆ†ææ’ä»¶

è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„AstrBotæ’ä»¶ï¼Œä¸“é—¨ç”¨äºç½‘é¡µå†…å®¹çš„æ™ºèƒ½åˆ†æå’Œæ€»ç»“ã€‚

âœ¨ æ ¸å¿ƒåŠŸèƒ½
- ğŸ¤– è‡ªåŠ¨è¯†åˆ«æ¶ˆæ¯ä¸­çš„ç½‘é¡µé“¾æ¥ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡ä»¤
- ğŸŒ æ™ºèƒ½æŠ“å–å’Œè§£æç½‘é¡µå†…å®¹ï¼Œæ”¯æŒå¤šç§ç½‘ç«™ç»“æ„
- ğŸ§  é›†æˆå¤§è¯­è¨€æ¨¡å‹(LLM)ï¼Œæä¾›æ·±åº¦åˆ†æå’Œæ€»ç»“
- ğŸ“¸ æ”¯æŒç½‘é¡µæˆªå›¾ï¼Œç›´è§‚å±•ç¤ºç½‘é¡µå†…å®¹
- ğŸ”„ å†…ç½®ç¼“å­˜æœºåˆ¶ï¼Œæå‡é‡å¤è®¿é—®çš„å“åº”é€Ÿåº¦
- ğŸ“ æ”¯æŒå¤šç§åˆ†æç»“æœå¯¼å‡ºæ ¼å¼
- ğŸ”§ æä¾›ä¸°å¯Œçš„ç®¡ç†å‘½ä»¤ï¼Œæ–¹ä¾¿é…ç½®å’Œç»´æŠ¤

ğŸ“– ä½¿ç”¨æ–¹å¼
- è‡ªåŠ¨æ¨¡å¼ï¼šç›´æ¥å‘é€åŒ…å«ç½‘é¡µé“¾æ¥çš„æ¶ˆæ¯
- æ‰‹åŠ¨æ¨¡å¼ï¼šä½¿ç”¨ `/ç½‘é¡µåˆ†æ` å‘½ä»¤ï¼Œä¾‹å¦‚ï¼š`/ç½‘é¡µåˆ†æ https://example.com`
- æ”¯æŒå¤šç§æŒ‡ä»¤åˆ«åï¼š`åˆ†æ`ã€`æ€»ç»“`ã€`web`ã€`analyze`

ğŸ¯ æ’ä»¶ä¼˜åŠ¿
- å¼‚æ­¥å¤„ç†è®¾è®¡ï¼Œæ”¯æŒå¹¶å‘åˆ†æå¤šä¸ªURL
- çµæ´»çš„é…ç½®é€‰é¡¹ï¼Œæ»¡è¶³ä¸åŒä½¿ç”¨åœºæ™¯
- å®Œå–„çš„é”™è¯¯å¤„ç†ï¼Œç¡®ä¿æ’ä»¶ç¨³å®šè¿è¡Œ
- æ”¯æŒåŸŸåç™½åå•å’Œé»‘åå•ï¼Œæ§åˆ¶è®¿é—®èŒƒå›´
- æ”¯æŒå†…å®¹ç¿»è¯‘ï¼Œçªç ´è¯­è¨€éšœç¢

æœ¬æ’ä»¶é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼ŒåŒ…å«ç¼“å­˜ç®¡ç†ã€ç½‘é¡µåˆ†æã€å‘½ä»¤å¤„ç†ç­‰å¤šä¸ªç»„ä»¶ï¼Œ
å¯æ ¹æ®éœ€æ±‚çµæ´»æ‰©å±•å’Œå®šåˆ¶ã€‚
"""

from typing import List, Dict, Any, Optional

from astrbot.api import AstrBotConfig
from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.star import Context, Star, register
from astrbot.api import logger

from .analyzer import WebAnalyzer
from .cache import CacheManager
from .utils import WebAnalyzerUtils


# é”™è¯¯ç±»å‹æšä¸¾
class ErrorType:
    """é”™è¯¯ç±»å‹æšä¸¾ï¼Œç”¨äºé”™è¯¯åˆ†ç±»å’Œå¤„ç†"""
    NETWORK_ERROR = "network_error"  # ç½‘ç»œé”™è¯¯
    PARSING_ERROR = "parsing_error"  # è§£æé”™è¯¯
    LLM_ERROR = "llm_error"  # LLMç›¸å…³é”™è¯¯
    SCREENSHOT_ERROR = "screenshot_error"  # æˆªå›¾é”™è¯¯
    CACHE_ERROR = "cache_error"  # ç¼“å­˜é”™è¯¯
    CONFIG_ERROR = "config_error"  # é…ç½®é”™è¯¯
    PERMISSION_ERROR = "permission_error"  # æƒé™é”™è¯¯
    UNKNOWN_ERROR = "unknown_error"  # æœªçŸ¥é”™è¯¯


# é”™è¯¯ä¸¥é‡ç¨‹åº¦æšä¸¾
class ErrorSeverity:
    """é”™è¯¯ä¸¥é‡ç¨‹åº¦æšä¸¾ï¼Œç”¨äºé”™è¯¯åˆ†çº§"""
    INFO = "info"  # ä¿¡æ¯çº§é”™è¯¯ï¼Œä»…è®°å½•ï¼Œä¸å½±å“åŠŸèƒ½
    WARNING = "warning"  # è­¦å‘Šçº§é”™è¯¯ï¼Œå¯èƒ½å½±å“éƒ¨åˆ†åŠŸèƒ½
    ERROR = "error"  # é”™è¯¯çº§é”™è¯¯ï¼Œå½±å“ä¸»è¦åŠŸèƒ½
    CRITICAL = "critical"  # ä¸¥é‡é”™è¯¯ï¼Œå¯¼è‡´åŠŸèƒ½å®Œå…¨ä¸å¯ç”¨


# é”™è¯¯å¤„ç†é…ç½®
ERROR_MESSAGES: Dict[str, Dict[str, Any]] = {
    "network_error": {
        "message": "ç½‘ç»œè¯·æ±‚å¤±è´¥",
        "solution": "è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–URLæ˜¯å¦æ­£ç¡®ï¼Œæˆ–å°è¯•è°ƒæ•´è¯·æ±‚è¶…æ—¶è®¾ç½®",
        "severity": ErrorSeverity.ERROR
    },
    "parsing_error": {
        "message": "ç½‘é¡µå†…å®¹è§£æå¤±è´¥",
        "solution": "è¯¥ç½‘é¡µç»“æ„å¯èƒ½è¾ƒä¸ºç‰¹æ®Šï¼Œå»ºè®®å°è¯•å…¶ä»–åˆ†ææ–¹å¼",
        "severity": ErrorSeverity.WARNING
    },
    "llm_error": {
        "message": "å¤§è¯­è¨€æ¨¡å‹åˆ†æå¤±è´¥",
        "solution": "è¯·æ£€æŸ¥LLMé…ç½®æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å°è¯•è°ƒæ•´åˆ†æå‚æ•°",
        "severity": ErrorSeverity.ERROR
    },
    "screenshot_error": {
        "message": "ç½‘é¡µæˆªå›¾å¤±è´¥",
        "solution": "è¯·æ£€æŸ¥æµè§ˆå™¨é…ç½®æˆ–ç½‘ç»œè¿æ¥ï¼Œæˆ–å°è¯•è°ƒæ•´æˆªå›¾å‚æ•°",
        "severity": ErrorSeverity.WARNING
    },
    "cache_error": {
        "message": "ç¼“å­˜æ“ä½œå¤±è´¥",
        "solution": "è¯·æ£€æŸ¥ç¼“å­˜ç›®å½•æƒé™æˆ–å­˜å‚¨ç©ºé—´",
        "severity": ErrorSeverity.WARNING
    },
    "config_error": {
        "message": "é…ç½®é”™è¯¯",
        "solution": "è¯·æ£€æŸ¥æ’ä»¶é…ç½®æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å°è¯•é‡ç½®é…ç½®",
        "severity": ErrorSeverity.ERROR
    },
    "permission_error": {
        "message": "æƒé™ä¸è¶³",
        "solution": "è¯·æ£€æŸ¥æ’ä»¶æƒé™é…ç½®ï¼Œæˆ–è”ç³»ç®¡ç†å‘˜è·å–æƒé™",
        "severity": ErrorSeverity.ERROR
    },
    "unknown_error": {
        "message": "æœªçŸ¥é”™è¯¯",
        "solution": "è¯·æ£€æŸ¥æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ï¼Œæˆ–å°è¯•é‡å¯æ’ä»¶",
        "severity": ErrorSeverity.CRITICAL
    }
}


@register(
    "astrbot_plugin_web_analyzer",
    "Sakura520222",
    "è‡ªåŠ¨è¯†åˆ«ç½‘é¡µé“¾æ¥å¹¶è¿›è¡Œå†…å®¹åˆ†æå’Œæ€»ç»“",
    "1.2.8",
    "https://github.com/Sakura520222/astrbot_plugin_web_analyzer",
)
class WebAnalyzerPlugin(Star):
    """ç½‘é¡µåˆ†ææ’ä»¶ä¸»ç±»

    è¿™æ˜¯æ’ä»¶çš„æ ¸å¿ƒåè°ƒç±»ï¼Œè´Ÿè´£ç®¡ç†å’Œè°ƒåº¦æ‰€æœ‰åŠŸèƒ½æ¨¡å—ï¼š
    - ğŸ”§ é…ç½®çš„åŠ è½½ã€éªŒè¯å’Œç®¡ç†
    - ğŸ“© æ¶ˆæ¯äº‹ä»¶çš„ç›‘å¬å’Œå¤„ç†
    - ğŸ”— URLçš„æå–ã€éªŒè¯å’Œè¿‡æ»¤
    - ğŸ•¸ï¸  ç½‘é¡µå†…å®¹çš„æŠ“å–ã€è§£æå’Œåˆ†æ
    - ğŸ§  å¤§è¯­è¨€æ¨¡å‹(LLM)çš„è°ƒç”¨å’Œç»“æœç”Ÿæˆ
    - ğŸ’¾ åˆ†æç»“æœçš„ç¼“å­˜ç®¡ç†
    - âš™ï¸  å„ç§ç®¡ç†å‘½ä»¤çš„å¤„ç†å’Œå“åº”

    æ’ä»¶æ”¯æŒä¸¤ç§åˆ†ææ¨¡å¼ï¼š
    - è‡ªåŠ¨æ¨¡å¼ï¼šè‡ªåŠ¨è¯†åˆ«æ¶ˆæ¯ä¸­çš„ç½‘é¡µé“¾æ¥å¹¶åˆ†æ
    - æ‰‹åŠ¨æ¨¡å¼ï¼šé€šè¿‡å‘½ä»¤è§¦å‘ç½‘é¡µåˆ†æ

    æä¾›äº†ä¸°å¯Œçš„é…ç½®é€‰é¡¹ï¼Œå¯æ ¹æ®éœ€æ±‚çµæ´»è°ƒæ•´æ’ä»¶è¡Œä¸ºã€‚
    """

    def __init__(self, context: Context, config: AstrBotConfig):
        """æ’ä»¶åˆå§‹åŒ–æ–¹æ³•

        è´Ÿè´£åŠ è½½ã€éªŒè¯å’Œåˆå§‹åŒ–æ‰€æœ‰é…ç½®é¡¹ï¼Œæ„å»ºæ’ä»¶çš„è¿è¡Œç¯å¢ƒï¼š

        ğŸ› ï¸ åŸºæœ¬é…ç½®ï¼š
        - è¯·æ±‚è¶…æ—¶æ—¶é—´å’Œé‡è¯•æœºåˆ¶
        - ç”¨æˆ·ä»£ç†å’Œä»£ç†è®¾ç½®
        - è‡ªåŠ¨åˆ†æå¼€å…³

        ğŸš« åŸŸåæ§åˆ¶ï¼š
        - å…è®¸è®¿é—®çš„åŸŸååˆ—è¡¨
        - ç¦æ­¢è®¿é—®çš„åŸŸååˆ—è¡¨

        ğŸ“Š åˆ†æè®¾ç½®ï¼š
        - æ˜¯å¦ä½¿ç”¨emojiå¢å¼ºæ˜¾ç¤º
        - æ˜¯å¦æ˜¾ç¤ºå†…å®¹ç»Ÿè®¡ä¿¡æ¯
        - æœ€å¤§æ‘˜è¦é•¿åº¦é™åˆ¶

        ğŸ“¸ æˆªå›¾é…ç½®ï¼š
        - æˆªå›¾è´¨é‡å’Œåˆ†è¾¨ç‡
        - æ˜¯å¦æˆªå–æ•´é¡µ
        - æˆªå›¾æ ¼å¼ï¼ˆJPEG/PNGï¼‰

        ğŸ§  LLMé…ç½®ï¼š
        - å¤§è¯­è¨€æ¨¡å‹æä¾›å•†
        - è‡ªå®šä¹‰æç¤ºè¯

        ğŸ‘¥ ç¾¤èŠç®¡ç†ï¼š
        - ç¾¤èŠé»‘åå•è®¾ç½®

        ğŸŒ ç¿»è¯‘åŠŸèƒ½ï¼š
        - æ˜¯å¦å¯ç”¨è‡ªåŠ¨ç¿»è¯‘
        - ç›®æ ‡è¯­è¨€è®¾ç½®

        ğŸ’¾ ç¼“å­˜ç®¡ç†ï¼š
        - ç¼“å­˜è¿‡æœŸæ—¶é—´
        - æœ€å¤§ç¼“å­˜æ•°é‡

        ğŸ“‹ å†…å®¹æå–ï¼š
        - æå–å†…å®¹ç±»å‹è®¾ç½®

        æ‰€æœ‰é…ç½®é¡¹éƒ½ä¼šè¿›è¡Œåˆç†æ€§éªŒè¯ï¼Œè‡ªåŠ¨ä¿®æ­£æ— æ•ˆå€¼å¹¶è®¾ç½®å®‰å…¨é»˜è®¤å€¼ï¼Œ
        ç¡®ä¿æ’ä»¶åœ¨å„ç§é…ç½®ä¸‹éƒ½èƒ½ç¨³å®šè¿è¡Œã€‚
        """
        super().__init__(context)
        self.config = config
        
        # åˆå§‹åŒ–é…ç½®
        self._load_network_settings()
        self._load_domain_settings()
        self._load_analysis_settings()
        self._load_screenshot_settings()
        self._load_llm_settings()
        self._load_group_settings()
        self._load_translation_settings()
        self._load_cache_settings()
        self._load_content_extraction_settings()
        self._load_recall_settings()
        self._load_command_settings()
        self._load_resource_settings()
        
        # URLå¤„ç†æ ‡å¿—é›†åˆï¼šç”¨äºé¿å…é‡å¤å¤„ç†åŒä¸€URL
        self.processing_urls = set()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_cache_manager()
        self._init_web_analyzer()
        
        # æ’¤å›ä»»åŠ¡åˆ—è¡¨ï¼šç”¨äºç®¡ç†æ‰€æœ‰æ’¤å›ä»»åŠ¡
        self.recall_tasks = []

        # è®°å½•é…ç½®åˆå§‹åŒ–å®Œæˆ
        logger.info("æ’ä»¶é…ç½®åˆå§‹åŒ–å®Œæˆ")
    
    def _load_network_settings(self):
        """åŠ è½½å’ŒéªŒè¯ç½‘ç»œè®¾ç½®"""
        network_settings = self.config.get("network_settings", {})
        # æœ€å¤§å†…å®¹é•¿åº¦ï¼šé™åˆ¶æŠ“å–çš„ç½‘é¡µå†…å®¹å¤§å°ï¼Œé¿å…å†…å­˜å ç”¨è¿‡é«˜
        self.max_content_length = max(1000, network_settings.get("max_content_length", 10000))
        # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼šè®¾ç½®åˆç†çš„è¶…æ—¶èŒƒå›´ï¼Œé¿å…è¯·æ±‚è¿‡é•¿æ—¶é—´é˜»å¡
        self.timeout = max(5, min(300, network_settings.get("request_timeout", 30)))
        # é‡è¯•æ¬¡æ•°ï¼šè¯·æ±‚å¤±è´¥æ—¶çš„é‡è¯•æ¬¡æ•°
        self.retry_count = max(0, min(10, network_settings.get("retry_count", 3)))
        # é‡è¯•å»¶è¿Ÿï¼šæ¯æ¬¡é‡è¯•ä¹‹é—´çš„ç­‰å¾…æ—¶é—´
        self.retry_delay = max(0, min(10, network_settings.get("retry_delay", 2)))
        # ç”¨æˆ·ä»£ç†ï¼šç”¨äºæ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚ï¼Œé¿å…è¢«ç½‘ç«™å°ç¦
        self.user_agent = network_settings.get(
            "user_agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )
        # ä»£ç†è®¾ç½®ï¼šç”¨äºç½‘ç»œä»£ç†ï¼ŒåŠ é€Ÿæˆ–ç»•è¿‡ç½‘ç»œé™åˆ¶
        self.proxy = network_settings.get("proxy", "")
        # å¹¶å‘å¤„ç†è®¾ç½®
        self.max_concurrency = max(1, min(20, network_settings.get("max_concurrency", 5)))
        self.dynamic_concurrency = bool(network_settings.get("dynamic_concurrency", True))
        # ä¼˜å…ˆçº§è®¾ç½®
        self.enable_priority_scheduling = bool(network_settings.get("enable_priority_scheduling", False))

        # éªŒè¯ä»£ç†æ ¼å¼æ˜¯å¦æ­£ç¡®
        if self.proxy:
            try:
                from urllib.parse import urlparse

                parsed = urlparse(self.proxy)
                if not all([parsed.scheme, parsed.netloc]):
                    logger.warning(f"æ— æ•ˆçš„ä»£ç†æ ¼å¼: {self.proxy}ï¼Œå°†å¿½ç•¥ä»£ç†è®¾ç½®")
                    self.proxy = ""
            except Exception as e:
                logger.warning(f"è§£æä»£ç†å¤±è´¥: {self.proxy}ï¼Œå°†å¿½ç•¥ä»£ç†è®¾ç½®ï¼Œé”™è¯¯: {e}")
                self.proxy = ""
    
    def _load_domain_settings(self):
        """åŠ è½½å’ŒéªŒè¯åŸŸåè®¾ç½®"""
        domain_settings = self.config.get("domain_settings", {})
        # è§£æå…è®¸å’Œç¦æ­¢çš„åŸŸååˆ—è¡¨
        self.allowed_domains = self._parse_domain_list(
            domain_settings.get("allowed_domains", "")
        )
        self.blocked_domains = self._parse_domain_list(
            domain_settings.get("blocked_domains", "")
        )
    
    def _load_analysis_settings(self):
        """åŠ è½½å’ŒéªŒè¯åˆ†æè®¾ç½®"""
        analysis_settings = self.config.get("analysis_settings", {})
        # æ˜¯å¦è‡ªåŠ¨åˆ†ææ£€æµ‹åˆ°çš„é“¾æ¥
        self.auto_analyze = bool(analysis_settings.get("auto_analyze", True))
        # æ˜¯å¦åœ¨ç»“æœä¸­ä½¿ç”¨emoji
        self.enable_emoji = bool(analysis_settings.get("enable_emoji", True))
        # æ˜¯å¦æ˜¾ç¤ºå†…å®¹ç»Ÿè®¡ä¿¡æ¯
        self.enable_statistics = bool(analysis_settings.get("enable_statistics", True))
        # æœ€å¤§æ‘˜è¦é•¿åº¦ï¼šé™åˆ¶LLMç”Ÿæˆçš„æ‘˜è¦å¤§å°
        self.max_summary_length = max(
            500, min(10000, analysis_settings.get("max_summary_length", 2000))
        )

        # å‘é€å†…å®¹ç±»å‹è®¾ç½®
        self.send_content_type = analysis_settings.get("send_content_type", "both")
        # éªŒè¯å‘é€å†…å®¹ç±»å‹æ˜¯å¦æœ‰æ•ˆ
        if self.send_content_type not in ["both", "analysis_only", "screenshot_only"]:
            logger.warning(
                f"æ— æ•ˆçš„å‘é€å†…å®¹ç±»å‹: {self.send_content_type}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ both"
            )
            self.send_content_type = "both"
        
        # ç»“æœå±•ç¤ºè®¾ç½®
        self.result_template = analysis_settings.get("result_template", "default")
        # éªŒè¯ç»“æœæ¨¡æ¿æ˜¯å¦æœ‰æ•ˆ
        valid_templates = ["default", "detailed", "compact", "markdown", "simple"]
        if self.result_template not in valid_templates:
            logger.warning(
                f"æ— æ•ˆçš„ç»“æœæ¨¡æ¿: {self.result_template}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ default"
            )
            self.result_template = "default"
        
        # ç»“æœæŠ˜å è®¾ç½®
        self.enable_collapsible = bool(analysis_settings.get("enable_collapsible", False))
        self.collapse_threshold = max(500, min(5000, analysis_settings.get("collapse_threshold", 1500)))
    
    def _load_screenshot_settings(self):
        """åŠ è½½å’ŒéªŒè¯æˆªå›¾è®¾ç½®"""
        screenshot_settings = self.config.get("screenshot_settings", {})
        # æ˜¯å¦å¯ç”¨ç½‘é¡µæˆªå›¾
        self.enable_screenshot = bool(screenshot_settings.get("enable_screenshot", True))
        # æˆªå›¾è´¨é‡ï¼šæ§åˆ¶æˆªå›¾çš„æ¸…æ™°åº¦å’Œæ–‡ä»¶å¤§å°
        self.screenshot_quality = max(
            10, min(100, screenshot_settings.get("screenshot_quality", 80))
        )
        # æˆªå›¾å®½åº¦å’Œé«˜åº¦ï¼šæ§åˆ¶æˆªå›¾çš„åˆ†è¾¨ç‡
        self.screenshot_width = max(
            320, min(4096, screenshot_settings.get("screenshot_width", 1280))
        )
        self.screenshot_height = max(
            240, min(4096, screenshot_settings.get("screenshot_height", 720))
        )
        # æ˜¯å¦æˆªå–æ•´é¡µï¼šæ§åˆ¶æ˜¯å¦æˆªå–å®Œæ•´çš„ç½‘é¡µå†…å®¹
        self.screenshot_full_page = bool(
            screenshot_settings.get("screenshot_full_page", False)
        )
        # æˆªå›¾ç­‰å¾…æ—¶é—´ï¼šé¡µé¢åŠ è½½å®Œæˆåç­‰å¾…çš„æ—¶é—´ï¼Œç¡®ä¿å†…å®¹å®Œæ•´æ˜¾ç¤º
        self.screenshot_wait_time = max(
            0, min(10000, screenshot_settings.get("screenshot_wait_time", 2000))
        )

        # éªŒè¯æˆªå›¾æ ¼å¼æ˜¯å¦æ”¯æŒ
        screenshot_format = screenshot_settings.get("screenshot_format", "jpeg").lower()
        if screenshot_format not in ["jpeg", "png"]:
            logger.warning(f"æ— æ•ˆçš„æˆªå›¾æ ¼å¼: {screenshot_format}ï¼Œå°†ä½¿ç”¨é»˜è®¤æ ¼å¼ jpeg")
            self.screenshot_format = "jpeg"
        else:
            self.screenshot_format = screenshot_format
        
        # æˆªå›¾è£å‰ªè®¾ç½®
        self.enable_crop = bool(screenshot_settings.get("enable_crop", False))
        # è£å‰ªåŒºåŸŸï¼Œæ ¼å¼ä¸º [left, top, right, bottom]
        crop_area = screenshot_settings.get("crop_area", [0, 0, self.screenshot_width, self.screenshot_height])
        
        # å¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„è£å‰ªåŒºåŸŸ
        if isinstance(crop_area, str):
            try:
                # å°è¯•å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºåˆ—è¡¨
                crop_area = eval(crop_area)
            except Exception as e:
                logger.warning(f"è§£æè£å‰ªåŒºåŸŸå¤±è´¥: {crop_area}ï¼Œé”™è¯¯: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
                crop_area = [0, 0, self.screenshot_width, self.screenshot_height]
        
        # éªŒè¯è£å‰ªåŒºåŸŸæ˜¯å¦æœ‰æ•ˆ
        if not isinstance(crop_area, list) or len(crop_area) != 4:
            logger.warning(f"æ— æ•ˆçš„è£å‰ªåŒºåŸŸ: {crop_area}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
            crop_area = [0, 0, self.screenshot_width, self.screenshot_height]
        
        self.crop_area = crop_area
        

    
    def _load_llm_settings(self):
        """åŠ è½½å’ŒéªŒè¯LLMè®¾ç½®"""
        llm_settings = self.config.get("llm_settings", {})
        # æ˜¯å¦å¯ç”¨LLMæ™ºèƒ½åˆ†æ
        self.llm_enabled = bool(llm_settings.get("llm_enabled", True))
        # LLMæä¾›å•†é…ç½®ï¼šæŒ‡å®šä½¿ç”¨çš„å¤§è¯­è¨€æ¨¡å‹æä¾›å•†
        self.llm_provider = llm_settings.get("llm_provider", "")
        # è‡ªå®šä¹‰æç¤ºè¯é…ç½®ï¼šå…è®¸ç”¨æˆ·è‡ªå®šä¹‰LLMåˆ†æçš„æç¤ºè¯
        self.custom_prompt = llm_settings.get("custom_prompt", "")
    
    def _load_group_settings(self):
        """åŠ è½½å’ŒéªŒè¯ç¾¤èŠè®¾ç½®"""
        group_settings = self.config.get("group_settings", {})
        # ç¾¤èŠé»‘åå•é…ç½®ï¼šç”¨äºæ§åˆ¶å“ªäº›ç¾¤èŠä¸å…è®¸ä½¿ç”¨æ’ä»¶
        group_blacklist_text = group_settings.get("group_blacklist", "")
        self.group_blacklist = self._parse_group_list(group_blacklist_text)
        
        # åˆå¹¶è½¬å‘é…ç½®ï¼šæ§åˆ¶æ˜¯å¦ä½¿ç”¨åˆå¹¶è½¬å‘åŠŸèƒ½å‘é€åˆ†æç»“æœ
        merge_forward_config = self.config.get("merge_forward_settings", {})
        self.merge_forward_enabled = {
            "group": bool(merge_forward_config.get("group", False)),
            "private": bool(merge_forward_config.get("private", False)),
            "include_screenshot": bool(
                merge_forward_config.get("include_screenshot", False)
            ),
        }
    
    def _load_translation_settings(self):
        """åŠ è½½å’ŒéªŒè¯ç¿»è¯‘è®¾ç½®"""
        translation_settings = self.config.get("translation_settings", {})
        self.enable_translation = bool(
            translation_settings.get("enable_translation", False)
        )

        # éªŒè¯ç›®æ ‡è¯­è¨€æ˜¯å¦æ”¯æŒ
        self.target_language = translation_settings.get("target_language", "zh").lower()
        valid_languages = ["zh", "en", "ja", "ko", "fr", "de", "es", "ru", "ar", "pt"]
        if self.target_language not in valid_languages:
            logger.warning(f"æ— æ•ˆçš„ç›®æ ‡è¯­è¨€: {self.target_language}ï¼Œå°†ä½¿ç”¨é»˜è®¤è¯­è¨€ zh")
            self.target_language = "zh"

        # ç¿»è¯‘æä¾›å•†é…ç½®
        self.translation_provider = translation_settings.get(
            "translation_provider", "llm"
        )
        # è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯ï¼šå…è®¸ç”¨æˆ·è‡ªå®šä¹‰ç¿»è¯‘çš„æç¤ºè¯
        self.custom_translation_prompt = translation_settings.get(
            "custom_translation_prompt", ""
        )
    
    def _load_cache_settings(self):
        """åŠ è½½å’ŒéªŒè¯ç¼“å­˜è®¾ç½®"""
        cache_settings = self.config.get("cache_settings", {})
        self.enable_cache = bool(cache_settings.get("enable_cache", True))
        # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼šæ§åˆ¶ç¼“å­˜ç»“æœçš„æœ‰æ•ˆæœŸ
        self.cache_expire_time = max(
            5, min(10080, cache_settings.get("cache_expire_time", 1440))
        )
        # æœ€å¤§ç¼“å­˜æ•°é‡ï¼šæ§åˆ¶ç¼“å­˜çš„æœ€å¤§æ¡ç›®æ•°ï¼Œé¿å…å†…å­˜å ç”¨è¿‡é«˜
        self.max_cache_size = max(
            10, min(1000, cache_settings.get("max_cache_size", 100))
        )
        # ç¼“å­˜é¢„åŠ è½½è®¾ç½®
        self.cache_preload_enabled = bool(cache_settings.get("cache_preload_enabled", False))
        self.cache_preload_count = max(
            0, min(100, cache_settings.get("cache_preload_count", 20))
        )
    
    def _load_content_extraction_settings(self):
        """åŠ è½½å’ŒéªŒè¯å†…å®¹æå–è®¾ç½®"""
        content_extraction_settings = self.config.get("content_extraction_settings", {})
        self.enable_specific_extraction = bool(
            content_extraction_settings.get("enable_specific_extraction", False)
        )
        # æå–ç±»å‹ï¼šæŒ‡å®šè¦æå–çš„å†…å®¹ç±»å‹
        extract_types_text = content_extraction_settings.get(
            "extract_types", "title\ncontent"
        )
        
        # ä½¿ç”¨è¾…åŠ©æ–¹æ³•å¤„ç†æå–ç±»å‹
        self.extract_types = WebAnalyzerUtils.parse_extract_types(extract_types_text)
        self.extract_types = WebAnalyzerUtils.validate_extract_types(self.extract_types)
        self.extract_types = WebAnalyzerUtils.ensure_minimal_extract_types(self.extract_types)
        self.extract_types = WebAnalyzerUtils.add_required_extract_types(self.extract_types)
    
    def _load_recall_settings(self):
        """åŠ è½½å’ŒéªŒè¯æ’¤å›è®¾ç½®"""
        recall_settings = self.config.get("recall_settings", {})
        # æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ’¤å›åŠŸèƒ½
        self.enable_recall = bool(recall_settings.get("enable_recall", True))
        # æ’¤å›å»¶è¿Ÿæ—¶é—´ï¼šè®¾ç½®åˆç†çš„èŒƒå›´ï¼Œé¿å…è¿‡çŸ­æˆ–è¿‡é•¿
        self.recall_time = max(0, min(120, recall_settings.get("recall_time", 10)))
    
    def _load_command_settings(self):
        """åŠ è½½å’ŒéªŒè¯å‘½ä»¤è®¾ç½®"""
        command_settings = self.config.get("command_settings", {})
        # è‡ªå®šä¹‰å‘½ä»¤åˆ«åé…ç½®
        custom_aliases = command_settings.get("custom_aliases", {})
        
        # å¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„è‡ªå®šä¹‰åˆ«å
        if isinstance(custom_aliases, str):
            try:
                # è§£æè‡ªå®šä¹‰åˆ«åï¼Œæ ¼å¼ä¸ºï¼šåŸå‘½ä»¤=åˆ«å1,åˆ«å2
                parsed_aliases = {}
                lines = custom_aliases.strip().split('\n')
                for line in lines:
                    line = line.strip()
                    if not line or '=' not in line:
                        continue
                    command, aliases = line.split('=', 1)
                    command = command.strip()
                    alias_list = [alias.strip() for alias in aliases.split(',') if alias.strip()]
                    if command and alias_list:
                        parsed_aliases[command] = alias_list
                self.custom_command_aliases = parsed_aliases
            except Exception as e:
                logger.warning(f"è§£æè‡ªå®šä¹‰å‘½ä»¤åˆ«åå¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
                self.custom_command_aliases = {}
        else:
            self.custom_command_aliases = custom_aliases
        
        # å‘½ä»¤è¡¥å…¨è®¾ç½®
        self.enable_command_completion = bool(command_settings.get("enable_completion", True))
        # å‘½ä»¤å¸®åŠ©è®¾ç½®
        self.enable_command_help = bool(command_settings.get("enable_help", True))
        # å‘½ä»¤å‚æ•°æç¤ºè®¾ç½®
        self.enable_param_hints = bool(command_settings.get("enable_param_hints", True))
    
    def _load_resource_settings(self):
        """åŠ è½½å’ŒéªŒè¯èµ„æºç®¡ç†è®¾ç½®"""
        resource_settings = self.config.get("resource_settings", {})
        # å†…å­˜ç›‘æ§è®¾ç½®
        self.enable_memory_monitor = bool(resource_settings.get("enable_memory_monitor", True))
        # å†…å­˜ä½¿ç”¨é˜ˆå€¼ï¼Œç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
        self.memory_threshold = max(0.0, min(100.0, resource_settings.get("memory_threshold", 80.0)))
    
    def _init_cache_manager(self):
        """åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨"""
        self.cache_manager = CacheManager(
            max_size=self.max_cache_size, 
            expire_time=self.cache_expire_time,
            preload_enabled=self.cache_preload_enabled,
            preload_count=self.cache_preload_count
        )
    
    def _init_web_analyzer(self):
        """åˆå§‹åŒ–ç½‘é¡µåˆ†æå™¨"""
        self.analyzer = WebAnalyzer(
            max_content_length=self.max_content_length,
            timeout=self.timeout,
            user_agent=self.user_agent,
            proxy=self.proxy,
            retry_count=self.retry_count,
            retry_delay=self.retry_delay,
            enable_memory_monitor=self.enable_memory_monitor,  # å¯ç”¨å†…å­˜ç›‘æ§
            memory_threshold=self.memory_threshold,  # å†…å­˜ä½¿ç”¨é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤é˜ˆå€¼æ—¶è‡ªåŠ¨é‡Šæ”¾å†…å­˜
        )

    def _parse_domain_list(self, domain_text: str) -> List[str]:
        """å°†å¤šè¡ŒåŸŸåæ–‡æœ¬è½¬æ¢ä¸ºPythonåˆ—è¡¨

        å¤„ç†é…ç½®ä¸­å®šä¹‰çš„åŸŸååˆ—è¡¨ï¼Œæ”¯æŒï¼š
        - æ¯è¡Œä¸€ä¸ªåŸŸåçš„æ ¼å¼
        - è‡ªåŠ¨å»é™¤ç©ºè¡Œå’Œå‰åç©ºç™½å­—ç¬¦
        - æ”¯æŒåŸŸåé€šé…ç¬¦ï¼ˆå¦‚*.example.comï¼‰

        Args:
            domain_text: åŒ…å«åŸŸåçš„å¤šè¡Œæ–‡æœ¬å­—ç¬¦ä¸²

        Returns:
            è§£æåçš„åŸŸååˆ—è¡¨ï¼Œå·²æ¸…ç†æ— æ•ˆå†…å®¹
        """
        return WebAnalyzerUtils.parse_domain_list(domain_text)

    def _parse_group_list(self, group_text: str) -> List[str]:
        """å°†å¤šè¡Œç¾¤èŠIDæ–‡æœ¬è½¬æ¢ä¸ºPythonåˆ—è¡¨

        å¤„ç†é…ç½®ä¸­å®šä¹‰çš„ç¾¤èŠé»‘åå•ï¼Œæ”¯æŒï¼š
        - æ¯è¡Œä¸€ä¸ªç¾¤èŠIDçš„æ ¼å¼
        - è‡ªåŠ¨å»é™¤ç©ºè¡Œå’Œå‰åç©ºç™½å­—ç¬¦
        - æ”¯æŒæ•°å­—å’Œå­—ç¬¦ä¸²ç±»å‹çš„ç¾¤èŠID

        Args:
            group_text: åŒ…å«ç¾¤èŠIDçš„å¤šè¡Œæ–‡æœ¬å­—ç¬¦ä¸²

        Returns:
            è§£æåçš„ç¾¤èŠIDåˆ—è¡¨ï¼Œå·²æ¸…ç†æ— æ•ˆå†…å®¹
        """
        return WebAnalyzerUtils.parse_group_list(group_text)

    def _is_group_blacklisted(self, group_id: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šç¾¤èŠæ˜¯å¦åœ¨é»‘åå•ä¸­

        ç¾¤èŠé»‘åå•åŠŸèƒ½å¯ä»¥æ§åˆ¶å“ªäº›ç¾¤èŠä¸èƒ½ä½¿ç”¨æ’ä»¶ï¼Œ
        é€‚ç”¨äºéœ€è¦é™åˆ¶æ’ä»¶ä½¿ç”¨èŒƒå›´çš„åœºæ™¯ã€‚

        Args:
            group_id: ç¾¤èŠIDï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æ•°å­—

        Returns:
            Trueè¡¨ç¤ºåœ¨é»‘åå•ä¸­ï¼ˆç¦æ­¢ä½¿ç”¨ï¼‰ï¼ŒFalseè¡¨ç¤ºä¸åœ¨é»‘åå•ä¸­ï¼ˆå…è®¸ä½¿ç”¨ï¼‰
        """
        if not group_id or not self.group_blacklist:
            return False
        return group_id in self.group_blacklist

    def _is_domain_allowed(self, url: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šURLçš„åŸŸåæ˜¯å¦å…è®¸è®¿é—®

        æ ¹æ®é…ç½®çš„å…è®¸å’Œç¦æ­¢åŸŸååˆ—è¡¨ï¼Œåˆ¤æ–­URLæ˜¯å¦å¯ä»¥è®¿é—®ï¼Œ
        æ”¯æŒçµæ´»çš„è®¿é—®æ§åˆ¶ç­–ç•¥ï¼š

        è®¿é—®è§„åˆ™ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š
        1. å¦‚æœåŸŸååœ¨ç¦æ­¢åˆ—è¡¨ä¸­ï¼Œç›´æ¥æ‹’ç»è®¿é—®
        2. å¦‚æœå…è®¸åˆ—è¡¨ä¸ä¸ºç©ºï¼Œåªæœ‰åœ¨åˆ—è¡¨ä¸­çš„åŸŸåæ‰å…è®¸è®¿é—®
        3. å¦‚æœå…è®¸åˆ—è¡¨ä¸ºç©ºï¼Œåˆ™å…è®¸æ‰€æœ‰æœªè¢«ç¦æ­¢çš„åŸŸå

        Args:
            url: è¦æ£€æŸ¥çš„å®Œæ•´URL

        Returns:
            Trueè¡¨ç¤ºå…è®¸è®¿é—®ï¼ŒFalseè¡¨ç¤ºç¦æ­¢è®¿é—®
        """
        return WebAnalyzerUtils.is_domain_allowed(url, self.allowed_domains, self.blocked_domains)

    @filter.command("ç½‘é¡µåˆ†æ", alias={"åˆ†æ", "æ€»ç»“", "web", "analyze"})
    async def analyze_webpage(self, event: AstrMessageEvent):
        """æ‰‹åŠ¨è§¦å‘ç½‘é¡µåˆ†æå‘½ä»¤

        è¿™æ˜¯æ’ä»¶çš„æ ¸å¿ƒå‘½ä»¤ï¼Œå…è®¸ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®šè¦åˆ†æçš„ç½‘é¡µé“¾æ¥ï¼Œ
        æ”¯æŒå¤šç§å‘½ä»¤åˆ«åï¼Œæ–¹ä¾¿ä¸åŒä½¿ç”¨ä¹ æƒ¯çš„ç”¨æˆ·ã€‚

        ğŸ“‹ ç”¨æ³•ç¤ºä¾‹ï¼š
        - `/ç½‘é¡µåˆ†æ https://example.com` - åˆ†æå•ä¸ªé“¾æ¥
        - `/åˆ†æ https://example.com https://test.com` - åˆ†æå¤šä¸ªé“¾æ¥
        - `/æ€»ç»“ https://example.com` - ä½¿ç”¨åˆ«åå‘½ä»¤

        ğŸ”§ åŠŸèƒ½ç‰¹æ€§ï¼š
        - æ”¯æŒåŒæ—¶åˆ†æå¤šä¸ªç½‘é¡µé“¾æ¥
        - è‡ªåŠ¨éªŒè¯URLæ ¼å¼æ­£ç¡®æ€§
        - æ ¹æ®åŸŸåé»‘ç™½åå•è¿‡æ»¤é“¾æ¥
        - å¼‚æ­¥å¤„ç†ï¼Œä¸é˜»å¡å…¶ä»–æ“ä½œ
        - æ”¯æŒå„ç§è¾“å‡ºæ ¼å¼

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼ŒåŒ…å«æ¶ˆæ¯å†…å®¹å’Œä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        message_text = event.message_str

        # ä»æ¶ˆæ¯ä¸­æå–æ‰€æœ‰URL
        urls = self.analyzer.extract_urls(message_text)
        if not urls:
            yield event.plain_result(
                "è¯·æä¾›è¦åˆ†æçš„ç½‘é¡µé“¾æ¥ï¼Œä¾‹å¦‚ï¼š/ç½‘é¡µåˆ†æ https://example.com"
            )
            return

        # éªŒè¯URLæ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œå¹¶è§„èŒƒåŒ–URL
        valid_urls = [self.analyzer.normalize_url(url) for url in urls if self.analyzer.is_valid_url(url)]
        # å»é‡ï¼Œé¿å…é‡å¤åˆ†æç›¸åŒURL
        valid_urls = list(set(valid_urls))
        if not valid_urls:
            yield event.plain_result("æ— æ•ˆçš„URLé“¾æ¥ï¼Œè¯·æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®")
            return

        # è¿‡æ»¤æ‰ä¸å…è®¸è®¿é—®çš„åŸŸå
        allowed_urls = [url for url in valid_urls if self._is_domain_allowed(url)]
        if not allowed_urls:
            yield event.plain_result("æ‰€æœ‰åŸŸåéƒ½ä¸åœ¨å…è®¸è®¿é—®çš„åˆ—è¡¨ä¸­ï¼Œæˆ–å·²è¢«ç¦æ­¢è®¿é—®")
            return

        # å‘é€å¤„ç†æç¤ºæ¶ˆæ¯ï¼Œå‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨åˆ†æ
        if len(allowed_urls) == 1:
            message = f"æ­£åœ¨åˆ†æç½‘é¡µ: {allowed_urls[0]}"
        else:
            message = f"æ­£åœ¨åˆ†æ{len(allowed_urls)}ä¸ªç½‘é¡µé“¾æ¥..."
        
        # ç›´æ¥è°ƒç”¨å‘é€æ–¹æ³•ï¼Œä¸ä½¿ç”¨yield
        await self._send_processing_message(event, message)

        # æ‰¹é‡å¤„ç†æ‰€æœ‰å…è®¸è®¿é—®çš„URL
        async for result in self._batch_process_urls(event, allowed_urls):
            yield result

    @filter.event_message_type(filter.EventMessageType.ALL)
    async def auto_detect_urls(self, event: AstrMessageEvent):
        """è‡ªåŠ¨æ£€æµ‹æ¶ˆæ¯ä¸­çš„URLé“¾æ¥å¹¶è¿›è¡Œåˆ†æ

        è¿™ä¸ªæ–¹æ³•å®ç°äº†æ’ä»¶çš„è‡ªåŠ¨åˆ†æåŠŸèƒ½ï¼Œæ— éœ€ç”¨æˆ·æ‰‹åŠ¨è°ƒç”¨å‘½ä»¤ï¼Œ
        åªè¦å‘é€åŒ…å«ç½‘é¡µé“¾æ¥çš„æ¶ˆæ¯ï¼Œæ’ä»¶å°±ä¼šè‡ªåŠ¨è¿›è¡Œåˆ†æã€‚

        ğŸš¦ è‡ªåŠ¨åˆ†æè§„åˆ™ï¼š
        1. ä»…å½“é…ç½®ä¸­auto_analyzeä¸ºTrueæ—¶å¯ç”¨
        2. æ™ºèƒ½è·³è¿‡å‘½ä»¤æ¶ˆæ¯ï¼Œé¿å…é‡å¤å¤„ç†
        3. è·³è¿‡åŒ…å«ç½‘é¡µåˆ†æç›¸å…³æŒ‡ä»¤çš„æ¶ˆæ¯
        4. è·³è¿‡åœ¨é»‘åå•ä¸­çš„ç¾¤èŠæ¶ˆæ¯
        5. ä»…å¤„ç†æ ¼å¼æ­£ç¡®çš„URL
        6. éµå®ˆåŸŸåé»‘ç™½åå•é™åˆ¶

        âœ¨ ä¼˜åŠ¿ï¼š
        - æå‡ç”¨æˆ·ä½“éªŒï¼Œæ— éœ€è®°å¿†å‘½ä»¤
        - æ”¯æŒæ‰€æœ‰æ¶ˆæ¯ç±»å‹ï¼ˆç§èŠã€ç¾¤èŠï¼‰
        - æ™ºèƒ½è¿‡æ»¤ï¼Œé¿å…è¯¯è§¦å‘
        - ä¸æ‰‹åŠ¨åˆ†æä½¿ç”¨ç›¸åŒçš„æ ¸å¿ƒé€»è¾‘

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼ŒåŒ…å«æ¶ˆæ¯å†…å®¹å’Œä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨åˆ†æåŠŸèƒ½
        if not self.auto_analyze:
            return

        # æ£€æŸ¥æ˜¯å¦ä¸ºæŒ‡ä»¤è°ƒç”¨ï¼Œé¿å…é‡å¤å¤„ç†
        message_text = event.message_str.strip()

        # æ–¹æ³•1ï¼šè·³è¿‡ä»¥/å¼€å¤´çš„æŒ‡ä»¤æ¶ˆæ¯
        if message_text.startswith("/"):
            logger.info("æ£€æµ‹åˆ°æŒ‡ä»¤è°ƒç”¨ï¼Œè·³è¿‡è‡ªåŠ¨åˆ†æ")
            return

        # æ–¹æ³•2ï¼šæ£€æŸ¥äº‹ä»¶æ˜¯å¦æœ‰commandå±æ€§ï¼ˆæŒ‡ä»¤è°ƒç”¨æ—¶ä¼šæœ‰ï¼‰
        if hasattr(event, "command"):
            logger.info("æ£€æµ‹åˆ°commandå±æ€§ï¼Œè·³è¿‡è‡ªåŠ¨åˆ†æ")
            return

        # æ–¹æ³•3ï¼šæ£€æŸ¥åŸå§‹æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«ç½‘é¡µåˆ†æç›¸å…³æŒ‡ä»¤å…³é”®å­—
        raw_message = None
        if hasattr(event, "raw_message"):
            raw_message = str(event.raw_message)
        elif hasattr(event, "message_obj"):
            raw_message = str(event.message_obj)

        if raw_message:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç½‘é¡µåˆ†æç›¸å…³æŒ‡ä»¤
            command_keywords = ["ç½‘é¡µåˆ†æ", "/åˆ†æ", "/æ€»ç»“", "/web", "/analyze"]
            for keyword in command_keywords:
                if keyword in raw_message:
                    logger.info(f"æ£€æµ‹åˆ°æŒ‡ä»¤å…³é”®å­— {keyword}ï¼Œè·³è¿‡è‡ªåŠ¨åˆ†æ")
                    return

        # æ£€æŸ¥ç¾¤èŠæ˜¯å¦åœ¨é»‘åå•ä¸­ï¼ˆä»…ç¾¤èŠæ¶ˆæ¯ï¼‰
        group_id = None

        # æ–¹æ³•1ï¼šä»äº‹ä»¶å¯¹è±¡ç›´æ¥è·å–ç¾¤èŠID
        if hasattr(event, "group_id") and event.group_id:
            group_id = event.group_id
        # æ–¹æ³•2ï¼šä»æ¶ˆæ¯å¯¹è±¡è·å–ç¾¤èŠID
        elif (
            hasattr(event, "message_obj")
            and hasattr(event.message_obj, "group_id")
            and event.message_obj.group_id
        ):
            group_id = event.message_obj.group_id
        # æ–¹æ³•3ï¼šä»åŸå§‹æ¶ˆæ¯è·å–ç¾¤èŠID
        elif (
            hasattr(event, "raw_message")
            and hasattr(event.raw_message, "group_id")
            and event.raw_message.group_id
        ):
            group_id = event.raw_message.group_id

        # ç¾¤èŠåœ¨é»‘åå•ä¸­æ—¶é™é»˜å¿½ç•¥ï¼Œä¸è¿›è¡Œä»»ä½•å¤„ç†
        if group_id and self._is_group_blacklisted(group_id):
            return

        # ä»æ¶ˆæ¯ä¸­æå–æ‰€æœ‰URL
        urls = self.analyzer.extract_urls(message_text)
        if not urls:
            return  # æ²¡æœ‰URLï¼Œä¸å¤„ç†

        # éªŒè¯URLæ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œå¹¶è§„èŒƒåŒ–URL
        valid_urls = [self.analyzer.normalize_url(url) for url in urls if self.analyzer.is_valid_url(url)]
        # å»é‡ï¼Œé¿å…é‡å¤åˆ†æç›¸åŒURL
        valid_urls = list(set(valid_urls))
        if not valid_urls:
            return  # æ²¡æœ‰æœ‰æ•ˆURLï¼Œä¸å¤„ç†

        # è¿‡æ»¤æ‰ä¸å…è®¸è®¿é—®çš„åŸŸå
        allowed_urls = [url for url in valid_urls if self._is_domain_allowed(url)]
        if not allowed_urls:
            return  # æ²¡æœ‰å…è®¸è®¿é—®çš„URLï¼Œä¸å¤„ç†

        # å‘é€å¤„ç†æç¤ºæ¶ˆæ¯ï¼Œå‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨åˆ†æ
        if len(allowed_urls) == 1:
            message = f"æ£€æµ‹åˆ°ç½‘é¡µé“¾æ¥ï¼Œæ­£åœ¨åˆ†æ: {allowed_urls[0]}"
        else:
            message = f"æ£€æµ‹åˆ°{len(allowed_urls)}ä¸ªç½‘é¡µé“¾æ¥ï¼Œæ­£åœ¨åˆ†æ..."
        
        # ç›´æ¥è°ƒç”¨å‘é€æ–¹æ³•ï¼Œä¸ä½¿ç”¨yield
        await self._send_processing_message(event, message)

        # æ‰¹é‡å¤„ç†æ‰€æœ‰å…è®¸è®¿é—®çš„URL
        async for result in self._batch_process_urls(event, allowed_urls):
            yield result

    async def _process_single_url(
        self, event: AstrMessageEvent, url: str, analyzer: WebAnalyzer
    ) -> dict:
        """å¤„ç†å•ä¸ªç½‘é¡µURLï¼Œç”Ÿæˆå®Œæ•´çš„åˆ†æç»“æœ

        è¿™æ˜¯å¤„ç†å•ä¸ªç½‘é¡µé“¾æ¥çš„æ ¸å¿ƒæ–¹æ³•ï¼ŒåŒ…å«å®Œæ•´çš„åˆ†ææµç¨‹ï¼š

        ğŸ”„ å¤„ç†æµç¨‹ï¼š
        1. ğŸ” æ£€æŸ¥ç¼“å­˜ï¼Œé¿å…é‡å¤åˆ†æ
        2. ğŸŒ æŠ“å–ç½‘é¡µHTMLå†…å®¹
        3. ğŸ“ æå–ç»“æ„åŒ–çš„ç½‘é¡µå†…å®¹
        4. ğŸŒ ç¿»è¯‘å†…å®¹ï¼ˆå¦‚æœå¯ç”¨äº†ç¿»è¯‘åŠŸèƒ½ï¼‰
        5. ğŸ§  è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)è¿›è¡Œæ™ºèƒ½åˆ†æ
        6. ğŸ“Š æå–ç‰¹å®šç±»å‹å†…å®¹ï¼ˆå›¾ç‰‡ã€é“¾æ¥ã€è¡¨æ ¼ç­‰ï¼‰
        7. ğŸ“¸ æ•è·ç½‘é¡µæˆªå›¾ï¼ˆå¦‚æœå¯ç”¨äº†æˆªå›¾åŠŸèƒ½ï¼‰
        8. ğŸ’¾ æ›´æ–°ç¼“å­˜ï¼Œä¿å­˜åˆ†æç»“æœ
        9. ğŸ“¤ è¿”å›å®Œæ•´çš„åˆ†æç»“æœ

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯
            url: è¦åˆ†æçš„ç½‘é¡µURL
            analyzer: WebAnalyzerå®ä¾‹ï¼Œç”¨äºç½‘é¡µæŠ“å–å’Œåˆ†æ

        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸ï¼š
            {
                'url': åˆ†æçš„URLåœ°å€,
                'result': æ ¼å¼åŒ–çš„åˆ†æç»“æœæ–‡æœ¬,
                'screenshot': ç½‘é¡µæˆªå›¾äºŒè¿›åˆ¶æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            }
        """
        try:
            # æ£€æŸ¥URLç¼“å­˜ï¼Œé¿å…é‡å¤åˆ†æ
            cached_result = self._check_cache(url)
            if cached_result:
                logger.info(f"ä½¿ç”¨URLç¼“å­˜ç»“æœ: {url}")
                return cached_result

            # æŠ“å–ç½‘é¡µHTMLå†…å®¹
            html = await analyzer.fetch_webpage(url)
            if not html:
                error_msg = self._handle_error(ErrorType.NETWORK_ERROR, Exception("æ— æ³•è·å–ç½‘é¡µå†…å®¹"), url)
                return {
                    "url": url,
                    "result": error_msg,
                    "screenshot": None,
                }

            # ä»HTMLä¸­æå–ç»“æ„åŒ–å†…å®¹
            content_data = analyzer.extract_content(html, url)
            if not content_data:
                error_msg = self._handle_error(ErrorType.PARSING_ERROR, Exception("æ— æ³•è§£æç½‘é¡µå†…å®¹"), url)
                return {
                    "url": url,
                    "result": error_msg,
                    "screenshot": None,
                }
            
            # æ£€æŸ¥åŸºäºå†…å®¹å“ˆå¸Œçš„ç¼“å­˜
            if self.enable_cache:
                content_hash_cache = self.cache_manager.get_by_content_hash(content_data["content"])
                if content_hash_cache:
                    logger.info(f"ä½¿ç”¨å†…å®¹å“ˆå¸Œç¼“å­˜ç»“æœ: {url}")
                    return content_hash_cache

            # å¦‚æœå¯ç”¨äº†ç¿»è¯‘åŠŸèƒ½ï¼Œå…ˆç¿»è¯‘å†…å®¹
            if self.enable_translation:
                try:
                    translated_content = await self._translate_content(
                        event, content_data["content"]
                    )
                    # åˆ›å»ºç¿»è¯‘åçš„å†…å®¹æ•°æ®å‰¯æœ¬
                    translated_content_data = content_data.copy()
                    translated_content_data["content"] = translated_content
                    # è°ƒç”¨LLMè¿›è¡Œåˆ†æï¼ˆä½¿ç”¨ç¿»è¯‘åçš„å†…å®¹ï¼‰
                    analysis_result = await self.analyze_with_llm(
                        event, translated_content_data
                    )
                except Exception as e:
                    # ç¿»è¯‘å¤±è´¥æ—¶ï¼Œä½¿ç”¨åŸå§‹å†…å®¹è¿›è¡Œåˆ†æ
                    logger.warning(f"ç¿»è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹è¿›è¡Œåˆ†æ: {url}, é”™è¯¯: {e}")
                    analysis_result = await self.analyze_with_llm(event, content_data)
            else:
                # ç›´æ¥è°ƒç”¨LLMè¿›è¡Œåˆ†æ
                analysis_result = await self.analyze_with_llm(event, content_data)

            # å¦‚æœå¯ç”¨äº†ç‰¹å®šå†…å®¹æå–ï¼Œæå–é¢å¤–ä¿¡æ¯
            try:
                specific_content = self._extract_specific_content(html, url)
                if specific_content:
                    # åœ¨åˆ†æç»“æœä¸­æ·»åŠ ç‰¹å®šå†…å®¹
                    specific_content_str = "\n\n**ç‰¹å®šå†…å®¹æå–**\n"

                    # æ·»åŠ å›¾ç‰‡é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
                    if "images" in specific_content and specific_content["images"]:
                        specific_content_str += (
                            f"\nğŸ“· å›¾ç‰‡é“¾æ¥ ({len(specific_content['images'])}):\n"
                        )
                        for img_url in specific_content["images"]:
                            specific_content_str += f"- {img_url}\n"

                    # æ·»åŠ ç›¸å…³é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼Œæœ€å¤šæ˜¾ç¤º5ä¸ªï¼‰
                    if "links" in specific_content and specific_content["links"]:
                        specific_content_str += (
                            f"\nğŸ”— ç›¸å…³é“¾æ¥ ({len(specific_content['links'])}):\n"
                        )
                        for link in specific_content["links"][:5]:
                            specific_content_str += f"- [{link['text']}]({link['url']})\n"

                    # æ·»åŠ ä»£ç å—ï¼ˆå¦‚æœæœ‰ï¼Œæœ€å¤šæ˜¾ç¤º2ä¸ªï¼‰
                    if (
                        "code_blocks" in specific_content
                        and specific_content["code_blocks"]
                    ):
                        specific_content_str += (
                            f"\nğŸ’» ä»£ç å— ({len(specific_content['code_blocks'])}):\n"
                        )
                        for i, code in enumerate(specific_content["code_blocks"][:2]):
                            specific_content_str += f"```\n{code}\n```\n"

                    # æ·»åŠ å…ƒä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                    if "meta" in specific_content and specific_content["meta"]:
                        meta_info = specific_content["meta"]
                        specific_content_str += "\nğŸ“‹ å…ƒä¿¡æ¯:\n"
                        for key, value in meta_info.items():
                            if value:
                                specific_content_str += f"- {key}: {value}\n"

                    # å°†ç‰¹å®šå†…å®¹æ·»åŠ åˆ°åˆ†æç»“æœä¸­
                    analysis_result += specific_content_str
            except Exception as e:
                # ç‰¹å®šå†…å®¹æå–å¤±è´¥æ—¶ï¼Œè®°å½•è­¦å‘Šä½†ä¸å½±å“ä¸»åˆ†æç»“æœ
                logger.warning(f"ç‰¹å®šå†…å®¹æå–å¤±è´¥: {url}, é”™è¯¯: {e}")

            # æ ¹æ®å‘é€å†…å®¹ç±»å‹å†³å®šæ˜¯å¦éœ€è¦ç”Ÿæˆæˆªå›¾
            screenshot = None
            if self.enable_screenshot and self.send_content_type != "analysis_only":
                try:
                    screenshot = await analyzer.capture_screenshot(
                        url,
                        quality=self.screenshot_quality,
                        width=self.screenshot_width,
                        height=self.screenshot_height,
                        full_page=self.screenshot_full_page,
                        wait_time=self.screenshot_wait_time,
                        format=self.screenshot_format,
                    )
                    
                    # åº”ç”¨æˆªå›¾è£å‰ª
                    if self.enable_crop and screenshot:
                        try:
                            screenshot = analyzer.crop_screenshot(screenshot, tuple(self.crop_area))
                        except Exception as e:
                            logger.warning(f"è£å‰ªæˆªå›¾å¤±è´¥: {url}, é”™è¯¯: {e}")
                    

                except Exception as e:
                    # æˆªå›¾å¤±è´¥æ—¶ï¼Œè®°å½•é”™è¯¯ä½†ä¸å½±å“ä¸»åˆ†æç»“æœ
                    error_msg = self._handle_error(ErrorType.SCREENSHOT_ERROR, e, url)
                    # å°†æˆªå›¾é”™è¯¯ä¿¡æ¯æ·»åŠ åˆ°åˆ†æç»“æœä¸­
                    analysis_result += f"\n\nâš ï¸ æˆªå›¾åŠŸèƒ½æç¤º: {error_msg.splitlines()[0]}"

            # åº”ç”¨ç»“æœæ¨¡æ¿å’ŒæŠ˜å åŠŸèƒ½
            final_result = self._apply_result_settings(analysis_result, url)
            
            # å‡†å¤‡æœ€ç»ˆçš„ç»“æœæ•°æ®
            result_data = {
                "url": url,
                "result": final_result,
                "screenshot": screenshot,
            }

            # æ›´æ–°ç¼“å­˜ï¼Œä¿å­˜åˆ†æç»“æœï¼ŒåŒæ—¶ä¼ é€’ç½‘é¡µå†…å®¹ç”¨äºåŸºäºå†…å®¹å“ˆå¸Œçš„ç¼“å­˜
            try:
                self._update_cache(url, result_data, content_data["content"])
            except Exception as e:
                # ç¼“å­˜å¤±è´¥æ—¶ï¼Œè®°å½•è­¦å‘Šä½†ä¸å½±å“ç»“æœè¿”å›
                self._handle_error(ErrorType.CACHE_ERROR, e, url)

            return result_data
        except Exception as e:
            # æ•è·æ‰€æœ‰å…¶ä»–å¼‚å¸¸ï¼Œç¡®ä¿æ–¹æ³•å§‹ç»ˆè¿”å›æœ‰æ•ˆç»“æœ
            error_type = self._get_error_type(e)
            error_msg = self._handle_error(error_type, e, url)
            return {
                "url": url,
                "result": error_msg,
                "screenshot": None,
            }

    def _get_url_priority(self, url: str) -> int:
        """è¯„ä¼°URLçš„å¤„ç†ä¼˜å…ˆçº§
        
        æ ¹æ®URLçš„ç‰¹æ€§è¯„ä¼°å…¶å¤„ç†ä¼˜å…ˆçº§ï¼Œä¼˜å…ˆçº§ä»1åˆ°10ï¼Œæ•°å­—è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜
        
        Args:
            url: è¦è¯„ä¼°ä¼˜å…ˆçº§çš„URL
            
        Returns:
            ä¼˜å…ˆçº§æ•°å€¼ï¼ˆ1-10ï¼‰
        """
        return WebAnalyzerUtils.get_url_priority(url)
    
    def _render_result_template(self, result: str, url: str, template_type: str) -> str:
        """æ ¹æ®æ¨¡æ¿ç±»å‹æ¸²æŸ“åˆ†æç»“æœ
        
        Args:
            result: åŸå§‹åˆ†æç»“æœ
            url: åˆ†æçš„URL
            template_type: æ¨¡æ¿ç±»å‹
            
        Returns:
            æ¸²æŸ“åçš„åˆ†æç»“æœ
        """
        # å®šä¹‰ä¸åŒæ¨¡æ¿çš„æ¸²æŸ“é€»è¾‘
        if template_type == "detailed":
            # è¯¦ç»†æ¨¡æ¿ï¼šåŒ…å«å®Œæ•´ä¿¡æ¯å’Œæ ¼å¼
            return f"ã€è¯¦ç»†åˆ†æç»“æœã€‘\n\nğŸ“Œ åˆ†æURLï¼š{url}\n\n{result}\n\n--- åˆ†æç»“æŸ ---"
        elif template_type == "compact":
            # ç´§å‡‘æ¨¡æ¿ï¼šç®€æ´å±•ç¤ºæ ¸å¿ƒä¿¡æ¯
            lines = result.splitlines()
            compact_result = []
            for line in lines:
                if line.strip() and not line.startswith("âš ï¸"):
                    compact_result.append(line)
                    if len(compact_result) >= 10:  # æœ€å¤šæ˜¾ç¤º10è¡Œ
                        break
            return f"ã€ç´§å‡‘åˆ†æç»“æœã€‘\n{url}\n\n" + "\n".join(compact_result) + "\n\n... æ›´å¤šå†…å®¹è¯·æŸ¥çœ‹å®Œæ•´åˆ†æ"
        elif template_type == "markdown":
            # Markdownæ¨¡æ¿ï¼šä½¿ç”¨Markdownæ ¼å¼
            return f"# ç½‘é¡µåˆ†æç»“æœ\n\n## URL\n{url}\n\n## åˆ†æå†…å®¹\n{result}\n\n---\n*åˆ†æå®Œæˆäº {self._get_current_time()}*"
        elif template_type == "simple":
            # ç®€å•æ¨¡æ¿ï¼šæç®€å±•ç¤º
            return f"{url}\n\n{result}"
        else:
            # é»˜è®¤æ¨¡æ¿ï¼šæ ‡å‡†æ ¼å¼
            return f"ã€ç½‘é¡µåˆ†æç»“æœã€‘\n{url}\n\n{result}"
    
    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
        
        Returns:
            æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸²
        """
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def _collapse_result(self, result: str) -> str:
        """æ ¹æ®é…ç½®æŠ˜å é•¿ç»“æœ
        
        Args:
            result: åŸå§‹åˆ†æç»“æœ
            
        Returns:
            å¤„ç†åçš„ç»“æœï¼Œå¯èƒ½åŒ…å«æŠ˜å æ ‡è®°
        """
        if self.enable_collapsible and len(result) > self.collapse_threshold:
            # è®¡ç®—æŠ˜å ä½ç½®ï¼Œå¯»æ‰¾åˆé€‚çš„æ¢è¡Œç‚¹
            collapse_pos = self.collapse_threshold
            # å°½é‡åœ¨æ®µè½ç»“æŸå¤„æŠ˜å 
            while collapse_pos < len(result) and result[collapse_pos] != '\n':
                collapse_pos += 1
            if collapse_pos == len(result):
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ¢è¡Œï¼Œç›´æ¥æˆªæ–­
                collapse_pos = self.collapse_threshold
            
            collapsed_content = result[:collapse_pos]
            remaining_content = result[collapse_pos:]
            return f"{collapsed_content}\n\n[å±•å¼€å…¨æ–‡]\n\n{remaining_content}"
        return result
    
    def _apply_result_settings(self, result: str, url: str) -> str:
        """åº”ç”¨æ‰€æœ‰ç»“æœè®¾ç½®ï¼ˆæ¨¡æ¿æ¸²æŸ“å’ŒæŠ˜å ï¼‰
        
        Args:
            result: åŸå§‹åˆ†æç»“æœ
            url: åˆ†æçš„URL
            
        Returns:
            æœ€ç»ˆå¤„ç†åçš„åˆ†æç»“æœ
        """
        # é¦–å…ˆåº”ç”¨æ¨¡æ¿æ¸²æŸ“
        rendered_result = self._render_result_template(result, url, self.result_template)
        # ç„¶ååº”ç”¨ç»“æœæŠ˜å 
        final_result = self._collapse_result(rendered_result)
        return final_result
    
    @filter.command("web_help", alias={"ç½‘é¡µåˆ†æå¸®åŠ©", "ç½‘é¡µåˆ†æå‘½ä»¤"})
    async def show_help(self, event: AstrMessageEvent):
        """æ˜¾ç¤ºæ’ä»¶çš„æ‰€æœ‰å¯ç”¨å‘½ä»¤å’Œå¸®åŠ©ä¿¡æ¯
        
        è¿™ä¸ªå‘½ä»¤å…è®¸ç”¨æˆ·æŸ¥çœ‹æ’ä»¶çš„æ‰€æœ‰å¯ç”¨å‘½ä»¤å’Œè¯¦ç»†å¸®åŠ©ä¿¡æ¯ï¼Œ
        æ–¹ä¾¿ç”¨æˆ·äº†è§£å’Œä½¿ç”¨æ’ä»¶çš„å„ç§åŠŸèƒ½ã€‚
        
        ğŸ“‹ å‘½ä»¤åˆ†ç±»ï¼š
        - ğŸ”  æ ¸å¿ƒåˆ†æå‘½ä»¤
        - ğŸ› ï¸  é…ç½®ç®¡ç†å‘½ä»¤
        - ğŸ—‘ï¸  ç¼“å­˜ç®¡ç†å‘½ä»¤
        - ğŸ‘¥  ç¾¤èŠç®¡ç†å‘½ä»¤
        - ğŸ“¤  å¯¼å‡ºåŠŸèƒ½å‘½ä»¤
        - ğŸ“‹  æµ‹è¯•åŠŸèƒ½å‘½ä»¤
        
        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
        """
        help_text = """ã€ç½‘é¡µåˆ†ææ’ä»¶å‘½ä»¤å¸®åŠ©ã€‘
        
ğŸ“‹ æ ¸å¿ƒåˆ†æå‘½ä»¤
ğŸ” /ç½‘é¡µåˆ†æ <URL1> <URL2>... - æ‰‹åŠ¨åˆ†ææŒ‡å®šç½‘é¡µé“¾æ¥
   åˆ«åï¼š/åˆ†æ, /æ€»ç»“, /web, /analyze
   ç¤ºä¾‹ï¼š/ç½‘é¡µåˆ†æ https://example.com
        
ğŸ“‹ é…ç½®ç®¡ç†å‘½ä»¤
ğŸ› ï¸ /web_config - æŸ¥çœ‹å½“å‰æ’ä»¶é…ç½®
   åˆ«åï¼š/ç½‘é¡µåˆ†æé…ç½®, /ç½‘é¡µåˆ†æè®¾ç½®
   ç¤ºä¾‹ï¼š/web_config
        
ğŸ“‹ ç¼“å­˜ç®¡ç†å‘½ä»¤
ğŸ—‘ï¸ /web_cache [clear] - ç®¡ç†åˆ†æç»“æœç¼“å­˜
   åˆ«åï¼š/ç½‘é¡µç¼“å­˜, /æ¸…ç†ç¼“å­˜
   é€‰é¡¹ï¼š
     - clear: æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
   ç¤ºä¾‹ï¼š/web_cache clear
        
ğŸ“‹ ç¾¤èŠç®¡ç†å‘½ä»¤
ğŸ‘¥ /group_blacklist [add/remove/clear] <ç¾¤å·> - ç®¡ç†ç¾¤èŠé»‘åå•
   åˆ«åï¼š/ç¾¤é»‘åå•, /é»‘åå•
   é€‰é¡¹ï¼š
     - (ç©º): æŸ¥çœ‹å½“å‰é»‘åå•
     - add <ç¾¤å·>: æ·»åŠ ç¾¤èŠåˆ°é»‘åå•
     - remove <ç¾¤å·>: ä»é»‘åå•ç§»é™¤ç¾¤èŠ
     - clear: æ¸…ç©ºé»‘åå•
   ç¤ºä¾‹ï¼š/ç¾¤é»‘åå• add 123456789
        
ğŸ“‹ å¯¼å‡ºåŠŸèƒ½å‘½ä»¤
ğŸ“¤ /web_export - å¯¼å‡ºåˆ†æç»“æœ
   åˆ«åï¼š/å¯¼å‡ºåˆ†æç»“æœ, /ç½‘é¡µå¯¼å‡º
   ç¤ºä¾‹ï¼š/web_export
        
ğŸ“‹ æµ‹è¯•åŠŸèƒ½å‘½ä»¤
ğŸ“‹ /test_merge - æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½
   åˆ«åï¼š/æµ‹è¯•åˆå¹¶è½¬å‘, /æµ‹è¯•è½¬å‘
   ç¤ºä¾‹ï¼š/test_merge
        
ğŸ“‹ å¸®åŠ©å‘½ä»¤
â“ /web_help - æ˜¾ç¤ºæœ¬å¸®åŠ©ä¿¡æ¯
   åˆ«åï¼š/ç½‘é¡µåˆ†æå¸®åŠ©, /ç½‘é¡µåˆ†æå‘½ä»¤
   ç¤ºä¾‹ï¼š/web_help
        
ğŸ’¡ ä½¿ç”¨æç¤ºï¼š
- æ‰€æœ‰å‘½ä»¤æ”¯æŒTabè¡¥å…¨ï¼ˆå¦‚æœå®¢æˆ·ç«¯æ”¯æŒï¼‰
- å‘½ä»¤å‚æ•°æ”¯æŒæç¤ºåŠŸèƒ½
- å¯ä»¥è‡ªå®šä¹‰å‘½ä»¤åˆ«å
        
ğŸ”§ é…ç½®æç¤ºï¼š
- åœ¨AstrBotç®¡ç†é¢æ¿ä¸­å¯ä»¥é…ç½®æ’ä»¶çš„å„é¡¹åŠŸèƒ½
- æ”¯æŒè‡ªå®šä¹‰å‘½ä»¤åˆ«å
- å¯ä»¥è°ƒæ•´åˆ†æç»“æœæ¨¡æ¿å’Œæ˜¾ç¤ºæ–¹å¼
"""
        
        yield event.plain_result(help_text)
        logger.info("æ˜¾ç¤ºå‘½ä»¤å¸®åŠ©ä¿¡æ¯")
    
    def _get_available_commands(self) -> dict:
        """è·å–æ‰€æœ‰å¯ç”¨å‘½ä»¤çš„ä¿¡æ¯
        
        Returns:
            åŒ…å«æ‰€æœ‰å‘½ä»¤ä¿¡æ¯çš„å­—å…¸
        """
        return {
            "ç½‘é¡µåˆ†æ": {
                "aliases": ["åˆ†æ", "æ€»ç»“", "web", "analyze"],
                "description": "æ‰‹åŠ¨åˆ†ææŒ‡å®šç½‘é¡µé“¾æ¥",
                "usage": "/ç½‘é¡µåˆ†æ <URL1> <URL2>...",
                "options": [],
                "example": "/ç½‘é¡µåˆ†æ https://example.com"
            },
            "web_config": {
                "aliases": ["ç½‘é¡µåˆ†æé…ç½®", "ç½‘é¡µåˆ†æè®¾ç½®"],
                "description": "æŸ¥çœ‹å½“å‰æ’ä»¶é…ç½®",
                "usage": "/web_config",
                "options": [],
                "example": "/web_config"
            },
            "web_cache": {
                "aliases": ["ç½‘é¡µç¼“å­˜", "æ¸…ç†ç¼“å­˜"],
                "description": "ç®¡ç†åˆ†æç»“æœç¼“å­˜",
                "usage": "/web_cache [clear]",
                "options": ["clear"],
                "example": "/web_cache clear"
            },
            "group_blacklist": {
                "aliases": ["ç¾¤é»‘åå•", "é»‘åå•"],
                "description": "ç®¡ç†ç¾¤èŠé»‘åå•",
                "usage": "/group_blacklist [add/remove/clear] <ç¾¤å·>",
                "options": ["add", "remove", "clear"],
                "example": "/ç¾¤é»‘åå• add 123456789"
            },
            "web_export": {
                "aliases": ["å¯¼å‡ºåˆ†æç»“æœ", "ç½‘é¡µå¯¼å‡º"],
                "description": "å¯¼å‡ºåˆ†æç»“æœ",
                "usage": "/web_export",
                "options": [],
                "example": "/web_export"
            },
            "test_merge": {
                "aliases": ["æµ‹è¯•åˆå¹¶è½¬å‘", "æµ‹è¯•è½¬å‘"],
                "description": "æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½",
                "usage": "/test_merge",
                "options": [],
                "example": "/test_merge"
            },
            "web_help": {
                "aliases": ["ç½‘é¡µåˆ†æå¸®åŠ©", "ç½‘é¡µåˆ†æå‘½ä»¤"],
                "description": "æ˜¾ç¤ºå‘½ä»¤å¸®åŠ©ä¿¡æ¯",
                "usage": "/web_help",
                "options": [],
                "example": "/web_help"
            }
        }
    
    def _get_command_completions(self, input_text: str) -> list:
        """æ ¹æ®ç”¨æˆ·è¾“å…¥è·å–å‘½ä»¤è¡¥å…¨å»ºè®®
        
        Args:
            input_text: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
            
        Returns:
            å‘½ä»¤è¡¥å…¨å»ºè®®åˆ—è¡¨
        """
        if not input_text.startswith("/"):
            return []
        
        # æå–å½“å‰è¾“å…¥çš„å‘½ä»¤å‰ç¼€
        input_cmd = input_text[1:].lower()
        completions = []
        
        # è·å–æ‰€æœ‰å¯ç”¨å‘½ä»¤
        commands = self._get_available_commands()
        
        # æ£€æŸ¥ä¸»å‘½ä»¤
        for cmd_name, cmd_info in commands.items():
            if cmd_name.lower().startswith(input_cmd):
                completions.append(f"/{cmd_name}")
            
            # æ£€æŸ¥åˆ«å
            for alias in cmd_info["aliases"]:
                if alias.lower().startswith(input_cmd):
                    completions.append(f"/{alias}")
        
        return completions
    
    def _get_param_hints(self, command: str, current_params: list) -> list:
        """è·å–å‘½ä»¤å‚æ•°æç¤º
        
        Args:
            command: å½“å‰å‘½ä»¤
            current_params: å·²è¾“å…¥çš„å‚æ•°åˆ—è¡¨
            
        Returns:
            å‚æ•°æç¤ºåˆ—è¡¨
        """
        commands = self._get_available_commands()
        
        # æŸ¥æ‰¾å‘½ä»¤ä¿¡æ¯
        cmd_info = None
        for cmd_name, info in commands.items():
            if command == cmd_name or command in info["aliases"]:
                cmd_info = info
                break
        
        if not cmd_info:
            return []
        
        # æ ¹æ®å‘½ä»¤å’Œå·²è¾“å…¥å‚æ•°è¿”å›æç¤º
        if cmd_name == "group_blacklist":
            if len(current_params) == 0:
                return ["add", "remove", "clear"]
            elif len(current_params) == 1 and current_params[0] in ["add", "remove"]:
                return ["<ç¾¤å·>"]
        elif cmd_name == "web_cache":
            if len(current_params) == 0:
                return ["clear"]
        
        return []
    
    async def _batch_process_urls(self, event: AstrMessageEvent, urls: List[str]):
        """æ‰¹é‡å¤„ç†å¤šä¸ªURLï¼Œå®ç°é«˜æ•ˆçš„å¹¶å‘åˆ†æ

        è¿™ä¸ªæ–¹æ³•è´Ÿè´£ç®¡ç†å¤šä¸ªURLçš„å¹¶å‘å¤„ç†ï¼Œæé«˜æ’ä»¶çš„å¤„ç†æ•ˆç‡ï¼Œ
        æ”¯æŒå¼‚æ­¥å¹¶å‘å¤„ç†ï¼Œé¿å…é˜»å¡ç­‰å¾…å•ä¸ªURLåˆ†æå®Œæˆã€‚

        ğŸ”„ å¤„ç†æµç¨‹ï¼š
        1. ğŸš« è¿‡æ»¤æ‰æ­£åœ¨å¤„ç†çš„URLï¼Œé¿å…é‡å¤åˆ†æ
        2. ğŸ¯ ä½¿ç”¨å¼‚æ­¥æ–¹å¼å¹¶å‘å¤„ç†å¤šä¸ªURL
        3. ğŸ“¤ è°ƒç”¨_send_analysis_resultå‘é€æ‰€æœ‰åˆ†æç»“æœ
        4. ğŸ§¹ ç¡®ä¿URLå¤„ç†å®Œæˆåä»å¤„ç†é˜Ÿåˆ—ä¸­ç§»é™¤

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼Œç”¨äºç”Ÿæˆå“åº”
            urls: è¦å¤„ç†çš„URLåˆ—è¡¨
        """
        # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
        analysis_results = []

        # è¿‡æ»¤æ‰æ­£åœ¨å¤„ç†çš„URLï¼Œé¿å…é‡å¤åˆ†æ
        filtered_urls = []
        for url in urls:
            if url not in self.processing_urls:
                filtered_urls.append(url)
                # æ·»åŠ åˆ°æ­£åœ¨å¤„ç†çš„é›†åˆä¸­ï¼Œé˜²æ­¢é‡å¤å¤„ç†
                self.processing_urls.add(url)
            else:
                logger.info(f"URL {url} æ­£åœ¨å¤„ç†ä¸­ï¼Œè·³è¿‡é‡å¤åˆ†æ")

        # å¦‚æœæ‰€æœ‰URLéƒ½æ­£åœ¨å¤„ç†ä¸­ï¼Œç›´æ¥è¿”å›
        if not filtered_urls:
            return
        
        # æ ¹æ®ä¼˜å…ˆçº§å¯¹URLè¿›è¡Œæ’åº
        if self.enable_priority_scheduling:
            filtered_urls = sorted(filtered_urls, key=lambda url: self._get_url_priority(url), reverse=True)
            logger.info(f"URLä¼˜å…ˆçº§æ’åºå®Œæˆ: {[(url, self._get_url_priority(url)) for url in filtered_urls]}")

        try:
            # åˆ›å»ºWebAnalyzerå®ä¾‹ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºæ­£ç¡®é‡Šæ”¾
            async with WebAnalyzer(
                self.max_content_length,
                self.timeout,
                self.user_agent,
                self.proxy,
                self.retry_count,
                self.retry_delay,
            ) as analyzer:
                # ä½¿ç”¨asyncio.gatherå¹¶å‘å¤„ç†å¤šä¸ªURLï¼Œæé«˜æ•ˆç‡
                import asyncio
                
                # åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°
                concurrency = self.max_concurrency
                if self.dynamic_concurrency:
                    # æ ¹æ®URLæ•°é‡åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°
                    # è®¡ç®—åˆç†çš„å¹¶å‘æ•°ï¼šURLæ•°é‡çš„å¹³æ–¹æ ¹ï¼Œä¸è¶…è¿‡max_concurrency
                    dynamic_concurrency = min(self.max_concurrency, max(1, int(len(filtered_urls) ** 0.5) + 1))
                    concurrency = dynamic_concurrency
                
                logger.info(f"ä½¿ç”¨å¹¶å‘æ•°: {concurrency} å¤„ç† {len(filtered_urls)} ä¸ªURL")
                
                # åˆ†æ‰¹æ¬¡å¤„ç†URLï¼Œæ§åˆ¶å¹¶å‘æ•°
                batch_size = concurrency
                results = []
                
                # å¦‚æœå¹¶å‘æ•°å¤§äºç­‰äºURLæ•°é‡ï¼Œç›´æ¥å¤„ç†æ‰€æœ‰URL
                if batch_size >= len(filtered_urls):
                    tasks = [self._process_single_url(event, url, analyzer) for url in filtered_urls]
                    results = await asyncio.gather(*tasks)
                else:
                    # åˆ†æ‰¹æ¬¡å¤„ç†
                    for i in range(0, len(filtered_urls), batch_size):
                        batch_urls = filtered_urls[i:i+batch_size]
                        logger.info(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(filtered_urls) + batch_size - 1)//batch_size}: {batch_urls}")
                        tasks = [self._process_single_url(event, url, analyzer) for url in batch_urls]
                        batch_results = await asyncio.gather(*tasks)
                        results.extend(batch_results)
                
                analysis_results = results

            # å‘é€æ‰€æœ‰åˆ†æç»“æœ
            async for result in self._send_analysis_result(event, analysis_results):
                yield result
        finally:
            # æ— è®ºå¤„ç†æˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼Œéƒ½è¦ä»å¤„ç†é›†åˆä¸­ç§»é™¤URL
            for url in filtered_urls:
                if url in self.processing_urls:
                    self.processing_urls.remove(url)

    def _get_analysis_template(self, content_type: str, emoji_prefix: str, max_length: int) -> str:
        """æ ¹æ®å†…å®¹ç±»å‹è·å–ç›¸åº”çš„åˆ†ææ¨¡æ¿
        
        Args:
            content_type: å†…å®¹ç±»å‹
            emoji_prefix: emojiå‰ç¼€
            max_length: æœ€å¤§é•¿åº¦é™åˆ¶
            
        Returns:
            å¯¹åº”çš„åˆ†ææ¨¡æ¿å­—ç¬¦ä¸²
        """
        # å®šä¹‰å¤šç§åˆ†ææ¨¡æ¿
        templates = {
            "æ–°é—»èµ„è®¯": f"""è¯·å¯¹ä»¥ä¸‹æ–°é—»èµ„è®¯è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**æ–°é—»å†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **æ ¸å¿ƒäº‹ä»¶**ï¼šç”¨50-100å­—æ¦‚æ‹¬æ–°é—»çš„æ ¸å¿ƒäº‹ä»¶å’ŒèƒŒæ™¯
2. **å…³é”®ä¿¡æ¯**ï¼šæå–3-5ä¸ªæœ€é‡è¦çš„äº‹å®è¦ç‚¹
3. **äº‹ä»¶å½±å“**ï¼šåˆ†æäº‹ä»¶å¯èƒ½äº§ç”Ÿçš„å½±å“å’Œæ„ä¹‰
4. **ç›¸å…³èƒŒæ™¯**ï¼šè¡¥å……å¿…è¦çš„ç›¸å…³èƒŒæ™¯ä¿¡æ¯
5. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜è¿™æ¡æ–°é—»å¯¹å“ªäº›äººç¾¤æœ€æœ‰ä»·å€¼

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚""",
            
            "æ•™ç¨‹æŒ‡å—": f"""è¯·å¯¹ä»¥ä¸‹æ•™ç¨‹æŒ‡å—è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**æ•™ç¨‹å†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **æ ¸å¿ƒç›®æ ‡**ï¼šç”¨50-100å­—æ¦‚æ‹¬æ•™ç¨‹çš„æ ¸å¿ƒç›®æ ‡å’Œé€‚ç”¨åœºæ™¯
2. **å­¦ä¹ ä»·å€¼**ï¼šåˆ†æè¯¥æ•™ç¨‹å¯¹å­¦ä¹ è€…çš„ä»·å€¼å’Œæ„ä¹‰
3. **å…³é”®æ­¥éª¤**ï¼šæå–æ•™ç¨‹çš„ä¸»è¦æ­¥éª¤å’Œå…³é”®ç‚¹
4. **æŠ€æœ¯è¦ç‚¹**ï¼šæ€»ç»“æ•™ç¨‹ä¸­æ¶‰åŠçš„æ ¸å¿ƒæŠ€æœ¯æˆ–çŸ¥è¯†ç‚¹
5. **æ³¨æ„äº‹é¡¹**ï¼šæ•´ç†æ•™ç¨‹ä¸­çš„é‡è¦æç¤ºå’Œæ³¨æ„äº‹é¡¹
6. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜é€‚åˆå­¦ä¹ è¯¥æ•™ç¨‹çš„äººç¾¤

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚""",
            
            "ä¸ªäººåšå®¢": f"""è¯·å¯¹ä»¥ä¸‹ä¸ªäººåšå®¢è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**åšå®¢å†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **æ ¸å¿ƒè§‚ç‚¹**ï¼šç”¨50-100å­—æ¦‚æ‹¬åšå®¢ä½œè€…çš„æ ¸å¿ƒè§‚ç‚¹å’Œç«‹åœº
2. **ä¸»è¦å†…å®¹**ï¼šæå–åšå®¢çš„ä¸»è¦å†…å®¹å’Œè®ºè¿°è¦ç‚¹
3. **å†™ä½œé£æ ¼**ï¼šåˆ†æåšå®¢çš„å†™ä½œé£æ ¼å’Œç‰¹ç‚¹
4. **ä»·å€¼è¯„ä¼°**ï¼šè¯„ä»·åšå®¢å†…å®¹çš„ä»·å€¼å’Œå®ç”¨æ€§
5. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜é€‚åˆé˜…è¯»è¯¥åšå®¢çš„äººç¾¤

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚""",
            
            "äº§å“ä»‹ç»": f"""è¯·å¯¹ä»¥ä¸‹äº§å“ä»‹ç»è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**äº§å“å†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **äº§å“å®šä½**ï¼šç”¨50-100å­—æ¦‚æ‹¬äº§å“çš„å®šä½å’Œæ ¸å¿ƒä»·å€¼
2. **æ ¸å¿ƒåŠŸèƒ½**ï¼šæå–äº§å“çš„ä¸»è¦åŠŸèƒ½å’Œç‰¹æ€§
3. **æŠ€æœ¯å‚æ•°**ï¼šæ€»ç»“äº§å“çš„å…³é”®æŠ€æœ¯å‚æ•°å’Œè§„æ ¼
4. **é€‚ç”¨åœºæ™¯**ï¼šåˆ†æäº§å“çš„é€‚ç”¨åœºæ™¯å’Œä½¿ç”¨æ–¹æ³•
5. **ç«äº‰ä¼˜åŠ¿**ï¼šåˆ†æäº§å“ç›¸æ¯”åŒç±»äº§å“çš„ä¼˜åŠ¿
6. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜é€‚åˆä½¿ç”¨è¯¥äº§å“çš„äººç¾¤

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚""",
            
            "æŠ€æœ¯æ–‡æ¡£": f"""è¯·å¯¹ä»¥ä¸‹æŠ€æœ¯æ–‡æ¡£è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**æ–‡æ¡£å†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **æ–‡æ¡£ç›®çš„**ï¼šç”¨50-100å­—æ¦‚æ‹¬æ–‡æ¡£çš„æ ¸å¿ƒç›®çš„å’Œé€‚ç”¨èŒƒå›´
2. **æ ¸å¿ƒæ¦‚å¿µ**ï¼šæå–æ–‡æ¡£ä¸­æ¶‰åŠçš„æ ¸å¿ƒæ¦‚å¿µå’Œæœ¯è¯­
3. **æŠ€æœ¯æ¶æ„**ï¼šåˆ†ææ–‡æ¡£ä¸­æè¿°çš„æŠ€æœ¯æ¶æ„å’Œè®¾è®¡æ€è·¯
4. **ä½¿ç”¨æ–¹æ³•**ï¼šæ€»ç»“æ–‡æ¡£ä¸­ä»‹ç»çš„ä½¿ç”¨æ–¹æ³•å’Œæœ€ä½³å®è·µ
5. **å…³é”®ç‰¹æ€§**ï¼šæ•´ç†æ–‡æ¡£ä¸­æåŠçš„å…³é”®ç‰¹æ€§å’ŒåŠŸèƒ½
6. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜é€‚åˆé˜…è¯»è¯¥æ–‡æ¡£çš„äººç¾¤

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚""",
            
            "å­¦æœ¯è®ºæ–‡": f"""è¯·å¯¹ä»¥ä¸‹å­¦æœ¯è®ºæ–‡è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**è®ºæ–‡å†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **ç ”ç©¶èƒŒæ™¯**ï¼šç”¨50-100å­—æ¦‚æ‹¬è®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯å’Œæ„ä¹‰
2. **æ ¸å¿ƒé—®é¢˜**ï¼šæå–è®ºæ–‡è¯•å›¾è§£å†³çš„æ ¸å¿ƒé—®é¢˜
3. **ç ”ç©¶æ–¹æ³•**ï¼šåˆ†æè®ºæ–‡é‡‡ç”¨çš„ç ”ç©¶æ–¹æ³•å’ŒæŠ€æœ¯è·¯çº¿
4. **ä¸»è¦å‘ç°**ï¼šæ€»ç»“è®ºæ–‡çš„ä¸»è¦ç ”ç©¶å‘ç°å’Œç»“è®º
5. **åˆ›æ–°ç‚¹**ï¼šåˆ†æè®ºæ–‡çš„åˆ›æ–°ç‚¹å’Œè´¡çŒ®
6. **é€‚ç”¨é¢†åŸŸ**ï¼šè¯´æ˜è¯¥ç ”ç©¶æˆæœçš„é€‚ç”¨é¢†åŸŸå’Œåº”ç”¨å‰æ™¯

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚""",
            
            "å•†ä¸šåˆ†æ": f"""è¯·å¯¹ä»¥ä¸‹å•†ä¸šåˆ†æè¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**åˆ†æå†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **æ ¸å¿ƒä¸»é¢˜**ï¼šç”¨50-100å­—æ¦‚æ‹¬åˆ†ææŠ¥å‘Šçš„æ ¸å¿ƒä¸»é¢˜å’Œç›®çš„
2. **å¸‚åœºè¶‹åŠ¿**ï¼šæå–æŠ¥å‘Šä¸­æŒ‡å‡ºçš„ä¸»è¦å¸‚åœºè¶‹åŠ¿å’Œå˜åŒ–
3. **å…³é”®æ•°æ®**ï¼šæ€»ç»“æŠ¥å‘Šä¸­çš„å…³é”®æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯
4. **åˆ†æç»“è®º**ï¼šåˆ†ææŠ¥å‘Šçš„ä¸»è¦ç»“è®ºå’Œé¢„æµ‹
5. **å•†ä¸šä»·å€¼**ï¼šè¯„ä»·æŠ¥å‘Šå¯¹ä¼ä¸šå’ŒæŠ•èµ„è€…çš„ä»·å€¼
6. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜é€‚åˆé˜…è¯»è¯¥æŠ¥å‘Šçš„äººç¾¤

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚""",
            
            # é»˜è®¤æ¨¡æ¿
            "é»˜è®¤": f"""è¯·å¯¹ä»¥ä¸‹ç½‘é¡µå†…å®¹è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{{title}}
- é“¾æ¥ï¼š{{url}}

**ç½‘é¡µå†…å®¹**ï¼š
{{content}}

**åˆ†æè¦æ±‚**ï¼š
1. **æ ¸å¿ƒæ‘˜è¦**ï¼šç”¨50-100å­—æ¦‚æ‹¬ç½‘é¡µçš„æ ¸å¿ƒå†…å®¹å’Œä¸»æ—¨
2. **å…³é”®è¦ç‚¹**ï¼šæå–2-3ä¸ªæœ€é‡è¦çš„ä¿¡æ¯ç‚¹æˆ–è§‚ç‚¹
3. **å†…å®¹ç±»å‹**ï¼šåˆ¤æ–­ç½‘é¡µå±äºä»€ä¹ˆç±»å‹ï¼ˆæ–°é—»ã€æ•™ç¨‹ã€åšå®¢ã€äº§å“ä»‹ç»ç­‰ï¼‰
4. **ä»·å€¼è¯„ä¼°**ï¼šç®€è¦è¯„ä»·å†…å®¹çš„ä»·å€¼å’Œå®ç”¨æ€§
5. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜é€‚åˆå“ªäº›äººç¾¤é˜…è¯»

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{max_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚"""
        }
        
        # è¿”å›å¯¹åº”çš„æ¨¡æ¿ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤æ¨¡æ¿
        return templates.get(content_type, templates["é»˜è®¤"])
    
    async def analyze_with_llm(
        self, event: AstrMessageEvent, content_data: dict
    ) -> str:
        """è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)è¿›è¡Œæ™ºèƒ½å†…å®¹åˆ†æå’Œæ€»ç»“

        è¿™æ˜¯å®ç°AIæ™ºèƒ½åˆ†æçš„æ ¸å¿ƒæ–¹æ³•ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å¯¹ç½‘é¡µå†…å®¹è¿›è¡Œæ·±åº¦ç†è§£ï¼Œ
        æ”¯æŒçµæ´»çš„é…ç½®å’Œä¼˜åŒ–ï¼š

        ğŸ”§ åŠŸèƒ½ç‰¹æ€§ï¼š
        1. âœ… æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨å’Œå¯ç”¨
        2. ğŸ¤– è·å–åˆé€‚çš„LLMæä¾›å•†
        3. ğŸ’¬ æ ¹æ®å†…å®¹ç±»å‹æ™ºèƒ½é€‰æ‹©æœ€ä½³åˆ†ææ¨¡æ¿
        4. ğŸ“ è°ƒç”¨LLMç”Ÿæˆé«˜è´¨é‡åˆ†æç»“æœ
        5. ğŸ¨ ç¾åŒ–å’Œæ ¼å¼åŒ–åˆ†æç»“æœ
        6. ğŸ”„ LLMä¸å¯ç”¨æ—¶è‡ªåŠ¨å›é€€åˆ°åŸºç¡€åˆ†æ

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼Œç”¨äºè·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
            content_data: åŒ…å«ç½‘é¡µå†…å®¹çš„ç»“æ„åŒ–å­—å…¸ï¼š
                {
                    'title': ç½‘é¡µæ ‡é¢˜,
                    'content': ç½‘é¡µæ­£æ–‡å†…å®¹,
                    'url': ç½‘é¡µURLåœ°å€
                }

        Returns:
            æ ¼å¼åŒ–çš„AIåˆ†æç»“æœæ–‡æœ¬ï¼ŒåŒ…å«æ ‡é¢˜ã€é“¾æ¥ã€åˆ†æå†…å®¹ç­‰
        """
        try:
            title = content_data["title"]
            content = content_data["content"]
            url = content_data["url"]

            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨å’Œå¯ç”¨
            if not hasattr(self.context, "llm_generate") or not self.llm_enabled:
                # LLMä¸å¯ç”¨æˆ–æœªå¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
                return self.get_enhanced_analysis(content_data)

            # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„LLMæä¾›å•†ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨å½“å‰ä¼šè¯çš„æ¨¡å‹
            provider_id = self.llm_provider
            if not provider_id:
                umo = event.unified_msg_origin
                provider_id = await self.context.get_current_chat_provider_id(umo=umo)

            if not provider_id:
                # æ— æ³•è·å–LLMæä¾›å•†ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
                return self.get_enhanced_analysis(content_data)

            # æ„å»ºä¼˜åŒ–çš„LLMæç¤ºè¯
            emoji_prefix = "æ¯ä¸ªè¦ç‚¹ç”¨emojiå›¾æ ‡æ ‡è®°" if self.enable_emoji else ""
            
            # æ™ºèƒ½æ£€æµ‹å†…å®¹ç±»å‹
            content_type = self._detect_content_type(content)

            # ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯æˆ–æ ¹æ®å†…å®¹ç±»å‹é€‰æ‹©æ¨¡æ¿
            if self.custom_prompt:
                # æ›¿æ¢è‡ªå®šä¹‰æç¤ºè¯ä¸­çš„å˜é‡
                prompt = self.custom_prompt.format(
                    title=title,
                    url=url,
                    content=content,
                    max_length=self.max_summary_length,
                    content_type=content_type
                )
            else:
                # æ ¹æ®å†…å®¹ç±»å‹è·å–ç›¸åº”çš„åˆ†ææ¨¡æ¿
                template = self._get_analysis_template(content_type, emoji_prefix, self.max_summary_length)
                # æ›¿æ¢æ¨¡æ¿ä¸­çš„å˜é‡
                prompt = template.format(
                    title=title,
                    url=url,
                    content=content
                )

            # ä½¿ç”¨å½“å‰ä¼šè¯çš„èŠå¤©æ¨¡å‹IDè°ƒç”¨å¤§æ¨¡å‹
            llm_resp = await self.context.llm_generate(
                chat_provider_id=provider_id,  # ä½¿ç”¨å½“å‰ä¼šè¯çš„èŠå¤©æ¨¡å‹
                prompt=prompt,
            )

            if llm_resp and llm_resp.completion_text:
                # ç¾åŒ–LLMè¿”å›çš„ç»“æœ
                analysis_text = llm_resp.completion_text.strip()

                # é™åˆ¶æ‘˜è¦é•¿åº¦ï¼Œé¿å…ç»“æœè¿‡é•¿
                if len(analysis_text) > self.max_summary_length:
                    analysis_text = analysis_text[: self.max_summary_length] + "..."

                # æ·»åŠ æ ‡é¢˜å’Œæ ¼å¼ç¾åŒ–
                link_emoji = "ğŸ”—" if self.enable_emoji else ""
                title_emoji = "ğŸ“" if self.enable_emoji else ""
                type_emoji = "ğŸ“‹" if self.enable_emoji else ""

                formatted_result = "**AIæ™ºèƒ½ç½‘é¡µåˆ†ææŠ¥å‘Š**\n\n"
                formatted_result += f"{link_emoji} **åˆ†æé“¾æ¥**: {url}\n"
                formatted_result += f"{title_emoji} **ç½‘é¡µæ ‡é¢˜**: {title}\n"
                formatted_result += f"{type_emoji} **å†…å®¹ç±»å‹**: {content_type}\n\n"
                formatted_result += "---\n\n"
                formatted_result += analysis_text
                formatted_result += "\n\n---\n"
                formatted_result += "*åˆ†æå®Œæˆï¼Œå¸Œæœ›å¯¹æ‚¨æœ‰å¸®åŠ©ï¼*"

                return formatted_result
            else:
                # LLMè¿”å›ä¸ºç©ºï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
                return self.get_enhanced_analysis(content_data)

        except Exception as e:
            # ä½¿ç”¨ç»Ÿä¸€é”™è¯¯å¤„ç†
            return self._handle_error(ErrorType.LLM_ERROR, e, url)

    def get_enhanced_analysis(self, content_data: dict) -> str:
        """å¢å¼ºç‰ˆåŸºç¡€åˆ†æ - LLMä¸å¯ç”¨æ—¶çš„æ™ºèƒ½å›é€€æ–¹æ¡ˆ

        å½“LLMä¸å¯ç”¨æˆ–æœªå¯ç”¨æ—¶ï¼Œæä¾›å¯é çš„åŸºç¡€åˆ†æåŠŸèƒ½ï¼Œ
        åŒ…å«å¤šç§æ™ºèƒ½åˆ†æç‰¹æ€§ï¼Œç¡®ä¿æ’ä»¶åœ¨å„ç§ç¯å¢ƒä¸‹éƒ½èƒ½æ­£å¸¸å·¥ä½œï¼š

        ğŸ“Š åˆ†æå†…å®¹ï¼š
        1. ğŸ”¢ å†…å®¹ç»Ÿè®¡ï¼ˆå­—ç¬¦æ•°ã€æ®µè½æ•°ã€è¯æ•°ï¼‰
        2. ğŸ§  æ™ºèƒ½å†…å®¹ç±»å‹æ£€æµ‹ï¼ˆæ–°é—»ã€æ•™ç¨‹ã€åšå®¢ç­‰ï¼‰
        3. ğŸ” æå–å…³é”®å¥å­ä½œä¸ºå†…å®¹æ‘˜è¦
        4. â­ å†…å®¹è´¨é‡è¯„ä¼°
        5. ğŸ¨ ç¾è§‚çš„æ ¼å¼åŒ–è¾“å‡º

        âœ¨ é…ç½®æ”¯æŒï¼š
        - æ ¹æ®é…ç½®æ˜¾ç¤º/éšè—emoji
        - æ ¹æ®é…ç½®æ˜¾ç¤º/éšè—è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        - æ”¯æŒè‡ªå®šä¹‰æ ¼å¼

        Args:
            content_data: åŒ…å«ç½‘é¡µå†…å®¹çš„ç»“æ„åŒ–å­—å…¸ï¼š
                {
                    'title': ç½‘é¡µæ ‡é¢˜,
                    'content': ç½‘é¡µæ­£æ–‡å†…å®¹,
                    'url': ç½‘é¡µURLåœ°å€
                }

        Returns:
            æ ¼å¼åŒ–çš„åŸºç¡€åˆ†æç»“æœæ–‡æœ¬ï¼ŒåŒ…å«æ‰€æœ‰åˆ†æå†…å®¹
        """
        title = content_data["title"]
        content = content_data["content"]
        url = content_data["url"]

        # è®¡ç®—å†…å®¹ç»Ÿè®¡ä¿¡æ¯
        content_stats = self._calculate_content_statistics(content)
        
        # æ™ºèƒ½æ£€æµ‹å†…å®¹ç±»å‹
        content_type = self._detect_content_type(content)
        
        # æå–å…³é”®å¥å­ä½œä¸ºå†…å®¹æ‘˜è¦
        paragraphs = [p.strip() for p in content.split("\n") if p.strip()]
        key_sentences = self._extract_key_sentences(paragraphs)
        
        # è¯„ä¼°å†…å®¹è´¨é‡
        quality_indicator = self._evaluate_content_quality(content_stats["char_count"])
        
        # æ„å»ºåˆ†æç»“æœ
        return self._build_analysis_result(
            title, url, content_type, quality_indicator, content_stats, paragraphs, key_sentences
        )
    
    def _calculate_content_statistics(self, content: str) -> dict:
        """è®¡ç®—å†…å®¹ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            content: ç½‘é¡µæ­£æ–‡å†…å®¹
            
        Returns:
            åŒ…å«å­—ç¬¦æ•°ã€è¯æ•°çš„ç»Ÿè®¡å­—å…¸
        """
        char_count = len(content)
        word_count = len(content.split())
        return {
            "char_count": char_count,
            "word_count": word_count
        }
    
    def _detect_content_type(self, content: str) -> str:
        """æ™ºèƒ½æ£€æµ‹å†…å®¹ç±»å‹
        
        Args:
            content: ç½‘é¡µæ­£æ–‡å†…å®¹
            
        Returns:
            æ£€æµ‹åˆ°çš„å†…å®¹ç±»å‹å­—ç¬¦ä¸²
        """
        content_lower = content.lower()
        
        content_type_rules = [
            ("æ–°é—»èµ„è®¯", ["æ–°é—»", "æŠ¥é“", "æ¶ˆæ¯", "æ—¶äº‹", "å¿«è®¯", "å¤´æ¡", "è¦é—»", "çƒ­ç‚¹", "äº‹ä»¶"]),
            ("æ•™ç¨‹æŒ‡å—", ["æ•™ç¨‹", "æŒ‡å—", "æ•™å­¦", "æ­¥éª¤", "æ–¹æ³•", "å¦‚ä½•", "æ€æ ·", "æ•™ç¨‹", "æŒ‡å—", "æ”»ç•¥", "æŠ€å·§"]),
            ("ä¸ªäººåšå®¢", ["åšå®¢", "éšç¬”", "æ—¥è®°", "ä¸ªäºº", "è§‚ç‚¹", "æ„Ÿæƒ³", "æ„Ÿæ‚Ÿ", "æ€è€ƒ", "åˆ†äº«"]),
            ("äº§å“ä»‹ç»", ["äº§å“", "æœåŠ¡", "è´­ä¹°", "ä»·æ ¼", "ä¼˜æƒ ", "åŠŸèƒ½", "ç‰¹æ€§", "å‚æ•°", "è§„æ ¼", "è¯„æµ‹"]),
            ("æŠ€æœ¯æ–‡æ¡£", ["æŠ€æœ¯", "å¼€å‘", "ç¼–ç¨‹", "ä»£ç ", "API", "SDK", "æ–‡æ¡£", "æ•™ç¨‹", "æŒ‡å—", "è¯´æ˜"]),
            ("å­¦æœ¯è®ºæ–‡", ["è®ºæ–‡", "ç ”ç©¶", "å®éªŒ", "ç»“è®º", "æ‘˜è¦", "å…³é”®è¯", "å¼•ç”¨", "å‚è€ƒæ–‡çŒ®"]),
            ("å•†ä¸šåˆ†æ", ["åˆ†æ", "æŠ¥å‘Š", "æ•°æ®", "ç»Ÿè®¡", "è¶‹åŠ¿", "é¢„æµ‹", "å¸‚åœº", "è¡Œä¸š"]),
            ("å¨±ä¹èµ„è®¯", ["å¨±ä¹", "æ˜æ˜Ÿ", "ç”µå½±", "éŸ³ä¹", "ç»¼è‰º", "æ¼”å”±ä¼š", "é¦–æ˜ ", "æ–°æ­Œ"]),
            ("ä½“è‚²æ–°é—»", ["ä½“è‚²", "æ¯”èµ›", "èµ›äº‹", "æ¯”åˆ†", "è¿åŠ¨å‘˜", "å† å†›", "äºšå†›", "å­£å†›"]),
            ("æ•™è‚²èµ„è®¯", ["æ•™è‚²", "å­¦æ ¡", "æ‹›ç”Ÿ", "è€ƒè¯•", "åŸ¹è®­", "å­¦ä¹ ", "è¯¾ç¨‹", "æ•™æ"])
        ]
        
        for type_name, keywords in content_type_rules:
            if any(keyword in content_lower for keyword in keywords):
                return type_name
        
        return "æ–‡ç« "
    
    def _extract_key_sentences(self, paragraphs: list) -> list:
        """æå–å…³é”®å¥å­ä½œä¸ºå†…å®¹æ‘˜è¦
        
        Args:
            paragraphs: æ®µè½åˆ—è¡¨
            
        Returns:
            å…³é”®å¥å­åˆ—è¡¨
        """
        # æå–å‰3ä¸ªæ®µè½ä½œä¸ºå…³é”®å¥å­
        return paragraphs[:3]
    
    def _evaluate_content_quality(self, char_count: int) -> str:
        """è¯„ä¼°å†…å®¹è´¨é‡
        
        Args:
            char_count: å†…å®¹å­—ç¬¦æ•°
            
        Returns:
            è´¨é‡è¯„ä¼°å­—ç¬¦ä¸²
        """
        if char_count > 5000:
            return "å†…å®¹è¯¦å®"
        elif char_count > 1000:
            return "å†…å®¹ä¸°å¯Œ"
        else:
            return "å†…å®¹ç®€æ´"
    
    def _build_analysis_header(self) -> str:
        """æ„å»ºåˆ†æç»“æœçš„æ ‡é¢˜éƒ¨åˆ†
        
        Returns:
            æ ¼å¼åŒ–çš„æ ‡é¢˜å­—ç¬¦ä¸²
        """
        robot_emoji = "ğŸ¤–" if self.enable_emoji else ""
        page_emoji = "ğŸ“„" if self.enable_emoji else ""
        return f"{robot_emoji} **æ™ºèƒ½ç½‘é¡µåˆ†æ** {page_emoji}\n\n"
    
    def _build_basic_info(self, title: str, url: str, content_type: str, 
                         quality_indicator: str) -> str:
        """æ„å»ºåˆ†æç»“æœçš„åŸºæœ¬ä¿¡æ¯éƒ¨åˆ†
        
        Args:
            title: ç½‘é¡µæ ‡é¢˜
            url: ç½‘é¡µURL
            content_type: å†…å®¹ç±»å‹
            quality_indicator: è´¨é‡è¯„ä¼°
            
        Returns:
            æ ¼å¼åŒ–çš„åŸºæœ¬ä¿¡æ¯å­—ç¬¦ä¸²
        """
        info_emoji = "ğŸ“" if self.enable_emoji else ""
        
        basic_info = []
        if self.enable_emoji:
            basic_info.append(f"**{info_emoji} åŸºæœ¬ä¿¡æ¯**\n")
        else:
            basic_info.append("**åŸºæœ¬ä¿¡æ¯**\n")
        
        basic_info.append(f"- **æ ‡é¢˜**: {title}\n")
        basic_info.append(f"- **é“¾æ¥**: {url}\n")
        basic_info.append(f"- **å†…å®¹ç±»å‹**: {content_type}\n")
        basic_info.append(f"- **è´¨é‡è¯„ä¼°**: {quality_indicator}\n\n")
        
        return "".join(basic_info)
    
    def _build_statistics_info(self, content_stats: dict, paragraphs: list) -> str:
        """æ„å»ºåˆ†æç»“æœçš„ç»Ÿè®¡ä¿¡æ¯éƒ¨åˆ†
        
        Args:
            content_stats: å†…å®¹ç»Ÿè®¡ä¿¡æ¯
            paragraphs: æ®µè½åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„ç»Ÿè®¡ä¿¡æ¯å­—ç¬¦ä¸²
        """
        if not self.enable_statistics:
            return ""
        
        stats_emoji = "ğŸ“Š" if self.enable_emoji else ""
        
        stats_info = []
        if self.enable_emoji:
            stats_info.append(f"**{stats_emoji} å†…å®¹ç»Ÿè®¡**\n")
        else:
            stats_info.append("**å†…å®¹ç»Ÿè®¡**\n")
        
        stats_info.append(f"- å­—ç¬¦æ•°: {content_stats['char_count']:,}\n")
        stats_info.append(f"- æ®µè½æ•°: {len(paragraphs)}\n")
        stats_info.append(f"- è¯æ•°: {content_stats['word_count']:,}\n\n")
        
        return "".join(stats_info)
    
    def _build_content_summary(self, key_sentences: list) -> str:
        """æ„å»ºåˆ†æç»“æœçš„å†…å®¹æ‘˜è¦éƒ¨åˆ†
        
        Args:
            key_sentences: å…³é”®å¥å­åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„å†…å®¹æ‘˜è¦å­—ç¬¦ä¸²
        """
        search_emoji = "ğŸ”" if self.enable_emoji else ""
        
        summary_info = []
        if self.enable_emoji:
            summary_info.append(f"**{search_emoji} å†…å®¹æ‘˜è¦**\n")
        else:
            summary_info.append("**å†…å®¹æ‘˜è¦**\n")
        
        # æ ¼å¼åŒ–å…³é”®å¥å­
        formatted_sentences = []
        for sentence in key_sentences:
            truncated = sentence[:100] + ('...' if len(sentence) > 100 else '')
            formatted_sentences.append(f"â€¢ {truncated}")
        
        summary_info.append(f"{chr(10).join(formatted_sentences)}\n\n")
        return "".join(summary_info)
    
    def _build_analysis_note(self) -> str:
        """æ„å»ºåˆ†æç»“æœçš„åˆ†æè¯´æ˜éƒ¨åˆ†
        
        Returns:
            æ ¼å¼åŒ–çš„åˆ†æè¯´æ˜å­—ç¬¦ä¸²
        """
        light_emoji = "ğŸ’¡" if self.enable_emoji else ""
        
        note_info = []
        if self.enable_emoji:
            note_info.append(f"**{light_emoji} åˆ†æè¯´æ˜**\n")
        else:
            note_info.append("**åˆ†æè¯´æ˜**\n")
        
        note_info.append("æ­¤åˆ†æåŸºäºç½‘é¡µå†…å®¹æå–ï¼Œå¦‚éœ€æ›´æ·±å…¥çš„AIæ™ºèƒ½åˆ†æï¼Œè¯·ç¡®ä¿AstrBotå·²æ­£ç¡®é…ç½®LLMåŠŸèƒ½ã€‚\n\n")
        note_info.append("*æç¤ºï¼šå®Œæ•´å†…å®¹é¢„è§ˆè¯·æŸ¥çœ‹åŸå§‹ç½‘é¡µ*")
        
        return "".join(note_info)
    
    def _build_analysis_result(self, title: str, url: str, content_type: str, 
                              quality_indicator: str, content_stats: dict, 
                              paragraphs: list, key_sentences: list) -> str:
        """æ„å»ºæœ€ç»ˆçš„åˆ†æç»“æœ
        
        Args:
            title: ç½‘é¡µæ ‡é¢˜
            url: ç½‘é¡µURL
            content_type: å†…å®¹ç±»å‹
            quality_indicator: è´¨é‡è¯„ä¼°
            content_stats: å†…å®¹ç»Ÿè®¡ä¿¡æ¯
            paragraphs: æ®µè½åˆ—è¡¨
            key_sentences: å…³é”®å¥å­åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„åˆ†æç»“æœå­—ç¬¦ä¸²
        """
        # æ„å»ºåˆ†æç»“æœ
        result_parts = []
        result_parts.append(self._build_analysis_header())
        result_parts.append(self._build_basic_info(title, url, content_type, quality_indicator))
        result_parts.append(self._build_statistics_info(content_stats, paragraphs))
        result_parts.append(self._build_content_summary(key_sentences))
        result_parts.append(self._build_analysis_note())
        
        return "".join(result_parts)
    

    
    def _handle_error(self, error_type: str, original_error: Exception, url: Optional[str] = None) -> str:
        """ç»Ÿä¸€é”™è¯¯å¤„ç†æ–¹æ³•
        
        æ ¹æ®é”™è¯¯ç±»å‹ç”Ÿæˆæ ¼å¼åŒ–çš„é”™è¯¯ä¿¡æ¯ï¼Œå¹¶è®°å½•è¯¦ç»†æ—¥å¿—
        
        Args:
            error_type: é”™è¯¯ç±»å‹
            original_error: åŸå§‹å¼‚å¸¸å¯¹è±¡
            url: ç›¸å…³URLï¼ˆå¦‚æœæœ‰ï¼‰
            
        Returns:
            æ ¼å¼åŒ–çš„é”™è¯¯ä¿¡æ¯å­—ç¬¦ä¸²
        """
        # è·å–é”™è¯¯é…ç½®
        error_config = ERROR_MESSAGES.get(error_type, ERROR_MESSAGES["unknown_error"])
        error_message = error_config["message"]
        solution = error_config["solution"]
        severity = error_config["severity"]
        
        # æ„å»ºæ—¥å¿—ä¿¡æ¯
        log_message = f"{error_message}: {str(original_error)}"
        if url:
            log_message += f" (URL: {url})"
        
        # æ ¹æ®ä¸¥é‡ç¨‹åº¦è®°å½•æ—¥å¿—
        if severity == ErrorSeverity.INFO:
            logger.info(log_message)
        elif severity == ErrorSeverity.WARNING:
            logger.warning(log_message)
        elif severity == ErrorSeverity.ERROR:
            logger.error(log_message)
        elif severity == ErrorSeverity.CRITICAL:
            logger.critical(log_message, exc_info=True)
        
        # ç”Ÿæˆç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯
        user_message = f"âŒ {error_message}"
        if url:
            user_message += f" (URL: {url})"
        user_message += f"\né”™è¯¯è¯¦æƒ…: {str(original_error)}"
        user_message += f"\nå»ºè®®è§£å†³æ–¹æ¡ˆ: {solution}"
        
        return user_message
    
    def _get_error_type(self, exception: Exception) -> str:
        """æ ¹æ®å¼‚å¸¸ç±»å‹è·å–å¯¹åº”çš„é”™è¯¯ç±»å‹
        
        Args:
            exception: å¼‚å¸¸å¯¹è±¡
            
        Returns:
            å¯¹åº”çš„é”™è¯¯ç±»å‹å­—ç¬¦ä¸²
        """
        from httpx import HTTPError
        
        exception_type = type(exception).__name__
        
        # ç½‘ç»œç›¸å…³é”™è¯¯
        if isinstance(exception, HTTPError) or "network" in exception_type.lower() or "timeout" in exception_type.lower():
            return ErrorType.NETWORK_ERROR
        # è§£æç›¸å…³é”™è¯¯
        elif "parse" in exception_type.lower() or "soup" in exception_type.lower() or "lxml" in exception_type.lower():
            return ErrorType.PARSING_ERROR
        # LLMç›¸å…³é”™è¯¯
        elif "llm" in exception_type.lower() or "generate" in exception_type.lower():
            return ErrorType.LLM_ERROR
        # æˆªå›¾ç›¸å…³é”™è¯¯
        elif "screenshot" in exception_type.lower() or "playwright" in exception_type.lower() or "browser" in exception_type.lower():
            return ErrorType.SCREENSHOT_ERROR
        # ç¼“å­˜ç›¸å…³é”™è¯¯
        elif "cache" in exception_type.lower() or "file" in exception_type.lower() or "io" in exception_type.lower():
            return ErrorType.CACHE_ERROR
        # é…ç½®ç›¸å…³é”™è¯¯
        elif "config" in exception_type.lower() or "setting" in exception_type.lower():
            return ErrorType.CONFIG_ERROR
        # æƒé™ç›¸å…³é”™è¯¯
        elif "permission" in exception_type.lower() or "auth" in exception_type.lower():
            return ErrorType.PERMISSION_ERROR
        # æœªçŸ¥é”™è¯¯
        else:
            return ErrorType.UNKNOWN_ERROR
    
    async def _auto_recall_message(self, bot, message_id: int, recall_time: int) -> None:
        """
        è‡ªåŠ¨æ’¤å›æ¶ˆæ¯
        
        å‚æ•°:
            bot: æœºå™¨äººå®ä¾‹ï¼Œç”¨äºå‘é€æ’¤å›è¯·æ±‚
            message_id: è¦æ’¤å›çš„æ¶ˆæ¯ID
            recall_time: å»¶è¿Ÿæ’¤å›çš„æ—¶é—´ï¼ˆç§’ï¼‰
        """
        try:
            import asyncio
            
            # ç­‰å¾…æŒ‡å®šæ—¶é—´
            if recall_time > 0:
                await asyncio.sleep(recall_time)
            
            # è°ƒç”¨botçš„delete_msgæ–¹æ³•æ’¤å›æ¶ˆæ¯
            await bot.delete_msg(message_id=message_id)
            logger.debug(f"å·²æ’¤å›æ¶ˆæ¯: {message_id}")
        except Exception as e:
            logger.error(f"æ’¤å›æ¶ˆæ¯å¤±è´¥: {e}")
    
    async def _send_processing_message(self, event: AstrMessageEvent, message: str) -> None:
        """
        å‘é€æ­£åœ¨åˆ†æçš„æ¶ˆæ¯å¹¶è®¾ç½®è‡ªåŠ¨æ’¤å›
        
        å‚æ•°:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼Œç”¨äºè·å–botå®ä¾‹å’Œæ¶ˆæ¯ä¸Šä¸‹æ–‡
            message: è¦å‘é€çš„æ¶ˆæ¯å†…å®¹
        """
        import asyncio
        
        # è·å–botå®ä¾‹
        bot = event.bot
        
        # ç›´æ¥è°ƒç”¨botçš„å‘é€æ¶ˆæ¯æ–¹æ³•ï¼Œè·å–æ¶ˆæ¯ID
        try:
            # æ ¹æ®äº‹ä»¶ç±»å‹é€‰æ‹©å‘é€æ–¹æ³•
            send_result = None
            group_id = None
            user_id = None
            
            # æ–¹æ³•1ï¼šä½¿ç”¨AiocqhttpMessageEventçš„æ–¹æ³•è·å–
            if hasattr(event, 'get_group_id'):
                group_id = event.get_group_id()
            if hasattr(event, 'get_sender_id'):
                user_id = event.get_sender_id()
            
            # æ–¹æ³•2ï¼šåˆ¤æ–­æ˜¯å¦ä¸ºç§èŠ
            is_private = False
            if hasattr(event, 'is_private_chat'):
                is_private = event.is_private_chat()
            
            # å‘é€æ¶ˆæ¯
            if group_id:
                # ç¾¤èŠæ¶ˆæ¯
                send_result = await bot.send_group_msg(
                    group_id=group_id,
                    message=message
                )
                logger.debug(f"å‘é€ç¾¤èŠå¤„ç†æ¶ˆæ¯: {message} åˆ°ç¾¤ {group_id}")
            elif user_id or is_private:
                # ç§èŠæ¶ˆæ¯
                if not user_id and hasattr(event, 'get_sender_id'):
                    user_id = event.get_sender_id()
                
                if user_id:
                    send_result = await bot.send_private_msg(
                        user_id=user_id,
                        message=message
                    )
                    logger.debug(f"å‘é€ç§èŠå¤„ç†æ¶ˆæ¯: {message} åˆ°ç”¨æˆ· {user_id}")
                else:
                    # æ— æ³•è·å–user_idï¼Œä½¿ç”¨åŸå§‹æ–¹å¼å‘é€
                    logger.warning(f"æ— æ³•è·å–user_idï¼Œä½¿ç”¨åŸå§‹æ–¹å¼å‘é€æ¶ˆæ¯: {message}")
                    response = event.plain_result(message)
                    if hasattr(event, 'send'):
                        await event.send(response)
                    return
            else:
                # æ— æ³•ç¡®å®šæ¶ˆæ¯ç±»å‹ï¼Œä½¿ç”¨åŸå§‹æ–¹å¼å‘é€å¹¶è®°å½•è¯¦ç»†ä¿¡æ¯
                logger.error(f"æ— æ³•ç¡®å®šæ¶ˆæ¯ç±»å‹ï¼Œeventç±»å‹: {type(event)}, eventæ–¹æ³•: get_group_id={hasattr(event, 'get_group_id')}, get_sender_id={hasattr(event, 'get_sender_id')}, is_private_chat={hasattr(event, 'is_private_chat')}")
                # å°è¯•ä½¿ç”¨event.plain_resultå‘é€ï¼Œè™½ç„¶æ— æ³•è·å–message_id
                response = event.plain_result(message)
                # ä½¿ç”¨eventçš„sendæ–¹æ³•å‘é€
                if hasattr(event, 'send'):
                    await event.send(response)
                return
            
            # æ£€æŸ¥send_resultæ˜¯å¦åŒ…å«message_id
            message_id = None
            if isinstance(send_result, dict):
                message_id = send_result.get('message_id')
            elif hasattr(send_result, 'message_id'):
                message_id = send_result.message_id
            
            logger.debug(f"å‘é€å¤„ç†æ¶ˆæ¯æˆåŠŸï¼Œmessage_id: {message_id}")
            
            # å¦‚æœè·å–åˆ°message_idä¸”å¯ç”¨äº†è‡ªåŠ¨æ’¤å›ï¼Œåˆ›å»ºæ’¤å›ä»»åŠ¡
            if message_id and self.enable_recall:
                logger.info(f"åˆ›å»ºæ’¤å›ä»»åŠ¡ï¼Œmessage_id: {message_id}ï¼Œå»¶è¿Ÿ: {self.recall_time}ç§’")
                
                async def _recall_task():
                    try:
                        await asyncio.sleep(self.recall_time)
                        await bot.delete_msg(message_id=message_id)
                        logger.info(f"å·²æ’¤å›æ¶ˆæ¯: {message_id}")
                    except Exception as e:
                        logger.error(f"æ’¤å›æ¶ˆæ¯å¤±è´¥: {e}")
                
                task = asyncio.create_task(_recall_task())
                
                # å°†ä»»åŠ¡æ·»åŠ åˆ°åˆ—è¡¨ä¸­ç®¡ç†
                self.recall_tasks.append(task)
                
                # æ·»åŠ å®Œæˆå›è°ƒï¼Œä»åˆ—è¡¨ä¸­ç§»é™¤å·²å®Œæˆçš„ä»»åŠ¡
                def _remove_task(t):
                    try:
                        self.recall_tasks.remove(t)
                    except ValueError:
                        pass
                
                task.add_done_callback(_remove_task)
        except Exception as e:
            logger.error(f"å‘é€å¤„ç†æ¶ˆæ¯æˆ–è®¾ç½®æ’¤å›å¤±è´¥: {e}")

    @filter.command("web_config", alias={"ç½‘é¡µåˆ†æé…ç½®", "ç½‘é¡µåˆ†æè®¾ç½®"})
    async def show_config(self, event: AstrMessageEvent):
        """æ˜¾ç¤ºå½“å‰æ’ä»¶çš„è¯¦ç»†é…ç½®ä¿¡æ¯

        è¿™ä¸ªå‘½ä»¤å…è®¸ç”¨æˆ·æŸ¥çœ‹æ’ä»¶çš„æ‰€æœ‰é…ç½®é¡¹ï¼Œæ–¹ä¾¿äº†è§£æ’ä»¶çš„å½“å‰çŠ¶æ€å’Œè®¾ç½®ï¼Œ
        æ”¯æŒå¤šç§å‘½ä»¤åˆ«åï¼Œæ–¹ä¾¿ç”¨æˆ·è°ƒç”¨ã€‚

        ğŸ“‹ æ˜¾ç¤ºå†…å®¹ï¼š
        - ğŸ› ï¸  åŸºæœ¬è®¾ç½®ï¼ˆè¶…æ—¶ã€é‡è¯•ã€è‡ªåŠ¨åˆ†æç­‰ï¼‰
        - ğŸš«  åŸŸåæ§åˆ¶ï¼ˆå…è®¸/ç¦æ­¢åˆ—è¡¨ï¼‰
        - ğŸ‘¥  ç¾¤èŠæ§åˆ¶ï¼ˆé»‘åå•ï¼‰
        - ğŸ“Š  åˆ†æè®¾ç½®ï¼ˆemojiã€ç»Ÿè®¡ã€æ‘˜è¦é•¿åº¦ç­‰ï¼‰
        - ğŸ§   LLMé…ç½®ï¼ˆæä¾›å•†ã€è‡ªå®šä¹‰æç¤ºè¯ç­‰ï¼‰
        - ğŸŒ  ç¿»è¯‘è®¾ç½®ï¼ˆè‡ªåŠ¨ç¿»è¯‘ã€ç›®æ ‡è¯­è¨€ç­‰ï¼‰
        - ğŸ’¾  ç¼“å­˜è®¾ç½®ï¼ˆè¿‡æœŸæ—¶é—´ã€æœ€å¤§æ•°é‡ç­‰ï¼‰
        - ğŸ“‹  å†…å®¹æå–è®¾ç½®ï¼ˆæå–ç±»å‹ç­‰ï¼‰

        ğŸ“ ä½¿ç”¨ç¤ºä¾‹ï¼š
        - `/web_config` - æŸ¥çœ‹é…ç½®
        - `/ç½‘é¡µåˆ†æé…ç½®` - ä¸­æ–‡å‘½ä»¤
        - `/ç½‘é¡µåˆ†æè®¾ç½®` - ä¸­æ–‡åˆ«å

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼Œç”¨äºç”Ÿæˆå“åº”
        """
        config_info = f"""**ç½‘é¡µåˆ†ææ’ä»¶é…ç½®ä¿¡æ¯**

**åŸºæœ¬è®¾ç½®**
- æœ€å¤§å†…å®¹é•¿åº¦: {self.max_content_length} å­—ç¬¦
- è¯·æ±‚è¶…æ—¶æ—¶é—´: {self.timeout} ç§’
- LLMæ™ºèƒ½åˆ†æ: {"âœ… å·²å¯ç”¨" if self.llm_enabled else "âŒ å·²ç¦ç”¨"}
- è‡ªåŠ¨åˆ†æé“¾æ¥: {"âœ… å·²å¯ç”¨" if self.auto_analyze else "âŒ å·²ç¦ç”¨"}
- åˆå¹¶è½¬å‘åŠŸèƒ½(ç¾¤èŠ): {"âœ… å·²å¯ç”¨" if self.merge_forward_enabled["group"] else "âŒ å·²ç¦ç”¨"}
- åˆå¹¶è½¬å‘åŠŸèƒ½(ç§èŠ): {"âœ… å·²å¯ç”¨" if self.merge_forward_enabled["private"] else "âŒ å·²ç¦ç”¨"}
- åˆå¹¶è½¬å‘åŒ…å«æˆªå›¾: {"âœ… å·²å¯ç”¨" if self.merge_forward_enabled["include_screenshot"] else "âŒ å·²ç¦ç”¨"}

**å¹¶å‘å¤„ç†è®¾ç½®**
- æœ€å¤§å¹¶å‘æ•°: {self.max_concurrency}
- åŠ¨æ€å¹¶å‘æ§åˆ¶: {"âœ… å·²å¯ç”¨" if self.dynamic_concurrency else "âŒ å·²ç¦ç”¨"}
- ä¼˜å…ˆçº§è°ƒåº¦: {"âœ… å·²å¯ç”¨" if self.enable_priority_scheduling else "âŒ å·²ç¦ç”¨"}

**åŸŸåæ§åˆ¶**
- å…è®¸åŸŸå: {len(self.allowed_domains)} ä¸ª
- ç¦æ­¢åŸŸå: {len(self.blocked_domains)} ä¸ª

**ç¾¤èŠæ§åˆ¶**
- ç¾¤èŠé»‘åå•: {len(self.group_blacklist)} ä¸ªç¾¤èŠ

**åˆ†æè®¾ç½®**
- å¯ç”¨emoji: {"âœ… å·²å¯ç”¨" if self.enable_emoji else "âŒ å·²ç¦ç”¨"}
- æ˜¾ç¤ºç»Ÿè®¡: {"âœ… å·²å¯ç”¨" if self.enable_statistics else "âŒ å·²ç¦ç”¨"}
- æœ€å¤§æ‘˜è¦é•¿åº¦: {self.max_summary_length} å­—ç¬¦
- å‘é€å†…å®¹ç±»å‹: {self.send_content_type}
- å¯ç”¨æˆªå›¾: {"âœ… å·²å¯ç”¨" if self.enable_screenshot else "âŒ å·²ç¦ç”¨"}
- æˆªå›¾è´¨é‡: {self.screenshot_quality}
- æˆªå›¾å®½åº¦: {self.screenshot_width}px
- æˆªå›¾é«˜åº¦: {self.screenshot_height}px
- æˆªå›¾æ ¼å¼: {self.screenshot_format}
- æˆªå–æ•´é¡µ: {"âœ… å·²å¯ç”¨" if self.screenshot_full_page else "âŒ å·²ç¦ç”¨"}
- æˆªå›¾ç­‰å¾…æ—¶é—´: {self.screenshot_wait_time}ms
- å¯ç”¨æˆªå›¾è£å‰ª: {"âœ… å·²å¯ç”¨" if self.enable_crop else "âŒ å·²ç¦ç”¨"}
- è£å‰ªåŒºåŸŸ: {self.crop_area}
- å¯ç”¨æ°´å°: {"âœ… å·²å¯ç”¨" if self.enable_watermark else "âŒ å·²ç¦ç”¨"}
- æ°´å°æ–‡æœ¬: {self.watermark_text}
- æ°´å°å­—ä½“å¤§å°: {self.watermark_font_size}
- æ°´å°é€æ˜åº¦: {self.watermark_opacity}%
- æ°´å°ä½ç½®: {self.watermark_position}

**LLMé…ç½®**
- æŒ‡å®šæä¾›å•†: {self.llm_provider if self.llm_provider else "ä½¿ç”¨ä¼šè¯é»˜è®¤"}
- è‡ªå®šä¹‰æç¤ºè¯: {"âœ… å·²å¯ç”¨" if self.custom_prompt else "âŒ æœªè®¾ç½®"}

**ç¿»è¯‘è®¾ç½®**
- å¯ç”¨ç½‘é¡µç¿»è¯‘: {"âœ… å·²å¯ç”¨" if self.enable_translation else "âŒ å·²ç¦ç”¨"}
- ç›®æ ‡è¯­è¨€: {self.target_language}
- ç¿»è¯‘æä¾›å•†: {self.translation_provider}
- è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯: {"âœ… å·²å¯ç”¨" if self.custom_translation_prompt else "âŒ æœªè®¾ç½®"}

**ç¼“å­˜è®¾ç½®**
- å¯ç”¨ç»“æœç¼“å­˜: {"âœ… å·²å¯ç”¨" if self.enable_cache else "âŒ å·²ç¦ç”¨"}
- ç¼“å­˜è¿‡æœŸæ—¶é—´: {self.cache_expire_time} åˆ†é’Ÿ
- æœ€å¤§ç¼“å­˜æ•°é‡: {self.max_cache_size} ä¸ª
- å¯ç”¨ç¼“å­˜é¢„åŠ è½½: {"âœ… å·²å¯ç”¨" if self.cache_preload_enabled else "âŒ å·²ç¦ç”¨"}
- é¢„åŠ è½½ç¼“å­˜æ•°é‡: {self.cache_preload_count} ä¸ª

**å†…å®¹æå–è®¾ç½®**
- å¯ç”¨ç‰¹å®šå†…å®¹æå–: {"âœ… å·²å¯ç”¨" if self.enable_specific_extraction else "âŒ å·²ç¦ç”¨"}
- æå–å†…å®¹ç±»å‹: {", ".join(self.extract_types)}

*æç¤º: å¦‚éœ€ä¿®æ”¹é…ç½®ï¼Œè¯·åœ¨AstrBotç®¡ç†é¢æ¿ä¸­ç¼–è¾‘æ’ä»¶é…ç½®*"""

        yield event.plain_result(config_info)

    @filter.command("test_merge", alias={"æµ‹è¯•åˆå¹¶è½¬å‘", "æµ‹è¯•è½¬å‘"})
    async def test_merge_forward(self, event: AstrMessageEvent):
        """æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½

        è¿™ä¸ªå‘½ä»¤ç”¨äºæµ‹è¯•æ’ä»¶çš„åˆå¹¶è½¬å‘åŠŸèƒ½ï¼Œä»…åœ¨ç¾¤èŠç¯å¢ƒä¸­å¯ç”¨ï¼Œ
        æ–¹ä¾¿ç”¨æˆ·éªŒè¯åˆå¹¶è½¬å‘åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚

        ğŸ“ ä½¿ç”¨ç¤ºä¾‹ï¼š
        - `/test_merge` - æµ‹è¯•åˆå¹¶è½¬å‘
        - `/æµ‹è¯•åˆå¹¶è½¬å‘` - ä¸­æ–‡å‘½ä»¤
        - `/æµ‹è¯•è½¬å‘` - ä¸­æ–‡åˆ«å

        âœ¨ åŠŸèƒ½è¯´æ˜ï¼š
        - åˆ›å»ºæµ‹è¯•ç”¨çš„åˆå¹¶è½¬å‘æ¶ˆæ¯
        - åŒ…å«æ ‡é¢˜èŠ‚ç‚¹å’Œä¸¤ä¸ªå†…å®¹èŠ‚ç‚¹
        - æ¼”ç¤ºåˆå¹¶è½¬å‘çš„åŸºæœ¬ç”¨æ³•å’Œæ•ˆæœ
        - éªŒè¯åˆå¹¶è½¬å‘åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼Œç”¨äºç”Ÿæˆæµ‹è¯•æ¶ˆæ¯
        """
        from astrbot.api.message_components import Node, Plain, Nodes

        # æ£€æŸ¥æ˜¯å¦ä¸ºç¾¤èŠæ¶ˆæ¯ï¼Œåˆå¹¶è½¬å‘ä»…æ”¯æŒç¾¤èŠ
        group_id = None
        if hasattr(event, "group_id") and event.group_id:
            group_id = event.group_id
        elif (
            hasattr(event, "message_obj")
            and hasattr(event.message_obj, "group_id")
            and event.message_obj.group_id
        ):
            group_id = event.message_obj.group_id

        if group_id:
            # åˆ›å»ºæµ‹è¯•ç”¨çš„åˆå¹¶è½¬å‘èŠ‚ç‚¹
            nodes = []

            # æ ‡é¢˜èŠ‚ç‚¹
            title_node = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•åˆå¹¶è½¬å‘",
                content=[Plain("è¿™æ˜¯åˆå¹¶è½¬å‘æµ‹è¯•æ¶ˆæ¯")],
            )
            nodes.append(title_node)

            # å†…å®¹èŠ‚ç‚¹1
            content_node1 = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•èŠ‚ç‚¹1",
                content=[Plain("è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•èŠ‚ç‚¹å†…å®¹")],
            )
            nodes.append(content_node1)

            # å†…å®¹èŠ‚ç‚¹2
            content_node2 = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•èŠ‚ç‚¹2",
                content=[Plain("è¿™æ˜¯ç¬¬äºŒä¸ªæµ‹è¯•èŠ‚ç‚¹å†…å®¹")],
            )
            nodes.append(content_node2)

            # ä½¿ç”¨NodesåŒ…è£…æ‰€æœ‰èŠ‚ç‚¹ï¼Œåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
            merge_forward_message = Nodes(nodes)
            yield event.chain_result([merge_forward_message])
            logger.info(f"æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½ï¼Œç¾¤èŠ {group_id}")
        else:
            yield event.plain_result("åˆå¹¶è½¬å‘åŠŸèƒ½ä»…æ”¯æŒç¾¤èŠæ¶ˆæ¯æµ‹è¯•")
            logger.info("ç§èŠæ¶ˆæ¯æ— æ³•æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½")

    @filter.command("group_blacklist", alias={"ç¾¤é»‘åå•", "é»‘åå•"})
    async def manage_group_blacklist(self, event: AstrMessageEvent):
        """ç®¡ç†ç¾¤èŠé»‘åå•

        è¿™ä¸ªå‘½ä»¤å…è®¸ç”¨æˆ·ç®¡ç†æ’ä»¶çš„ç¾¤èŠé»‘åå•ï¼Œæ§åˆ¶å“ªäº›ç¾¤èŠä¸èƒ½ä½¿ç”¨æ’ä»¶ï¼Œ
        æ”¯æŒå¤šç§æ“ä½œï¼ŒåŒ…æ‹¬æŸ¥çœ‹ã€æ·»åŠ ã€ç§»é™¤å’Œæ¸…ç©ºé»‘åå•ã€‚

        ğŸ“‹ å‘½ä»¤ç”¨æ³•ï¼š
        1. ğŸ” æŸ¥çœ‹é»‘åå•ï¼š`/group_blacklist`
        2. â• æ·»åŠ ç¾¤èŠï¼š`/group_blacklist add <ç¾¤å·>`
        3. â– ç§»é™¤ç¾¤èŠï¼š`/group_blacklist remove <ç¾¤å·>`
        4. ğŸ—‘ï¸ æ¸…ç©ºé»‘åå•ï¼š`/group_blacklist clear`

        ğŸ”¤ æ”¯æŒåˆ«åï¼š
        - `/ç¾¤é»‘åå•`
        - `/é»‘åå•`

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼Œç”¨äºè·å–å‘½ä»¤å‚æ•°å’Œç”Ÿæˆå“åº”
        """
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()

        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰é»‘åå•åˆ—è¡¨
        if len(message_parts) <= 1:
            if not self.group_blacklist:
                yield event.plain_result("å½“å‰ç¾¤èŠé»‘åå•ä¸ºç©º")
                return

            blacklist_info = "**å½“å‰ç¾¤èŠé»‘åå•**\n\n"
            for i, group_id in enumerate(self.group_blacklist, 1):
                blacklist_info += f"{i}. {group_id}\n"

            blacklist_info += "\nä½¿ç”¨ `/group_blacklist add <ç¾¤å·>` æ·»åŠ ç¾¤èŠåˆ°é»‘åå•"
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist remove <ç¾¤å·>` ä»é»‘åå•ç§»é™¤ç¾¤èŠ"
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist clear` æ¸…ç©ºé»‘åå•"

            yield event.plain_result(blacklist_info)
            return

        # è§£ææ“ä½œç±»å‹å’Œå‚æ•°
        action = message_parts[1].lower() if len(message_parts) > 1 else ""
        group_id = message_parts[2] if len(message_parts) > 2 else ""

        # æ·»åŠ ç¾¤èŠåˆ°é»‘åå•
        if action == "add" and group_id:
            if group_id in self.group_blacklist:
                yield event.plain_result(f"ç¾¤èŠ {group_id} å·²åœ¨é»‘åå•ä¸­")
                return

            self.group_blacklist.append(group_id)
            self._save_group_blacklist()
            yield event.plain_result(f"âœ… å·²æ·»åŠ ç¾¤èŠ {group_id} åˆ°é»‘åå•")

        # ä»é»‘åå•ç§»é™¤ç¾¤èŠ
        elif action == "remove" and group_id:
            if group_id not in self.group_blacklist:
                yield event.plain_result(f"ç¾¤èŠ {group_id} ä¸åœ¨é»‘åå•ä¸­")
                return

            self.group_blacklist.remove(group_id)
            self._save_group_blacklist()
            yield event.plain_result(f"âœ… å·²ä»é»‘åå•ç§»é™¤ç¾¤èŠ {group_id}")

        # æ¸…ç©ºé»‘åå•
        elif action == "clear":
            if not self.group_blacklist:
                yield event.plain_result("é»‘åå•å·²ä¸ºç©º")
                return

            self.group_blacklist.clear()
            self._save_group_blacklist()
            yield event.plain_result("âœ… å·²æ¸…ç©ºç¾¤èŠé»‘åå•")

        # æ— æ•ˆæ“ä½œ
        else:
            yield event.plain_result(
                "æ— æ•ˆçš„æ“ä½œï¼Œè¯·ä½¿ç”¨: add <ç¾¤å·>, remove <ç¾¤å·>, clear"
            )

    @filter.permission_type(filter.PermissionType.ADMIN)
    @filter.command("web_cache", alias={"ç½‘é¡µç¼“å­˜", "æ¸…ç†ç¼“å­˜"})
    async def manage_cache(self, event: AstrMessageEvent):
        """ç®¡ç†æ’ä»¶çš„ç½‘é¡µåˆ†æç»“æœç¼“å­˜

        è¿™ä¸ªå‘½ä»¤å…è®¸ç”¨æˆ·ç®¡ç†æ’ä»¶çš„ç¼“å­˜ï¼ŒåŒ…æ‹¬æŸ¥çœ‹ç¼“å­˜çŠ¶æ€å’Œæ¸…ç©ºç¼“å­˜ï¼Œ
        æ–¹ä¾¿ç”¨æˆ·æ§åˆ¶ç¼“å­˜çš„ä½¿ç”¨å’Œé‡Šæ”¾å­˜å‚¨ç©ºé—´ã€‚

        ğŸ“‹ å‘½ä»¤ç”¨æ³•ï¼š
        1. ğŸ” æŸ¥çœ‹ç¼“å­˜çŠ¶æ€ï¼š`/web_cache`
        2. ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ï¼š`/web_cache clear`

        ğŸ”¤ æ”¯æŒåˆ«åï¼š
        - `/ç½‘é¡µç¼“å­˜`
        - `/æ¸…ç†ç¼“å­˜`

        ğŸ’¡ ç¼“å­˜è¯´æ˜ï¼š
        - â° ç¼“å­˜æœ‰æ•ˆæœŸï¼šç”±é…ç½®ä¸­çš„`cache_expire_time`å†³å®š
        - ğŸ“Š æœ€å¤§ç¼“å­˜æ•°é‡ï¼šç”±é…ç½®ä¸­çš„`max_cache_size`å†³å®š
        - ğŸ“¦ ç¼“å­˜å†…å®¹ï¼šåŒ…æ‹¬ç½‘é¡µåˆ†æç»“æœå’Œæˆªå›¾æ•°æ®

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼Œç”¨äºè·å–å‘½ä»¤å‚æ•°å’Œç”Ÿæˆå“åº”
        """
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()

        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰ç¼“å­˜çŠ¶æ€
        if len(message_parts) <= 1:
            cache_stats = self.cache_manager.get_stats()
            cache_info = "**å½“å‰ç¼“å­˜çŠ¶æ€**\n\n"
            cache_info += f"- ç¼“å­˜æ€»æ•°: {cache_stats['total']} ä¸ª\n"
            cache_info += f"- æœ‰æ•ˆç¼“å­˜: {cache_stats['valid']} ä¸ª\n"
            cache_info += f"- è¿‡æœŸç¼“å­˜: {cache_stats['expired']} ä¸ª\n"
            cache_info += f"- ç¼“å­˜è¿‡æœŸæ—¶é—´: {self.cache_expire_time} åˆ†é’Ÿ\n"
            cache_info += f"- æœ€å¤§ç¼“å­˜æ•°é‡: {self.max_cache_size} ä¸ª\n"
            cache_info += (
                f"- ç¼“å­˜åŠŸèƒ½: {'âœ… å·²å¯ç”¨' if self.enable_cache else 'âŒ å·²ç¦ç”¨'}\n"
            )

            cache_info += "\nä½¿ç”¨ `/web_cache clear` æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"

            yield event.plain_result(cache_info)
            return

        # è§£ææ“ä½œç±»å‹
        action = message_parts[1].lower() if len(message_parts) > 1 else ""

        # æ¸…ç©ºç¼“å­˜æ“ä½œ
        if action == "clear":
            # æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
            self.cache_manager.clear()
            cache_stats = self.cache_manager.get_stats()
            yield event.plain_result(
                f"âœ… å·²æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ï¼Œå½“å‰ç¼“å­˜æ•°é‡: {cache_stats['total']} ä¸ª"
            )

        # æ— æ•ˆæ“ä½œ
        else:
            yield event.plain_result("æ— æ•ˆçš„æ“ä½œï¼Œè¯·ä½¿ç”¨: clear")

    @filter.command("web_export", alias={"å¯¼å‡ºåˆ†æç»“æœ", "ç½‘é¡µå¯¼å‡º"})
    async def export_analysis_result(self, event: AstrMessageEvent):
        """å¯¼å‡ºç½‘é¡µåˆ†æç»“æœ

        è¿™ä¸ªå‘½ä»¤å…è®¸ç”¨æˆ·å¯¼å‡ºç½‘é¡µåˆ†æç»“æœï¼Œæ”¯æŒå¤šç§æ ¼å¼å’Œå¯¼å‡ºèŒƒå›´ï¼Œ
        æ–¹ä¾¿ç”¨æˆ·ä¿å­˜å’Œåˆ†äº«åˆ†æç»“æœã€‚

        ğŸ“‹ å‘½ä»¤ç”¨æ³•ï¼š
        1. ğŸ“„ å¯¼å‡ºå•ä¸ªURLï¼š`/web_export https://example.com [æ ¼å¼]`
        2. ğŸ“š å¯¼å‡ºæ‰€æœ‰ç¼“å­˜ï¼š`/web_export all [æ ¼å¼]`

        ğŸ“ æ”¯æŒçš„æ ¼å¼ï¼š
        - ğŸ“ `md/markdown`ï¼šMarkdownæ ¼å¼ï¼Œé€‚åˆé˜…è¯»å’Œç¼–è¾‘
        - ğŸ“Š `json`ï¼šJSONæ ¼å¼ï¼Œé€‚åˆç¨‹åºå¤„ç†
        - ğŸ“„ `txt`ï¼šçº¯æ–‡æœ¬æ ¼å¼ï¼Œé€‚åˆç®€å•æŸ¥çœ‹

        ğŸ”¤ æ”¯æŒåˆ«åï¼š
        - `/å¯¼å‡ºåˆ†æç»“æœ`
        - `/ç½‘é¡µå¯¼å‡º`

        âœ¨ åŠŸèƒ½ç‰¹ç‚¹ï¼š
        - ğŸ”„ æ”¯æŒå¯¼å‡ºå•ä¸ªURLçš„åˆ†æç»“æœ
        - ğŸ“š æ”¯æŒå¯¼å‡ºæ‰€æœ‰ç¼“å­˜çš„åˆ†æç»“æœ
        - ğŸ“¤ å¯¼å‡ºæ–‡ä»¶ä¼šè‡ªåŠ¨å‘é€ç»™ç”¨æˆ·
        - ğŸ” å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ç»“æœï¼Œä¼šå…ˆè¿›è¡Œåˆ†æ
        - ğŸ“¦ æ”¯æŒå¤šç§å¯¼å‡ºæ ¼å¼ï¼Œæ»¡è¶³ä¸åŒéœ€æ±‚

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼Œç”¨äºè·å–å‘½ä»¤å‚æ•°å’Œç”Ÿæˆå“åº”
        """
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()

        # æ£€æŸ¥å‚æ•°æ˜¯å¦è¶³å¤Ÿ
        if len(message_parts) < 2:
            yield event.plain_result(
                "è¯·æä¾›è¦å¯¼å‡ºçš„URLé“¾æ¥å’Œæ ¼å¼ï¼Œä¾‹å¦‚ï¼š/web_export https://example.com md æˆ– /web_export all json"
            )
            return

        # è·å–å¯¼å‡ºèŒƒå›´å’Œæ ¼å¼
        url_or_all = message_parts[1]
        format_type = message_parts[2] if len(message_parts) > 2 else "md"

        # éªŒè¯æ ¼å¼ç±»å‹æ˜¯å¦æ”¯æŒ
        supported_formats = ["md", "markdown", "json", "txt"]
        if format_type.lower() not in supported_formats:
            yield event.plain_result(
                f"ä¸æ”¯æŒçš„æ ¼å¼ç±»å‹ï¼Œè¯·ä½¿ç”¨ï¼š{', '.join(supported_formats)}"
            )
            return

        # å‡†å¤‡å¯¼å‡ºæ•°æ®
        export_results = []

        if url_or_all.lower() == "all":
            # å¯¼å‡ºæ‰€æœ‰ç¼“å­˜çš„åˆ†æç»“æœ
            if not self.cache_manager.memory_cache:
                yield event.plain_result("å½“å‰æ²¡æœ‰ç¼“å­˜çš„åˆ†æç»“æœ")
                return

            for url, cache_data in self.cache_manager.memory_cache.items():
                export_results.append({"url": url, "result": cache_data["result"]})
        else:
            # å¯¼å‡ºæŒ‡å®šURLçš„åˆ†æç»“æœ
            url = url_or_all

            # æ£€æŸ¥URLæ ¼å¼æ˜¯å¦æœ‰æ•ˆ
            if not self.analyzer.is_valid_url(url):
                yield event.plain_result("æ— æ•ˆçš„URLé“¾æ¥")
                return

            # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰è¯¥URLçš„åˆ†æç»“æœ
            cached_result = self._check_cache(url)
            if cached_result:
                export_results.append({"url": url, "result": cached_result})
            else:
                # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå…ˆè¿›è¡Œåˆ†æ
                yield event.plain_result("ç¼“å­˜ä¸­æ²¡æœ‰è¯¥URLçš„åˆ†æç»“æœï¼Œæ­£åœ¨è¿›è¡Œåˆ†æ...")

                # æŠ“å–å¹¶åˆ†æç½‘é¡µ
                async with WebAnalyzer(
                    self.max_content_length,
                    self.timeout,
                    self.user_agent,
                    self.proxy,
                    self.retry_count,
                    self.retry_delay,
                ) as analyzer:
                    html = await analyzer.fetch_webpage(url)
                    if not html:
                        yield event.plain_result(f"æ— æ³•æŠ“å–ç½‘é¡µå†…å®¹: {url}")
                        return

                    content_data = analyzer.extract_content(html, url)
                    if not content_data:
                        yield event.plain_result(f"æ— æ³•è§£æç½‘é¡µå†…å®¹: {url}")
                        return

                    # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
                    if self.enable_translation:
                        translated_content = await self._translate_content(
                            event, content_data["content"]
                        )
                        translated_content_data = content_data.copy()
                        translated_content_data["content"] = translated_content
                        analysis_result = await self.analyze_with_llm(
                            event, translated_content_data
                        )
                    else:
                        analysis_result = await self.analyze_with_llm(
                            event, content_data
                        )

                    # æå–ç‰¹å®šå†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    specific_content = self._extract_specific_content(html, url)
                    if specific_content:
                        # åœ¨åˆ†æç»“æœä¸­æ·»åŠ ç‰¹å®šå†…å®¹
                        specific_content_str = "\n\n**ç‰¹å®šå†…å®¹æå–**\n"

                        if "images" in specific_content and specific_content["images"]:
                            specific_content_str += (
                                f"\nğŸ“· å›¾ç‰‡é“¾æ¥ ({len(specific_content['images'])}):\n"
                            )
                            for img_url in specific_content["images"]:
                                specific_content_str += f"- {img_url}\n"

                        if "links" in specific_content and specific_content["links"]:
                            specific_content_str += (
                                f"\nğŸ”— ç›¸å…³é“¾æ¥ ({len(specific_content['links'])}):\n"
                            )
                            for link in specific_content["links"][
                                :5
                            ]:  # åªæ˜¾ç¤ºå‰5ä¸ªé“¾æ¥
                                specific_content_str += (
                                    f"- [{link['text']}]({link['url']})\n"
                                )

                        if (
                            "code_blocks" in specific_content
                            and specific_content["code_blocks"]
                        ):
                            specific_content_str += f"\nğŸ’» ä»£ç å— ({len(specific_content['code_blocks'])}):\n"
                            for i, code in enumerate(
                                specific_content["code_blocks"][:2]
                            ):  # åªæ˜¾ç¤ºå‰2ä¸ªä»£ç å—
                                specific_content_str += f"```\n{code}\n```\n"

                        analysis_result += specific_content_str

                    # å‡†å¤‡å¯¼å‡ºæ•°æ®
                    export_results.append(
                        {
                            "url": url,
                            "result": {
                                "url": url,
                                "result": analysis_result,
                                "screenshot": None,
                            },
                        }
                    )

        # æ‰§è¡Œå¯¼å‡ºæ“ä½œ
        try:
            import os
            import json
            import time

            # åˆ›å»ºdataç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            data_dir = os.path.join(os.path.dirname(__file__), "data")
            os.makedirs(data_dir, exist_ok=True)

            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = int(time.time())
            if len(export_results) == 1:
                # å•ä¸ªURLå¯¼å‡ºï¼Œä½¿ç”¨åŸŸåä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†
                url = export_results[0]["url"]
                from urllib.parse import urlparse

                parsed = urlparse(url)
                domain = parsed.netloc.replace(".", "_")
                filename = f"web_analysis_{domain}_{timestamp}"
            else:
                # å¤šä¸ªURLå¯¼å‡º
                filename = f"web_analysis_all_{timestamp}"

            # ç¡®å®šæ–‡ä»¶æ‰©å±•å
            file_extension = format_type.lower()
            if file_extension == "markdown":
                file_extension = "md"

            file_path = os.path.join(data_dir, f"{filename}.{file_extension}")

            if format_type.lower() in ["md", "markdown"]:
                # ç”ŸæˆMarkdownæ ¼å¼å†…å®¹
                md_content = "# ç½‘é¡µåˆ†æç»“æœå¯¼å‡º\n\n"
                md_content += f"å¯¼å‡ºæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}\n\n"
                md_content += f"å…± {len(export_results)} ä¸ªåˆ†æç»“æœ\n\n"
                md_content += "---\n\n"

                for i, export_item in enumerate(export_results, 1):
                    url = export_item["url"]
                    result_data = export_item["result"]

                    md_content += f"## {i}. {url}\n\n"
                    md_content += result_data["result"]
                    md_content += "\n\n"
                    md_content += "---\n\n"

                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(md_content)

            elif format_type.lower() == "json":
                # ç”ŸæˆJSONæ ¼å¼å†…å®¹
                json_data = {
                    "export_time": timestamp,
                    "export_time_str": time.strftime(
                        "%Y-%m-%d %H:%M:%S", time.localtime(timestamp)
                    ),
                    "total_results": len(export_results),
                    "results": [],
                }

                for export_item in export_results:
                    url = export_item["url"]
                    result_data = export_item["result"]

                    json_data["results"].append(
                        {
                            "url": url,
                            "analysis_result": result_data["result"],
                            "has_screenshot": result_data["screenshot"] is not None,
                        }
                    )

                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    json.dump(json_data, f, ensure_ascii=False, indent=2)

            elif format_type.lower() == "txt":
                # ç”Ÿæˆçº¯æ–‡æœ¬æ ¼å¼å†…å®¹
                txt_content = "ç½‘é¡µåˆ†æç»“æœå¯¼å‡º\n"
                txt_content += f"å¯¼å‡ºæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}\n"
                txt_content += f"å…± {len(export_results)} ä¸ªåˆ†æç»“æœ\n"
                txt_content += "=" * 50 + "\n\n"

                for i, export_item in enumerate(export_results, 1):
                    url = export_item["url"]
                    result_data = export_item["result"]

                    txt_content += f"{i}. {url}\n"
                    txt_content += "-" * 30 + "\n"
                    txt_content += result_data["result"]
                    txt_content += "\n\n"
                    txt_content += "=" * 50 + "\n\n"

                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(txt_content)

            # å‘é€å¯¼å‡ºæˆåŠŸæ¶ˆæ¯ï¼Œå¹¶é™„å¸¦å¯¼å‡ºæ–‡ä»¶
            from astrbot.api.message_components import Plain, File

            # æ„å»ºæ¶ˆæ¯é“¾
            message_chain = [
                Plain("âœ… åˆ†æç»“æœå¯¼å‡ºæˆåŠŸï¼\n\n"),
                Plain(f"å¯¼å‡ºæ ¼å¼: {format_type}\n"),
                Plain(f"å¯¼å‡ºæ•°é‡: {len(export_results)}\n\n"),
                Plain("ğŸ“ å¯¼å‡ºæ–‡ä»¶ï¼š\n"),
                File(file=file_path, name=os.path.basename(file_path)),
            ]

            yield event.chain_result(message_chain)

            logger.info(
                f"æˆåŠŸå¯¼å‡º {len(export_results)} ä¸ªåˆ†æç»“æœåˆ° {file_path}ï¼Œå¹¶å‘é€ç»™ç”¨æˆ·"
            )

        except Exception as e:
            logger.error(f"å¯¼å‡ºåˆ†æç»“æœå¤±è´¥: {e}")
            yield event.plain_result(f"âŒ å¯¼å‡ºåˆ†æç»“æœå¤±è´¥: {str(e)}")

    def _save_group_blacklist(self):
        """ä¿å­˜ç¾¤èŠé»‘åå•åˆ°é…ç½®æ–‡ä»¶

        è¯¥æ–¹æ³•è´Ÿè´£å°†ç¾¤èŠé»‘åå•åˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œå¹¶ä¿å­˜åˆ°é…ç½®ä¸­

        åŠŸèƒ½è¯´æ˜ï¼š
        - å°†ç¾¤èŠIDåˆ—è¡¨è½¬æ¢ä¸ºæ¢è¡Œåˆ†éš”çš„æ–‡æœ¬
        - æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„group_settings.group_blacklistå­—æ®µ
        - ä¿å­˜é…ç½®æ›´æ”¹
        - å¤„ç†ä¿å­˜è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„å¼‚å¸¸
        """
        try:
            # å°†ç¾¤èŠåˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªç¾¤èŠID
            group_text = "\n".join(self.group_blacklist)
            # è·å–å½“å‰group_settingsé…ç½®
            group_settings = self.config.get("group_settings", {})
            # æ›´æ–°group_blacklist
            group_settings["group_blacklist"] = group_text
            # æ›´æ–°é…ç½®å¹¶ä¿å­˜åˆ°æ–‡ä»¶
            self.config["group_settings"] = group_settings
            self.config.save_config()
        except Exception as e:
            logger.error(f"ä¿å­˜ç¾¤èŠé»‘åå•å¤±è´¥: {e}")

    def _check_cache(self, url: str) -> dict:
        """æ£€æŸ¥æŒ‡å®šURLçš„ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ

        Args:
            url: è¦æ£€æŸ¥ç¼“å­˜çš„ç½‘é¡µURL

        Returns:
            - å¦‚æœç¼“å­˜å­˜åœ¨ä¸”æœ‰æ•ˆï¼Œè¿”å›ç¼“å­˜çš„åˆ†æç»“æœ
            - å¦‚æœç¼“å­˜ä¸å­˜åœ¨æˆ–æ— æ•ˆï¼Œè¿”å›None
        """
        if not self.enable_cache:
            return None
        
        # è§„èŒƒåŒ–URLï¼Œç»Ÿä¸€æ ¼å¼
        normalized_url = self.analyzer.normalize_url(url)
        return self.cache_manager.get(normalized_url)

    def _update_cache(self, url: str, result: dict, content: str = None):
        """æ›´æ–°æŒ‡å®šURLçš„ç¼“å­˜ï¼Œæ”¯æŒåŸºäºå†…å®¹å“ˆå¸Œçš„ç¼“å­˜ç­–ç•¥

        Args:
            url: è¦æ›´æ–°ç¼“å­˜çš„ç½‘é¡µURL
            result: åŒ…å«åˆ†æç»“æœçš„å­—å…¸ï¼Œæ ¼å¼ä¸_check_cacheè¿”å›å€¼ä¸€è‡´
            content: ç½‘é¡µå†…å®¹ï¼Œç”¨äºåŸºäºå†…å®¹å“ˆå¸Œçš„ç¼“å­˜
        """
        if not self.enable_cache:
            return
        
        # è§„èŒƒåŒ–URLï¼Œç»Ÿä¸€æ ¼å¼
        normalized_url = self.analyzer.normalize_url(url)
        
        # å¦‚æœæä¾›äº†å†…å®¹ï¼Œä½¿ç”¨åŸºäºå†…å®¹å“ˆå¸Œçš„ç¼“å­˜ç­–ç•¥
        if content:
            self.cache_manager.set_with_content_hash(normalized_url, result, content)
        else:
            # å¦åˆ™ä½¿ç”¨æ™®é€šçš„URLç¼“å­˜ç­–ç•¥
            self.cache_manager.set(normalized_url, result)

    def _clean_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜

        æ³¨æ„ï¼šç¼“å­˜ç®¡ç†å™¨ä¼šè‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜ï¼Œå› æ­¤è¯¥æ–¹æ³•ç›®å‰ç•™ç©º
        å¦‚éœ€æ‰‹åŠ¨æ¸…ç†ï¼Œå¯ä»¥åœ¨æ­¤æ–¹æ³•ä¸­æ·»åŠ ç›¸åº”é€»è¾‘
        """
        # ç¼“å­˜ç®¡ç†å™¨ä¼šè‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜ï¼Œè¿™é‡Œç•™ç©ºå³å¯
        pass

    async def _translate_content(self, event: AstrMessageEvent, content: str) -> str:
        """ç¿»è¯‘ç½‘é¡µå†…å®¹

        è¯¥æ–¹æ³•è´Ÿè´£è°ƒç”¨LLMå¯¹ç½‘é¡µå†…å®¹è¿›è¡Œç¿»è¯‘

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
            content: è¦ç¿»è¯‘çš„ç½‘é¡µå†…å®¹

        Returns:
            - å¦‚æœç¿»è¯‘æˆåŠŸï¼Œè¿”å›ç¿»è¯‘åçš„å†…å®¹
            - å¦‚æœç¿»è¯‘å¤±è´¥æˆ–æœªå¯ç”¨ç¿»è¯‘ï¼Œè¿”å›åŸå§‹å†…å®¹
        """
        if not self.enable_translation:
            return content

        try:
            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨
            if not hasattr(self.context, "llm_generate"):
                logger.error("LLMä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘")
                return content

            # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„LLMæä¾›å•†ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨å½“å‰ä¼šè¯çš„æ¨¡å‹
            provider_id = self.llm_provider
            if not provider_id:
                umo = event.unified_msg_origin
                provider_id = await self.context.get_current_chat_provider_id(umo=umo)

            if not provider_id:
                logger.error("æ— æ³•è·å–LLMæä¾›å•†IDï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘")
                return content

            # ä½¿ç”¨è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯æˆ–é»˜è®¤æç¤ºè¯
            if self.custom_translation_prompt:
                # æ›¿æ¢è‡ªå®šä¹‰æç¤ºè¯ä¸­çš„å˜é‡
                prompt = self.custom_translation_prompt.format(
                    content=content, target_language=self.target_language
                )
            else:
                # é»˜è®¤ç¿»è¯‘æç¤ºè¯
                prompt = f"è¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆ{self.target_language}è¯­è¨€ï¼Œä¿æŒåŸæ–‡æ„æ€ä¸å˜ï¼Œè¯­è¨€æµç•…è‡ªç„¶ï¼š\n\n{content}"

            # è°ƒç”¨LLMè¿›è¡Œç¿»è¯‘
            llm_resp = await self.context.llm_generate(
                chat_provider_id=provider_id, prompt=prompt
            )

            if llm_resp and llm_resp.completion_text:
                return llm_resp.completion_text.strip()
            else:
                logger.error("LLMç¿»è¯‘è¿”å›ä¸ºç©º")
                return content
        except Exception as e:
            logger.error(f"ç¿»è¯‘å†…å®¹å¤±è´¥: {e}")
            return content

    def _extract_specific_content(self, html: str, url: str) -> dict:
        """æå–ç‰¹å®šç±»å‹çš„å†…å®¹

        è¯¥æ–¹æ³•è´Ÿè´£ä»HTMLä¸­æå–ç‰¹å®šç±»å‹çš„å†…å®¹ï¼Œå¦‚å›¾ç‰‡ã€é“¾æ¥ã€ä»£ç å—ç­‰

        Args:
            html: ç½‘é¡µHTMLå†…å®¹
            url: ç½‘é¡µURL

        Returns:
            - å¦‚æœæå–æˆåŠŸï¼Œè¿”å›åŒ…å«ç‰¹å®šå†…å®¹çš„å­—å…¸
            - å¦‚æœæå–å¤±è´¥æˆ–æœªå¯ç”¨ç‰¹å®šå†…å®¹æå–ï¼Œè¿”å›ç©ºå­—å…¸
        """
        if not self.enable_specific_extraction:
            return {}

        try:
            # ç›´æ¥ä½¿ç”¨å·²æœ‰analyzerå®ä¾‹ï¼Œé¿å…é‡å¤åˆ›å»º
            return self.analyzer.extract_specific_content(html, url, self.extract_types)
        except Exception as e:
            logger.error(f"æå–ç‰¹å®šå†…å®¹å¤±è´¥: {e}")
            return {}

    async def _send_analysis_result(self, event, analysis_results):
        """å‘é€åˆ†æç»“æœï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨åˆå¹¶è½¬å‘

        è¯¥æ–¹æ³•è´Ÿè´£å°†åˆ†æç»“æœå‘é€ç»™ç”¨æˆ·ï¼Œæ”¯æŒæ™®é€šæ¶ˆæ¯å’Œåˆå¹¶è½¬å‘ä¸¤ç§æ–¹å¼

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
            analysis_results: åŒ…å«æ‰€æœ‰åˆ†æç»“æœçš„åˆ—è¡¨
        """
        try:
            from astrbot.api.message_components import Node, Plain, Nodes, Image
            import tempfile
            import os

            # æ£€æŸ¥æ˜¯å¦ä¸ºç¾¤èŠæ¶ˆæ¯ä¸”åˆå¹¶è½¬å‘åŠŸèƒ½å·²å¯ç”¨
            group_id = None
            if hasattr(event, "group_id") and event.group_id:
                group_id = event.group_id
            elif (
                hasattr(event, "message_obj")
                and hasattr(event.message_obj, "group_id")
                and event.message_obj.group_id
            ):
                group_id = event.message_obj.group_id

            # æ ¹æ®æ¶ˆæ¯ç±»å‹å†³å®šæ˜¯å¦ä½¿ç”¨åˆå¹¶è½¬å‘
            is_group = bool(group_id)
            is_private = not is_group

            # å¦‚æœæ˜¯ç¾¤èŠä¸”ç¾¤èŠåˆå¹¶è½¬å‘å·²å¯ç”¨ï¼Œæˆ–è€…æ˜¯ç§èŠä¸”ç§èŠåˆå¹¶è½¬å‘å·²å¯ç”¨ï¼Œä¸”ä¸æ˜¯åªå‘é€æˆªå›¾
            if (self.send_content_type != "screenshot_only") and (
                (is_group and self.merge_forward_enabled["group"])
                or (is_private and self.merge_forward_enabled["private"])
            ):
                # ä½¿ç”¨åˆå¹¶è½¬å‘ - å°†æ‰€æœ‰åˆ†æç»“æœåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
                nodes = []

                # æ·»åŠ æ€»æ ‡é¢˜èŠ‚ç‚¹
                total_title_node = Node(
                    uin=event.get_sender_id(),
                    name="ç½‘é¡µåˆ†æç»“æœæ±‡æ€»",
                    content=[Plain(f"å…±{len(analysis_results)}ä¸ªç½‘é¡µåˆ†æç»“æœ")],
                )
                nodes.append(total_title_node)

                # ä¸ºæ¯ä¸ªURLæ·»åŠ åˆ†æç»“æœèŠ‚ç‚¹
                for i, result_data in enumerate(analysis_results, 1):
                    url = result_data["url"]
                    analysis_result = result_data["result"]
                    screenshot = result_data.get("screenshot")

                    # æ·»åŠ å½“å‰URLçš„æ ‡é¢˜èŠ‚ç‚¹
                    url_title_node = Node(
                        uin=event.get_sender_id(),
                        name=f"åˆ†æç»“æœ {i}",
                        content=[Plain(f"ç¬¬{i}ä¸ªç½‘é¡µåˆ†æç»“æœ - {url}")],
                    )
                    nodes.append(url_title_node)

                    # å¤„ç†æˆªå›¾ï¼Œå‡†å¤‡åˆ›å»ºå›¾ç‰‡ç»„ä»¶
                    image_component = None
                    if (
                        self.merge_forward_enabled.get("include_screenshot", False)
                        and screenshot
                        and self.send_content_type != "analysis_only"
                    ):
                        try:
                            # æ ¹æ®æˆªå›¾æ ¼å¼è®¾ç½®æ–‡ä»¶åç¼€
                            suffix = (
                                f".{self.screenshot_format}"
                                if self.screenshot_format
                                else ".jpg"
                            )
                            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æˆªå›¾
                            with tempfile.NamedTemporaryFile(
                                suffix=suffix, delete=False
                            ) as temp_file:
                                temp_file.write(screenshot)
                                temp_file_path = temp_file.name

                            # åˆ›å»ºå›¾ç‰‡ç»„ä»¶
                            image_component = Image.fromFileSystem(temp_file_path)

                            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶è·¯å¾„ï¼Œä»¥ä¾¿åç»­æ¸…ç†
                            if "temp_files" not in locals():
                                temp_files = []
                            temp_files.append(temp_file_path)
                        except Exception as e:
                            logger.error(f"å¤„ç†æˆªå›¾å¤±è´¥: {e}")
                            # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
                            if "temp_file_path" in locals() and os.path.exists(
                                temp_file_path
                            ):
                                os.unlink(temp_file_path)

                    # æ ¹æ®å‘é€å†…å®¹ç±»å‹å†³å®šæ˜¯å¦æ·»åŠ åˆ†æç»“æœèŠ‚ç‚¹
                    if self.send_content_type != "screenshot_only":
                        content = [Plain(analysis_result)]
                        content_node = Node(
                            uin=event.get_sender_id(),
                            name="è¯¦ç»†åˆ†æ",
                            content=content,
                        )
                        nodes.append(content_node)

                    # å¦‚æœå¯ç”¨äº†åˆå¹¶è½¬å‘åŒ…å«æˆªå›¾åŠŸèƒ½ï¼Œå¹¶ä¸”æœ‰æˆªå›¾ï¼Œä¸”éœ€è¦å‘é€æˆªå›¾ï¼Œåˆ™åˆ›å»ºå•ç‹¬çš„æˆªå›¾èŠ‚ç‚¹
                    if (
                        self.merge_forward_enabled.get("include_screenshot", False)
                        and screenshot
                        and self.send_content_type != "analysis_only"
                    ):
                        try:
                            # åˆ›å»ºå•ç‹¬çš„æˆªå›¾èŠ‚ç‚¹
                            screenshot_node = Node(
                                uin=event.get_sender_id(),
                                name="ç½‘é¡µæˆªå›¾",
                                content=[image_component],
                            )
                            nodes.append(screenshot_node)
                        except Exception as e:
                            logger.error(f"åˆ›å»ºæˆªå›¾èŠ‚ç‚¹å¤±è´¥: {e}")

                # ä½¿ç”¨NodesåŒ…è£…æ‰€æœ‰èŠ‚ç‚¹ï¼Œåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
                merge_forward_message = Nodes(nodes)

                # å‘é€åˆå¹¶è½¬å‘æ¶ˆæ¯
                yield event.chain_result([merge_forward_message])

                # å¦‚æœæœªå¯ç”¨åˆå¹¶è½¬å‘åŒ…å«æˆªå›¾åŠŸèƒ½ï¼Œä¸”éœ€è¦å‘é€æˆªå›¾ï¼Œåˆ™é€ä¸ªå‘é€æˆªå›¾
                if (
                    not self.merge_forward_enabled.get("include_screenshot", False)
                    and self.send_content_type != "analysis_only"
                ):
                    for result_data in analysis_results:
                        screenshot = result_data.get("screenshot")
                        if screenshot:
                            try:
                                # æ ¹æ®æˆªå›¾æ ¼å¼è®¾ç½®æ–‡ä»¶åç¼€
                                suffix = (
                                    f".{self.screenshot_format}"
                                    if self.screenshot_format
                                    else ".jpg"
                                )
                                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æˆªå›¾
                                with tempfile.NamedTemporaryFile(
                                    suffix=suffix, delete=False
                                ) as temp_file:
                                    temp_file.write(screenshot)
                                    temp_file_path = temp_file.name

                                # ä½¿ç”¨Image.fromFileSystem()æ–¹æ³•å‘é€å›¾ç‰‡
                                image_component = Image.fromFileSystem(temp_file_path)
                                yield event.chain_result([image_component])
                                logger.info(
                                    f"ç¾¤èŠ {group_id} ä½¿ç”¨åˆå¹¶è½¬å‘å‘é€åˆ†æç»“æœï¼Œå¹¶å‘é€æˆªå›¾"
                                )

                                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                                os.unlink(temp_file_path)
                            except Exception as e:
                                logger.error(f"å‘é€æˆªå›¾å¤±è´¥: {e}")
                                # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
                                if "temp_file_path" in locals() and os.path.exists(
                                    temp_file_path
                                ):
                                    os.unlink(temp_file_path)
                            if "temp_file_path" in locals() and os.path.exists(
                                temp_file_path
                            ):
                                os.unlink(temp_file_path)
                # æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
                if "temp_files" in locals():
                    for temp_file_path in temp_files:
                        try:
                            if os.path.exists(temp_file_path):
                                os.unlink(temp_file_path)
                        except Exception as e:
                            logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
                logger.info(
                    f"ç¾¤èŠ {group_id} ä½¿ç”¨åˆå¹¶è½¬å‘å‘é€{len(analysis_results)}ä¸ªåˆ†æç»“æœ"
                )
            else:
                # æ™®é€šå‘é€
                for i, result_data in enumerate(analysis_results, 1):
                    screenshot = result_data.get("screenshot")
                    analysis_result = result_data.get("result")

                    # å¦‚æœåªå‘é€æˆªå›¾
                    if self.send_content_type == "screenshot_only":
                        if screenshot:
                            try:
                                # æ ¹æ®æˆªå›¾æ ¼å¼è®¾ç½®æ–‡ä»¶åç¼€
                                suffix = (
                                    f".{self.screenshot_format}"
                                    if self.screenshot_format
                                    else ".jpg"
                                )
                                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æˆªå›¾
                                with tempfile.NamedTemporaryFile(
                                    suffix=suffix, delete=False
                                ) as temp_file:
                                    temp_file.write(screenshot)
                                    temp_file_path = temp_file.name

                                # ä½¿ç”¨Image.fromFileSystem()æ–¹æ³•å‘é€å›¾ç‰‡
                                image_component = Image.fromFileSystem(temp_file_path)
                                yield event.chain_result([image_component])
                                logger.info("åªå‘é€æˆªå›¾")

                                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                                os.unlink(temp_file_path)
                            except Exception as e:
                                logger.error(f"å‘é€æˆªå›¾å¤±è´¥: {e}")
                                # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
                                if "temp_file_path" in locals() and os.path.exists(
                                    temp_file_path
                                ):
                                    os.unlink(temp_file_path)
                    # å‘é€åˆ†æç»“æœæˆ–ä¸¤è€…éƒ½å‘é€
                    else:
                        url = result_data["url"]
                        # æ ¹æ®å‘é€å†…å®¹ç±»å‹å†³å®šæ˜¯å¦å‘é€åˆ†æç»“æœæ–‡æœ¬
                        if self.send_content_type != "screenshot_only":
                            if len(analysis_results) == 1:
                                result_text = f"ç½‘é¡µåˆ†æç»“æœï¼š\n{analysis_result}"
                            else:
                                result_text = f"ç¬¬{i}/{len(analysis_results)}ä¸ªç½‘é¡µåˆ†æç»“æœï¼š\n{analysis_result}"
                            yield event.plain_result(result_text)

                        # æ ¹æ®å‘é€å†…å®¹ç±»å‹å†³å®šæ˜¯å¦å‘é€æˆªå›¾
                        if screenshot and self.send_content_type != "analysis_only":
                            try:
                                # æ ¹æ®æˆªå›¾æ ¼å¼è®¾ç½®æ–‡ä»¶åç¼€
                                suffix = (
                                    f".{self.screenshot_format}"
                                    if self.screenshot_format
                                    else ".jpg"
                                )
                                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æˆªå›¾
                                with tempfile.NamedTemporaryFile(
                                    suffix=suffix, delete=False
                                ) as temp_file:
                                    temp_file.write(screenshot)
                                    temp_file_path = temp_file.name

                                # ä½¿ç”¨Image.fromFileSystem()æ–¹æ³•å‘é€å›¾ç‰‡
                                image_component = Image.fromFileSystem(temp_file_path)
                                yield event.chain_result([image_component])
                                logger.info("æ™®é€šå‘é€åˆ†æç»“æœï¼Œå¹¶å‘é€æˆªå›¾")

                                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                                os.unlink(temp_file_path)
                            except Exception as e:
                                logger.error(f"å‘é€æˆªå›¾å¤±è´¥: {e}")
                                # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
                                if "temp_file_path" in locals() and os.path.exists(
                                    temp_file_path
                                ):
                                    os.unlink(temp_file_path)
                message_type = "ç¾¤èŠ" if group_id else "ç§èŠ"
                logger.info(
                    f"{message_type}æ¶ˆæ¯æ™®é€šå‘é€{len(analysis_results)}ä¸ªåˆ†æç»“æœ"
                )
        except Exception as e:
            logger.error(f"å‘é€åˆ†æç»“æœå¤±è´¥: {e}")
            yield event.plain_result(f"âŒ å‘é€åˆ†æç»“æœå¤±è´¥: {str(e)}")

    async def terminate(self):
        """æ’ä»¶å¸è½½æ—¶çš„æ¸…ç†å·¥ä½œ"""
        logger.info("ç½‘é¡µåˆ†ææ’ä»¶å·²å¸è½½")
