"""
AstrBot ç½‘é¡µåˆ†ææ’ä»¶

è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„AstrBotæ’ä»¶ï¼Œä¸“é—¨ç”¨äºç½‘é¡µå†…å®¹çš„æ™ºèƒ½åˆ†æå’Œæ€»ç»“ã€‚

âœ¨ æ ¸å¿ƒåŠŸèƒ½
- ğŸ¤– è‡ªåŠ¨è¯†åˆ«æ¶ˆæ¯ä¸­çš„ç½‘é¡µé“¾æ¥ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡ä»¤
- ğŸŒ æ™ºèƒ½æŠ“å–å’Œè§£æç½‘é¡µå†…å®¹ï¼Œæ”¯æŒå¤šç§ç½‘ç«™ç»“æ„
- ğŸ§  é›†æˆå¤§è¯­è¨€æ¨¡å‹(LLM)ï¼Œæä¾›æ·±åº¦åˆ†æå’Œæ€»ç»“
- ğŸ“¸ æ”¯æŒç½‘é¡µæˆªå›¾ï¼Œç›´è§‚å±•ç¤ºç½‘é¡µå†…å®¹
- ğŸ”„ å†…ç½®ç¼“å­˜æœºåˆ¶ï¼Œæå‡é‡å¤è®¿é—®çš„å“åº”é€Ÿåº¦
- ğŸ“ æ”¯æŒå¤šç§åˆ†æç»“æœå¯¼å‡ºæ ¼å¼
- ğŸ”§ æä¾›ä¸°å¯Œçš„ç®¡ç†å‘½ä»¤ï¼Œæ–¹ä¾¿é…ç½®å’Œç»´æŠ¤

ğŸ“– ä½¿ç”¨æ–¹å¼
- è‡ªåŠ¨æ¨¡å¼ï¼šç›´æ¥å‘é€åŒ…å«ç½‘é¡µé“¾æ¥çš„æ¶ˆæ¯
- æ‰‹åŠ¨æ¨¡å¼ï¼šä½¿ç”¨ `/ç½‘é¡µåˆ†æ` å‘½ä»¤ï¼Œä¾‹å¦‚ï¼š`/ç½‘é¡µåˆ†æ https://example.com`
- æ”¯æŒå¤šç§æŒ‡ä»¤åˆ«åï¼š`åˆ†æ`ã€`æ€»ç»“`ã€`web`ã€`analyze`

ğŸ¯ æ’ä»¶ä¼˜åŠ¿
- å¼‚æ­¥å¤„ç†è®¾è®¡ï¼Œæ”¯æŒå¹¶å‘åˆ†æå¤šä¸ªURL
- çµæ´»çš„é…ç½®é€‰é¡¹ï¼Œæ»¡è¶³ä¸åŒä½¿ç”¨åœºæ™¯
- å®Œå–„çš„é”™è¯¯å¤„ç†ï¼Œç¡®ä¿æ’ä»¶ç¨³å®šè¿è¡Œ
- æ”¯æŒåŸŸåç™½åå•å’Œé»‘åå•ï¼Œæ§åˆ¶è®¿é—®èŒƒå›´
- æ”¯æŒå†…å®¹ç¿»è¯‘ï¼Œçªç ´è¯­è¨€éšœç¢

æœ¬æ’ä»¶é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼ŒåŒ…å«ç¼“å­˜ç®¡ç†ã€ç½‘é¡µåˆ†æã€å‘½ä»¤å¤„ç†ç­‰å¤šä¸ªç»„ä»¶ï¼Œ
å¯æ ¹æ®éœ€æ±‚çµæ´»æ‰©å±•å’Œå®šåˆ¶ã€‚
"""

from typing import List

from astrbot.api import AstrBotConfig
from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.star import Context, Star, register
from astrbot.api import logger

from .analyzer import WebAnalyzer
from .cache import CacheManager


@register(
    "astrbot_plugin_web_analyzer",
    "Sakura520222",
    "è‡ªåŠ¨è¯†åˆ«ç½‘é¡µé“¾æ¥å¹¶è¿›è¡Œå†…å®¹åˆ†æå’Œæ€»ç»“",
    "1.2.4",
    "https://github.com/Sakura520222/astrbot_plugin_web_analyzer",
)
class WebAnalyzerPlugin(Star):
    """ç½‘é¡µåˆ†ææ’ä»¶ä¸»ç±»

    è¿™æ˜¯æ’ä»¶çš„æ ¸å¿ƒåè°ƒç±»ï¼Œè´Ÿè´£ç®¡ç†å’Œè°ƒåº¦æ‰€æœ‰åŠŸèƒ½æ¨¡å—ï¼š
    - ğŸ”§ é…ç½®çš„åŠ è½½ã€éªŒè¯å’Œç®¡ç†
    - ğŸ“© æ¶ˆæ¯äº‹ä»¶çš„ç›‘å¬å’Œå¤„ç†
    - ğŸ”— URLçš„æå–ã€éªŒè¯å’Œè¿‡æ»¤
    - ğŸ•¸ï¸  ç½‘é¡µå†…å®¹çš„æŠ“å–ã€è§£æå’Œåˆ†æ
    - ğŸ§  å¤§è¯­è¨€æ¨¡å‹(LLM)çš„è°ƒç”¨å’Œç»“æœç”Ÿæˆ
    - ğŸ’¾ åˆ†æç»“æœçš„ç¼“å­˜ç®¡ç†
    - âš™ï¸  å„ç§ç®¡ç†å‘½ä»¤çš„å¤„ç†å’Œå“åº”

    æ’ä»¶æ”¯æŒä¸¤ç§åˆ†ææ¨¡å¼ï¼š
    - è‡ªåŠ¨æ¨¡å¼ï¼šè‡ªåŠ¨è¯†åˆ«æ¶ˆæ¯ä¸­çš„ç½‘é¡µé“¾æ¥å¹¶åˆ†æ
    - æ‰‹åŠ¨æ¨¡å¼ï¼šé€šè¿‡å‘½ä»¤è§¦å‘ç½‘é¡µåˆ†æ

    æä¾›äº†ä¸°å¯Œçš„é…ç½®é€‰é¡¹ï¼Œå¯æ ¹æ®éœ€æ±‚çµæ´»è°ƒæ•´æ’ä»¶è¡Œä¸ºã€‚
    """

    def __init__(self, context: Context, config: AstrBotConfig):
        """æ’ä»¶åˆå§‹åŒ–æ–¹æ³•

        è´Ÿè´£åŠ è½½ã€éªŒè¯å’Œåˆå§‹åŒ–æ‰€æœ‰é…ç½®é¡¹ï¼Œæ„å»ºæ’ä»¶çš„è¿è¡Œç¯å¢ƒï¼š
        
        ğŸ› ï¸ åŸºæœ¬é…ç½®ï¼š
        - è¯·æ±‚è¶…æ—¶æ—¶é—´å’Œé‡è¯•æœºåˆ¶
        - ç”¨æˆ·ä»£ç†å’Œä»£ç†è®¾ç½®
        - è‡ªåŠ¨åˆ†æå¼€å…³
        
        ğŸš« åŸŸåæ§åˆ¶ï¼š
        - å…è®¸è®¿é—®çš„åŸŸååˆ—è¡¨
        - ç¦æ­¢è®¿é—®çš„åŸŸååˆ—è¡¨
        
        ğŸ“Š åˆ†æè®¾ç½®ï¼š
        - æ˜¯å¦ä½¿ç”¨emojiå¢å¼ºæ˜¾ç¤º
        - æ˜¯å¦æ˜¾ç¤ºå†…å®¹ç»Ÿè®¡ä¿¡æ¯
        - æœ€å¤§æ‘˜è¦é•¿åº¦é™åˆ¶
        
        ğŸ“¸ æˆªå›¾é…ç½®ï¼š
        - æˆªå›¾è´¨é‡å’Œåˆ†è¾¨ç‡
        - æ˜¯å¦æˆªå–æ•´é¡µ
        - æˆªå›¾æ ¼å¼ï¼ˆJPEG/PNGï¼‰
        
        ğŸ§  LLMé…ç½®ï¼š
        - å¤§è¯­è¨€æ¨¡å‹æä¾›å•†
        - è‡ªå®šä¹‰æç¤ºè¯
        
        ğŸ‘¥ ç¾¤èŠç®¡ç†ï¼š
        - ç¾¤èŠé»‘åå•è®¾ç½®
        
        ğŸŒ ç¿»è¯‘åŠŸèƒ½ï¼š
        - æ˜¯å¦å¯ç”¨è‡ªåŠ¨ç¿»è¯‘
        - ç›®æ ‡è¯­è¨€è®¾ç½®
        
        ğŸ’¾ ç¼“å­˜ç®¡ç†ï¼š
        - ç¼“å­˜è¿‡æœŸæ—¶é—´
        - æœ€å¤§ç¼“å­˜æ•°é‡
        
        ğŸ“‹ å†…å®¹æå–ï¼š
        - æå–å†…å®¹ç±»å‹è®¾ç½®
        
        æ‰€æœ‰é…ç½®é¡¹éƒ½ä¼šè¿›è¡Œåˆç†æ€§éªŒè¯ï¼Œè‡ªåŠ¨ä¿®æ­£æ— æ•ˆå€¼å¹¶è®¾ç½®å®‰å…¨é»˜è®¤å€¼ï¼Œ
        ç¡®ä¿æ’ä»¶åœ¨å„ç§é…ç½®ä¸‹éƒ½èƒ½ç¨³å®šè¿è¡Œã€‚
        """
        super().__init__(context)
        self.config = config

        # åŸºæœ¬é…ç½®åŠ è½½ä¸éªŒè¯
        # æœ€å¤§å†…å®¹é•¿åº¦ï¼šé™åˆ¶æŠ“å–çš„ç½‘é¡µå†…å®¹å¤§å°ï¼Œé¿å…å†…å­˜å ç”¨è¿‡é«˜
        self.max_content_length = max(1000, config.get("max_content_length", 10000))
        # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼šè®¾ç½®åˆç†çš„è¶…æ—¶èŒƒå›´ï¼Œé¿å…è¯·æ±‚è¿‡é•¿æ—¶é—´é˜»å¡
        self.timeout = max(5, min(300, config.get("request_timeout", 30)))
        # é‡è¯•æ¬¡æ•°ï¼šè¯·æ±‚å¤±è´¥æ—¶çš„é‡è¯•æ¬¡æ•°
        self.retry_count = max(0, min(10, config.get("retry_count", 3)))
        # é‡è¯•å»¶è¿Ÿï¼šæ¯æ¬¡é‡è¯•ä¹‹é—´çš„ç­‰å¾…æ—¶é—´
        self.retry_delay = max(0, min(10, config.get("retry_delay", 2)))
        # æ˜¯å¦å¯ç”¨LLMåˆ†æï¼šæ§åˆ¶æ˜¯å¦è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ™ºèƒ½åˆ†æ
        self.llm_enabled = bool(config.get("llm_enabled", True))
        # æ˜¯å¦è‡ªåŠ¨åˆ†æï¼šæ§åˆ¶æ˜¯å¦è‡ªåŠ¨è¯†åˆ«æ¶ˆæ¯ä¸­çš„é“¾æ¥å¹¶åˆ†æ
        self.auto_analyze = bool(config.get("auto_analyze", True))
        # ç”¨æˆ·ä»£ç†ï¼šç”¨äºæ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚ï¼Œé¿å…è¢«ç½‘ç«™å°ç¦
        self.user_agent = config.get(
            "user_agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )
        # ä»£ç†è®¾ç½®ï¼šç”¨äºç½‘ç»œä»£ç†ï¼ŒåŠ é€Ÿæˆ–ç»•è¿‡ç½‘ç»œé™åˆ¶
        self.proxy = config.get("proxy", "")

        # éªŒè¯ä»£ç†æ ¼å¼æ˜¯å¦æ­£ç¡®
        if self.proxy:
            try:
                from urllib.parse import urlparse

                parsed = urlparse(self.proxy)
                if not all([parsed.scheme, parsed.netloc]):
                    logger.warning(f"æ— æ•ˆçš„ä»£ç†æ ¼å¼: {self.proxy}ï¼Œå°†å¿½ç•¥ä»£ç†è®¾ç½®")
                    self.proxy = ""
            except Exception as e:
                logger.warning(f"è§£æä»£ç†å¤±è´¥: {self.proxy}ï¼Œå°†å¿½ç•¥ä»£ç†è®¾ç½®ï¼Œé”™è¯¯: {e}")
                self.proxy = ""

        # è§£æå…è®¸å’Œç¦æ­¢çš„åŸŸååˆ—è¡¨
        self.allowed_domains = self._parse_domain_list(
            config.get("allowed_domains", "")
        )
        self.blocked_domains = self._parse_domain_list(
            config.get("blocked_domains", "")
        )

        # åˆ†æè®¾ç½®éªŒè¯
        analysis_settings = config.get("analysis_settings", {})
        # æ˜¯å¦åœ¨ç»“æœä¸­ä½¿ç”¨emoji
        self.enable_emoji = bool(analysis_settings.get("enable_emoji", True))
        # æ˜¯å¦æ˜¾ç¤ºå†…å®¹ç»Ÿè®¡ä¿¡æ¯
        self.enable_statistics = bool(analysis_settings.get("enable_statistics", True))
        # æœ€å¤§æ‘˜è¦é•¿åº¦ï¼šé™åˆ¶LLMç”Ÿæˆçš„æ‘˜è¦å¤§å°
        self.max_summary_length = max(
            500, min(10000, analysis_settings.get("max_summary_length", 2000))
        )

        # æˆªå›¾è®¾ç½®éªŒè¯
        self.enable_screenshot = bool(analysis_settings.get("enable_screenshot", True))
        # æˆªå›¾è´¨é‡ï¼šæ§åˆ¶æˆªå›¾çš„æ¸…æ™°åº¦å’Œæ–‡ä»¶å¤§å°
        self.screenshot_quality = max(
            10, min(100, analysis_settings.get("screenshot_quality", 80))
        )
        # æˆªå›¾å®½åº¦å’Œé«˜åº¦ï¼šæ§åˆ¶æˆªå›¾çš„åˆ†è¾¨ç‡
        self.screenshot_width = max(
            320, min(4096, analysis_settings.get("screenshot_width", 1280))
        )
        self.screenshot_height = max(
            240, min(4096, analysis_settings.get("screenshot_height", 720))
        )
        # æ˜¯å¦æˆªå–æ•´é¡µï¼šæ§åˆ¶æ˜¯å¦æˆªå–å®Œæ•´çš„ç½‘é¡µå†…å®¹
        self.screenshot_full_page = bool(
            analysis_settings.get("screenshot_full_page", False)
        )
        # æˆªå›¾ç­‰å¾…æ—¶é—´ï¼šé¡µé¢åŠ è½½å®Œæˆåç­‰å¾…çš„æ—¶é—´ï¼Œç¡®ä¿å†…å®¹å®Œæ•´æ˜¾ç¤º
        self.screenshot_wait_time = max(
            0, min(10000, analysis_settings.get("screenshot_wait_time", 2000))
        )

        # éªŒè¯æˆªå›¾æ ¼å¼æ˜¯å¦æ”¯æŒ
        screenshot_format = analysis_settings.get("screenshot_format", "jpeg").lower()
        if screenshot_format not in ["jpeg", "png"]:
            logger.warning(f"æ— æ•ˆçš„æˆªå›¾æ ¼å¼: {screenshot_format}ï¼Œå°†ä½¿ç”¨é»˜è®¤æ ¼å¼ jpeg")
            self.screenshot_format = "jpeg"
        else:
            self.screenshot_format = screenshot_format

        # LLMæä¾›å•†é…ç½®ï¼šæŒ‡å®šä½¿ç”¨çš„å¤§è¯­è¨€æ¨¡å‹æä¾›å•†
        self.llm_provider = config.get("llm_provider", "")

        # ç¾¤èŠé»‘åå•é…ç½®ï¼šç”¨äºæ§åˆ¶å“ªäº›ç¾¤èŠä¸å…è®¸ä½¿ç”¨æ’ä»¶
        group_blacklist_text = config.get("group_blacklist", "")
        self.group_blacklist = self._parse_group_list(group_blacklist_text)

        # åˆå¹¶è½¬å‘é…ç½®ï¼šæ§åˆ¶æ˜¯å¦ä½¿ç”¨åˆå¹¶è½¬å‘åŠŸèƒ½å‘é€åˆ†æç»“æœ
        merge_forward_config = config.get("merge_forward_enabled", {})
        self.merge_forward_enabled = {
            "group": bool(merge_forward_config.get("group", False)),
            "private": bool(merge_forward_config.get("private", False))
        }

        # è‡ªå®šä¹‰æç¤ºè¯é…ç½®ï¼šå…è®¸ç”¨æˆ·è‡ªå®šä¹‰LLMåˆ†æçš„æç¤ºè¯
        self.custom_prompt = config.get("custom_prompt", "")

        # ç¿»è¯‘è®¾ç½®éªŒè¯ï¼šæ§åˆ¶æ˜¯å¦è‡ªåŠ¨ç¿»è¯‘ç½‘é¡µå†…å®¹
        translation_settings = config.get("translation_settings", {})
        self.enable_translation = bool(
            translation_settings.get("enable_translation", False)
        )

        # éªŒè¯ç›®æ ‡è¯­è¨€æ˜¯å¦æ”¯æŒ
        self.target_language = translation_settings.get("target_language", "zh").lower()
        valid_languages = ["zh", "en", "ja", "ko", "fr", "de", "es", "ru", "ar", "pt"]
        if self.target_language not in valid_languages:
            logger.warning(f"æ— æ•ˆçš„ç›®æ ‡è¯­è¨€: {self.target_language}ï¼Œå°†ä½¿ç”¨é»˜è®¤è¯­è¨€ zh")
            self.target_language = "zh"

        # ç¿»è¯‘æä¾›å•†é…ç½®
        self.translation_provider = translation_settings.get(
            "translation_provider", "llm"
        )
        # è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯ï¼šå…è®¸ç”¨æˆ·è‡ªå®šä¹‰ç¿»è¯‘çš„æç¤ºè¯
        self.custom_translation_prompt = translation_settings.get(
            "custom_translation_prompt", ""
        )

        # ç¼“å­˜è®¾ç½®éªŒè¯ï¼šæ§åˆ¶æ˜¯å¦å¯ç”¨ç»“æœç¼“å­˜
        cache_settings = config.get("cache_settings", {})
        self.enable_cache = bool(cache_settings.get("enable_cache", True))
        # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼šæ§åˆ¶ç¼“å­˜ç»“æœçš„æœ‰æ•ˆæœŸ
        self.cache_expire_time = max(
            5, min(10080, cache_settings.get("cache_expire_time", 1440))
        )
        # æœ€å¤§ç¼“å­˜æ•°é‡ï¼šæ§åˆ¶ç¼“å­˜çš„æœ€å¤§æ¡ç›®æ•°ï¼Œé¿å…å†…å­˜å ç”¨è¿‡é«˜
        self.max_cache_size = max(
            10, min(1000, cache_settings.get("max_cache_size", 100))
        )

        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨ï¼šç”¨äºç®¡ç†åˆ†æç»“æœçš„ç¼“å­˜
        self.cache_manager = CacheManager(
            max_size=self.max_cache_size, expire_time=self.cache_expire_time
        )

        # å†…å®¹æå–è®¾ç½®éªŒè¯ï¼šæ§åˆ¶æ˜¯å¦å¯ç”¨ç‰¹å®šå†…å®¹æå–
        content_extraction_settings = config.get("content_extraction_settings", {})
        self.enable_specific_extraction = bool(
            content_extraction_settings.get("enable_specific_extraction", False)
        )
        # æå–ç±»å‹ï¼šæŒ‡å®šè¦æå–çš„å†…å®¹ç±»å‹
        extract_types_text = content_extraction_settings.get(
            "extract_types", "title\ncontent"
        )
        self.extract_types = [
            t.strip() for t in extract_types_text.split("\n") if t.strip()
        ]

        # éªŒè¯æå–ç±»å‹æ˜¯å¦æœ‰æ•ˆ
        valid_extract_types = [
            "title",
            "content",
            "images",
            "links",
            "tables",
            "lists",
            "code",
            "meta",
        ]
        invalid_types = [t for t in self.extract_types if t not in valid_extract_types]
        if invalid_types:
            logger.warning(
                f"æ— æ•ˆçš„æå–ç±»å‹: {', '.join(invalid_types)}ï¼Œå°†å¿½ç•¥è¿™äº›ç±»å‹"
            )
            self.extract_types = [
                t for t in self.extract_types if t in valid_extract_types
            ]

        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæå–ç±»å‹
        if not self.extract_types:
            self.extract_types = ["title", "content"]

        # è‡ªåŠ¨æ·»åŠ metaç±»å‹ï¼Œç”¨äºæå–ç½‘é¡µå…ƒä¿¡æ¯
        if "meta" not in self.extract_types:
            self.extract_types.append("meta")

        # åˆå§‹åŒ–ç½‘é¡µåˆ†æå™¨ï¼šç”¨äºæŠ“å–å’Œåˆ†æç½‘é¡µå†…å®¹
        self.analyzer = WebAnalyzer(
            max_content_length=self.max_content_length,
            timeout=self.timeout,
            user_agent=self.user_agent,
            proxy=self.proxy,
            retry_count=self.retry_count,
            retry_delay=self.retry_delay,
        )

        # URLå¤„ç†æ ‡å¿—é›†åˆï¼šç”¨äºé¿å…é‡å¤å¤„ç†åŒä¸€URL
        self.processing_urls = set()

        # è®°å½•é…ç½®åˆå§‹åŒ–å®Œæˆ
        logger.info("æ’ä»¶é…ç½®åˆå§‹åŒ–å®Œæˆ")

    def _parse_domain_list(self, domain_text: str) -> List[str]:
        """å°†å¤šè¡ŒåŸŸåæ–‡æœ¬è½¬æ¢ä¸ºPythonåˆ—è¡¨

        å¤„ç†é…ç½®ä¸­å®šä¹‰çš„åŸŸååˆ—è¡¨ï¼Œæ”¯æŒï¼š
        - æ¯è¡Œä¸€ä¸ªåŸŸåçš„æ ¼å¼
        - è‡ªåŠ¨å»é™¤ç©ºè¡Œå’Œå‰åç©ºç™½å­—ç¬¦
        - æ”¯æŒåŸŸåé€šé…ç¬¦ï¼ˆå¦‚*.example.comï¼‰

        Args:
            domain_text: åŒ…å«åŸŸåçš„å¤šè¡Œæ–‡æœ¬å­—ç¬¦ä¸²

        Returns:
            è§£æåçš„åŸŸååˆ—è¡¨ï¼Œå·²æ¸…ç†æ— æ•ˆå†…å®¹
        """
        if not domain_text:
            return []
        domains = [
            domain.strip() for domain in domain_text.split("\n") if domain.strip()
        ]
        return domains

    def _parse_group_list(self, group_text: str) -> List[str]:
        """å°†å¤šè¡Œç¾¤èŠIDæ–‡æœ¬è½¬æ¢ä¸ºPythonåˆ—è¡¨

        å¤„ç†é…ç½®ä¸­å®šä¹‰çš„ç¾¤èŠé»‘åå•ï¼Œæ”¯æŒï¼š
        - æ¯è¡Œä¸€ä¸ªç¾¤èŠIDçš„æ ¼å¼
        - è‡ªåŠ¨å»é™¤ç©ºè¡Œå’Œå‰åç©ºç™½å­—ç¬¦
        - æ”¯æŒæ•°å­—å’Œå­—ç¬¦ä¸²ç±»å‹çš„ç¾¤èŠID

        Args:
            group_text: åŒ…å«ç¾¤èŠIDçš„å¤šè¡Œæ–‡æœ¬å­—ç¬¦ä¸²

        Returns:
            è§£æåçš„ç¾¤èŠIDåˆ—è¡¨ï¼Œå·²æ¸…ç†æ— æ•ˆå†…å®¹
        """
        if not group_text:
            return []
        groups = [group.strip() for group in group_text.split("\n") if group.strip()]
        return groups

    def _is_group_blacklisted(self, group_id: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šç¾¤èŠæ˜¯å¦åœ¨é»‘åå•ä¸­

        ç¾¤èŠé»‘åå•åŠŸèƒ½å¯ä»¥æ§åˆ¶å“ªäº›ç¾¤èŠä¸èƒ½ä½¿ç”¨æ’ä»¶ï¼Œ
        é€‚ç”¨äºéœ€è¦é™åˆ¶æ’ä»¶ä½¿ç”¨èŒƒå›´çš„åœºæ™¯ã€‚

        Args:
            group_id: ç¾¤èŠIDï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æ•°å­—

        Returns:
            Trueè¡¨ç¤ºåœ¨é»‘åå•ä¸­ï¼ˆç¦æ­¢ä½¿ç”¨ï¼‰ï¼ŒFalseè¡¨ç¤ºä¸åœ¨é»‘åå•ä¸­ï¼ˆå…è®¸ä½¿ç”¨ï¼‰
        """
        if not group_id or not self.group_blacklist:
            return False
        return group_id in self.group_blacklist

    def _is_domain_allowed(self, url: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šURLçš„åŸŸåæ˜¯å¦å…è®¸è®¿é—®

        æ ¹æ®é…ç½®çš„å…è®¸å’Œç¦æ­¢åŸŸååˆ—è¡¨ï¼Œåˆ¤æ–­URLæ˜¯å¦å¯ä»¥è®¿é—®ï¼Œ
        æ”¯æŒçµæ´»çš„è®¿é—®æ§åˆ¶ç­–ç•¥ï¼š
        
        è®¿é—®è§„åˆ™ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š
        1. å¦‚æœåŸŸååœ¨ç¦æ­¢åˆ—è¡¨ä¸­ï¼Œç›´æ¥æ‹’ç»è®¿é—®
        2. å¦‚æœå…è®¸åˆ—è¡¨ä¸ä¸ºç©ºï¼Œåªæœ‰åœ¨åˆ—è¡¨ä¸­çš„åŸŸåæ‰å…è®¸è®¿é—®
        3. å¦‚æœå…è®¸åˆ—è¡¨ä¸ºç©ºï¼Œåˆ™å…è®¸æ‰€æœ‰æœªè¢«ç¦æ­¢çš„åŸŸå

        Args:
            url: è¦æ£€æŸ¥çš„å®Œæ•´URL

        Returns:
            Trueè¡¨ç¤ºå…è®¸è®¿é—®ï¼ŒFalseè¡¨ç¤ºç¦æ­¢è®¿é—®
        """
        try:
            from urllib.parse import urlparse

            parsed = urlparse(url)
            domain = parsed.netloc.lower()

            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦åœ¨ç¦æ­¢åˆ—è¡¨ä¸­
            if self.blocked_domains:
                for blocked_domain in self.blocked_domains:
                    if blocked_domain.lower() in domain:
                        return False

            # ç„¶åæ£€æŸ¥æ˜¯å¦åœ¨å…è®¸åˆ—è¡¨ä¸­ï¼ˆå¦‚æœå…è®¸åˆ—è¡¨ä¸ä¸ºç©ºï¼‰
            if self.allowed_domains:
                for allowed_domain in self.allowed_domains:
                    if allowed_domain.lower() in domain:
                        return True
                return False  # å…è®¸åˆ—è¡¨ä¸ä¸ºç©ºï¼Œä½†åŸŸåä¸åœ¨å…¶ä¸­ï¼Œæ‹’ç»è®¿é—®

            return True  # å…è®¸åˆ—è¡¨ä¸ºç©ºï¼Œå…è®¸æ‰€æœ‰æœªè¢«ç¦æ­¢çš„åŸŸå
        except Exception:
            return False

    @filter.command("ç½‘é¡µåˆ†æ", alias={"åˆ†æ", "æ€»ç»“", "web", "analyze"})
    async def analyze_webpage(self, event: AstrMessageEvent):
        """æ‰‹åŠ¨è§¦å‘ç½‘é¡µåˆ†æå‘½ä»¤

        è¿™æ˜¯æ’ä»¶çš„æ ¸å¿ƒå‘½ä»¤ï¼Œå…è®¸ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®šè¦åˆ†æçš„ç½‘é¡µé“¾æ¥ï¼Œ
        æ”¯æŒå¤šç§å‘½ä»¤åˆ«åï¼Œæ–¹ä¾¿ä¸åŒä½¿ç”¨ä¹ æƒ¯çš„ç”¨æˆ·ã€‚

        ğŸ“‹ ç”¨æ³•ç¤ºä¾‹ï¼š
        - `/ç½‘é¡µåˆ†æ https://example.com` - åˆ†æå•ä¸ªé“¾æ¥
        - `/åˆ†æ https://example.com https://test.com` - åˆ†æå¤šä¸ªé“¾æ¥
        - `/æ€»ç»“ https://example.com` - ä½¿ç”¨åˆ«åå‘½ä»¤
        
        ğŸ”§ åŠŸèƒ½ç‰¹æ€§ï¼š
        - æ”¯æŒåŒæ—¶åˆ†æå¤šä¸ªç½‘é¡µé“¾æ¥
        - è‡ªåŠ¨éªŒè¯URLæ ¼å¼æ­£ç¡®æ€§
        - æ ¹æ®åŸŸåé»‘ç™½åå•è¿‡æ»¤é“¾æ¥
        - å¼‚æ­¥å¤„ç†ï¼Œä¸é˜»å¡å…¶ä»–æ“ä½œ
        - æ”¯æŒå„ç§è¾“å‡ºæ ¼å¼

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼ŒåŒ…å«æ¶ˆæ¯å†…å®¹å’Œä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        message_text = event.message_str

        # ä»æ¶ˆæ¯ä¸­æå–æ‰€æœ‰URL
        urls = self.analyzer.extract_urls(message_text)
        if not urls:
            yield event.plain_result(
                "è¯·æä¾›è¦åˆ†æçš„ç½‘é¡µé“¾æ¥ï¼Œä¾‹å¦‚ï¼š/ç½‘é¡µåˆ†æ https://example.com"
            )
            return

        # éªŒè¯URLæ ¼å¼æ˜¯å¦æ­£ç¡®
        valid_urls = [url for url in urls if self.analyzer.is_valid_url(url)]
        if not valid_urls:
            yield event.plain_result("æ— æ•ˆçš„URLé“¾æ¥ï¼Œè¯·æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®")
            return

        # è¿‡æ»¤æ‰ä¸å…è®¸è®¿é—®çš„åŸŸå
        allowed_urls = [url for url in valid_urls if self._is_domain_allowed(url)]
        if not allowed_urls:
            yield event.plain_result("æ‰€æœ‰åŸŸåéƒ½ä¸åœ¨å…è®¸è®¿é—®çš„åˆ—è¡¨ä¸­ï¼Œæˆ–å·²è¢«ç¦æ­¢è®¿é—®")
            return

        # å‘é€å¤„ç†æç¤ºæ¶ˆæ¯ï¼Œå‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨åˆ†æ
        if len(allowed_urls) == 1:
            yield event.plain_result(f"æ­£åœ¨åˆ†æç½‘é¡µ: {allowed_urls[0]}")
        else:
            yield event.plain_result(f"æ­£åœ¨åˆ†æ{len(allowed_urls)}ä¸ªç½‘é¡µé“¾æ¥...")

        # æ‰¹é‡å¤„ç†æ‰€æœ‰å…è®¸è®¿é—®çš„URL
        async for result in self._batch_process_urls(event, allowed_urls):
            yield result

    @filter.event_message_type(filter.EventMessageType.ALL)
    async def auto_detect_urls(self, event: AstrMessageEvent):
        """è‡ªåŠ¨æ£€æµ‹æ¶ˆæ¯ä¸­çš„URLé“¾æ¥å¹¶è¿›è¡Œåˆ†æ

        è¿™ä¸ªæ–¹æ³•å®ç°äº†æ’ä»¶çš„è‡ªåŠ¨åˆ†æåŠŸèƒ½ï¼Œæ— éœ€ç”¨æˆ·æ‰‹åŠ¨è°ƒç”¨å‘½ä»¤ï¼Œ
        åªè¦å‘é€åŒ…å«ç½‘é¡µé“¾æ¥çš„æ¶ˆæ¯ï¼Œæ’ä»¶å°±ä¼šè‡ªåŠ¨è¿›è¡Œåˆ†æã€‚

        ğŸš¦ è‡ªåŠ¨åˆ†æè§„åˆ™ï¼š
        1. ä»…å½“é…ç½®ä¸­auto_analyzeä¸ºTrueæ—¶å¯ç”¨
        2. æ™ºèƒ½è·³è¿‡å‘½ä»¤æ¶ˆæ¯ï¼Œé¿å…é‡å¤å¤„ç†
        3. è·³è¿‡åŒ…å«ç½‘é¡µåˆ†æç›¸å…³æŒ‡ä»¤çš„æ¶ˆæ¯
        4. è·³è¿‡åœ¨é»‘åå•ä¸­çš„ç¾¤èŠæ¶ˆæ¯
        5. ä»…å¤„ç†æ ¼å¼æ­£ç¡®çš„URL
        6. éµå®ˆåŸŸåé»‘ç™½åå•é™åˆ¶

        âœ¨ ä¼˜åŠ¿ï¼š
        - æå‡ç”¨æˆ·ä½“éªŒï¼Œæ— éœ€è®°å¿†å‘½ä»¤
        - æ”¯æŒæ‰€æœ‰æ¶ˆæ¯ç±»å‹ï¼ˆç§èŠã€ç¾¤èŠï¼‰
        - æ™ºèƒ½è¿‡æ»¤ï¼Œé¿å…è¯¯è§¦å‘
        - ä¸æ‰‹åŠ¨åˆ†æä½¿ç”¨ç›¸åŒçš„æ ¸å¿ƒé€»è¾‘

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼ŒåŒ…å«æ¶ˆæ¯å†…å®¹å’Œä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨åˆ†æåŠŸèƒ½
        if not self.auto_analyze:
            return

        # æ£€æŸ¥æ˜¯å¦ä¸ºæŒ‡ä»¤è°ƒç”¨ï¼Œé¿å…é‡å¤å¤„ç†
        message_text = event.message_str.strip()

        # æ–¹æ³•1ï¼šè·³è¿‡ä»¥/å¼€å¤´çš„æŒ‡ä»¤æ¶ˆæ¯
        if message_text.startswith("/"):
            logger.info("æ£€æµ‹åˆ°æŒ‡ä»¤è°ƒç”¨ï¼Œè·³è¿‡è‡ªåŠ¨åˆ†æ")
            return

        # æ–¹æ³•2ï¼šæ£€æŸ¥äº‹ä»¶æ˜¯å¦æœ‰commandå±æ€§ï¼ˆæŒ‡ä»¤è°ƒç”¨æ—¶ä¼šæœ‰ï¼‰
        if hasattr(event, "command"):
            logger.info("æ£€æµ‹åˆ°commandå±æ€§ï¼Œè·³è¿‡è‡ªåŠ¨åˆ†æ")
            return

        # æ–¹æ³•3ï¼šæ£€æŸ¥åŸå§‹æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«ç½‘é¡µåˆ†æç›¸å…³æŒ‡ä»¤å…³é”®å­—
        raw_message = None
        if hasattr(event, "raw_message"):
            raw_message = str(event.raw_message)
        elif hasattr(event, "message_obj"):
            raw_message = str(event.message_obj)

        if raw_message:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç½‘é¡µåˆ†æç›¸å…³æŒ‡ä»¤
            command_keywords = ["ç½‘é¡µåˆ†æ", "/åˆ†æ", "/æ€»ç»“", "/web", "/analyze"]
            for keyword in command_keywords:
                if keyword in raw_message:
                    logger.info(f"æ£€æµ‹åˆ°æŒ‡ä»¤å…³é”®å­— {keyword}ï¼Œè·³è¿‡è‡ªåŠ¨åˆ†æ")
                    return

        # æ£€æŸ¥ç¾¤èŠæ˜¯å¦åœ¨é»‘åå•ä¸­ï¼ˆä»…ç¾¤èŠæ¶ˆæ¯ï¼‰
        group_id = None

        # æ–¹æ³•1ï¼šä»äº‹ä»¶å¯¹è±¡ç›´æ¥è·å–ç¾¤èŠID
        if hasattr(event, "group_id") and event.group_id:
            group_id = event.group_id
        # æ–¹æ³•2ï¼šä»æ¶ˆæ¯å¯¹è±¡è·å–ç¾¤èŠID
        elif (
            hasattr(event, "message_obj")
            and hasattr(event.message_obj, "group_id")
            and event.message_obj.group_id
        ):
            group_id = event.message_obj.group_id
        # æ–¹æ³•3ï¼šä»åŸå§‹æ¶ˆæ¯è·å–ç¾¤èŠID
        elif (
            hasattr(event, "raw_message")
            and hasattr(event.raw_message, "group_id")
            and event.raw_message.group_id
        ):
            group_id = event.raw_message.group_id

        # ç¾¤èŠåœ¨é»‘åå•ä¸­æ—¶é™é»˜å¿½ç•¥ï¼Œä¸è¿›è¡Œä»»ä½•å¤„ç†
        if group_id and self._is_group_blacklisted(group_id):
            return

        # ä»æ¶ˆæ¯ä¸­æå–æ‰€æœ‰URL
        urls = self.analyzer.extract_urls(message_text)
        if not urls:
            return  # æ²¡æœ‰URLï¼Œä¸å¤„ç†

        # éªŒè¯URLæ ¼å¼æ˜¯å¦æ­£ç¡®
        valid_urls = [url for url in urls if self.analyzer.is_valid_url(url)]
        if not valid_urls:
            return  # æ²¡æœ‰æœ‰æ•ˆURLï¼Œä¸å¤„ç†

        # è¿‡æ»¤æ‰ä¸å…è®¸è®¿é—®çš„åŸŸå
        allowed_urls = [url for url in valid_urls if self._is_domain_allowed(url)]
        if not allowed_urls:
            return  # æ²¡æœ‰å…è®¸è®¿é—®çš„URLï¼Œä¸å¤„ç†

        # å‘é€å¤„ç†æç¤ºæ¶ˆæ¯ï¼Œå‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨åˆ†æ
        if len(allowed_urls) == 1:
            yield event.plain_result(f"æ£€æµ‹åˆ°ç½‘é¡µé“¾æ¥ï¼Œæ­£åœ¨åˆ†æ: {allowed_urls[0]}")
        else:
            yield event.plain_result(
                f"æ£€æµ‹åˆ°{len(allowed_urls)}ä¸ªç½‘é¡µé“¾æ¥ï¼Œæ­£åœ¨åˆ†æ..."
            )

        # æ‰¹é‡å¤„ç†æ‰€æœ‰å…è®¸è®¿é—®çš„URL
        async for result in self._batch_process_urls(event, allowed_urls):
            yield result

    async def _process_single_url(
        self, event: AstrMessageEvent, url: str, analyzer: WebAnalyzer
    ) -> dict:
        """å¤„ç†å•ä¸ªç½‘é¡µURLï¼Œç”Ÿæˆå®Œæ•´çš„åˆ†æç»“æœ

        è¿™æ˜¯å¤„ç†å•ä¸ªç½‘é¡µé“¾æ¥çš„æ ¸å¿ƒæ–¹æ³•ï¼ŒåŒ…å«å®Œæ•´çš„åˆ†ææµç¨‹ï¼š
        
        ğŸ”„ å¤„ç†æµç¨‹ï¼š
        1. ğŸ” æ£€æŸ¥ç¼“å­˜ï¼Œé¿å…é‡å¤åˆ†æ
        2. ğŸŒ æŠ“å–ç½‘é¡µHTMLå†…å®¹
        3. ğŸ“ æå–ç»“æ„åŒ–çš„ç½‘é¡µå†…å®¹
        4. ğŸŒ ç¿»è¯‘å†…å®¹ï¼ˆå¦‚æœå¯ç”¨äº†ç¿»è¯‘åŠŸèƒ½ï¼‰
        5. ğŸ§  è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)è¿›è¡Œæ™ºèƒ½åˆ†æ
        6. ğŸ“Š æå–ç‰¹å®šç±»å‹å†…å®¹ï¼ˆå›¾ç‰‡ã€é“¾æ¥ã€è¡¨æ ¼ç­‰ï¼‰
        7. ğŸ“¸ æ•è·ç½‘é¡µæˆªå›¾ï¼ˆå¦‚æœå¯ç”¨äº†æˆªå›¾åŠŸèƒ½ï¼‰
        8. ğŸ’¾ æ›´æ–°ç¼“å­˜ï¼Œä¿å­˜åˆ†æç»“æœ
        9. ğŸ“¤ è¿”å›å®Œæ•´çš„åˆ†æç»“æœ

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯
            url: è¦åˆ†æçš„ç½‘é¡µURL
            analyzer: WebAnalyzerå®ä¾‹ï¼Œç”¨äºç½‘é¡µæŠ“å–å’Œåˆ†æ

        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸ï¼š
            {
                'url': åˆ†æçš„URLåœ°å€,
                'result': æ ¼å¼åŒ–çš„åˆ†æç»“æœæ–‡æœ¬,
                'screenshot': ç½‘é¡µæˆªå›¾äºŒè¿›åˆ¶æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            }
        """
        try:
            # æ£€æŸ¥ç¼“å­˜ï¼Œé¿å…é‡å¤åˆ†æ
            cached_result = self._check_cache(url)
            if cached_result:
                logger.info(f"ä½¿ç”¨ç¼“å­˜ç»“æœ: {url}")
                return cached_result

            # æŠ“å–ç½‘é¡µHTMLå†…å®¹
            html = await analyzer.fetch_webpage(url)
            if not html:
                return {
                    "url": url,
                    "result": f"âŒ æ— æ³•æŠ“å–ç½‘é¡µå†…å®¹: {url}",
                    "screenshot": None,
                }

            # ä»HTMLä¸­æå–ç»“æ„åŒ–å†…å®¹
            content_data = analyzer.extract_content(html, url)
            if not content_data:
                return {
                    "url": url,
                    "result": f"âŒ æ— æ³•è§£æç½‘é¡µå†…å®¹: {url}",
                    "screenshot": None,
                }

            # å¦‚æœå¯ç”¨äº†ç¿»è¯‘åŠŸèƒ½ï¼Œå…ˆç¿»è¯‘å†…å®¹
            if self.enable_translation:
                translated_content = await self._translate_content(
                    event, content_data["content"]
                )
                # åˆ›å»ºç¿»è¯‘åçš„å†…å®¹æ•°æ®å‰¯æœ¬
                translated_content_data = content_data.copy()
                translated_content_data["content"] = translated_content
                # è°ƒç”¨LLMè¿›è¡Œåˆ†æï¼ˆä½¿ç”¨ç¿»è¯‘åçš„å†…å®¹ï¼‰
                analysis_result = await self.analyze_with_llm(
                    event, translated_content_data
                )
            else:
                # ç›´æ¥è°ƒç”¨LLMè¿›è¡Œåˆ†æ
                analysis_result = await self.analyze_with_llm(event, content_data)

            # å¦‚æœå¯ç”¨äº†ç‰¹å®šå†…å®¹æå–ï¼Œæå–é¢å¤–ä¿¡æ¯
            specific_content = self._extract_specific_content(html, url)
            if specific_content:
                # åœ¨åˆ†æç»“æœä¸­æ·»åŠ ç‰¹å®šå†…å®¹
                specific_content_str = "\n\n**ç‰¹å®šå†…å®¹æå–**\n"

                # æ·»åŠ å›¾ç‰‡é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
                if "images" in specific_content and specific_content["images"]:
                    specific_content_str += (
                        f"\nğŸ“· å›¾ç‰‡é“¾æ¥ ({len(specific_content['images'])}):\n"
                    )
                    for img_url in specific_content["images"]:
                        specific_content_str += f"- {img_url}\n"

                # æ·»åŠ ç›¸å…³é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼Œæœ€å¤šæ˜¾ç¤º5ä¸ªï¼‰
                if "links" in specific_content and specific_content["links"]:
                    specific_content_str += (
                        f"\nğŸ”— ç›¸å…³é“¾æ¥ ({len(specific_content['links'])}):\n"
                    )
                    for link in specific_content["links"][:5]:
                        specific_content_str += f"- [{link['text']}]({link['url']})\n"

                # æ·»åŠ ä»£ç å—ï¼ˆå¦‚æœæœ‰ï¼Œæœ€å¤šæ˜¾ç¤º2ä¸ªï¼‰
                if (
                    "code_blocks" in specific_content
                    and specific_content["code_blocks"]
                ):
                    specific_content_str += (
                        f"\nğŸ’» ä»£ç å— ({len(specific_content['code_blocks'])}):\n"
                    )
                    for i, code in enumerate(specific_content["code_blocks"][:2]):
                        specific_content_str += f"```\n{code}\n```\n"

                # æ·»åŠ å…ƒä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                if "meta" in specific_content and specific_content["meta"]:
                    meta_info = specific_content["meta"]
                    specific_content_str += "\nğŸ“‹ å…ƒä¿¡æ¯:\n"
                    for key, value in meta_info.items():
                        if value:
                            specific_content_str += f"- {key}: {value}\n"

                # å°†ç‰¹å®šå†…å®¹æ·»åŠ åˆ°åˆ†æç»“æœä¸­
                analysis_result += specific_content_str

            # å¦‚æœå¯ç”¨äº†æˆªå›¾åŠŸèƒ½ï¼Œæ•è·ç½‘é¡µæˆªå›¾
            screenshot = None
            if self.enable_screenshot:
                screenshot = await analyzer.capture_screenshot(
                    url,
                    quality=self.screenshot_quality,
                    width=self.screenshot_width,
                    height=self.screenshot_height,
                    full_page=self.screenshot_full_page,
                    wait_time=self.screenshot_wait_time,
                    format=self.screenshot_format,
                )

            # å‡†å¤‡æœ€ç»ˆçš„ç»“æœæ•°æ®
            result_data = {
                "url": url,
                "result": analysis_result,
                "screenshot": screenshot,
            }

            # æ›´æ–°ç¼“å­˜ï¼Œä¿å­˜åˆ†æç»“æœ
            self._update_cache(url, result_data)

            return result_data
        except Exception as e:
            # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œç¡®ä¿æ–¹æ³•å§‹ç»ˆè¿”å›æœ‰æ•ˆç»“æœ
            logger.error(f"å¤„ç†URL {url} æ—¶å‡ºé”™: {e}")
            return {
                "url": url,
                "result": f"âŒ å¤„ç†URLæ—¶å‡ºé”™: {url}\né”™è¯¯ä¿¡æ¯: {str(e)}",
                "screenshot": None,
            }

    async def _batch_process_urls(self, event: AstrMessageEvent, urls: List[str]):
        """æ‰¹é‡å¤„ç†å¤šä¸ªURLï¼Œå®ç°é«˜æ•ˆçš„å¹¶å‘åˆ†æ

        è¿™ä¸ªæ–¹æ³•è´Ÿè´£ç®¡ç†å¤šä¸ªURLçš„å¹¶å‘å¤„ç†ï¼Œæé«˜æ’ä»¶çš„å¤„ç†æ•ˆç‡ï¼Œ
        æ”¯æŒå¼‚æ­¥å¹¶å‘å¤„ç†ï¼Œé¿å…é˜»å¡ç­‰å¾…å•ä¸ªURLåˆ†æå®Œæˆã€‚

        ğŸ”„ å¤„ç†æµç¨‹ï¼š
        1. ğŸš« è¿‡æ»¤æ‰æ­£åœ¨å¤„ç†çš„URLï¼Œé¿å…é‡å¤åˆ†æ
        2. ğŸ¯ ä½¿ç”¨å¼‚æ­¥æ–¹å¼å¹¶å‘å¤„ç†å¤šä¸ªURL
        3. ğŸ“¤ è°ƒç”¨_send_analysis_resultå‘é€æ‰€æœ‰åˆ†æç»“æœ
        4. ğŸ§¹ ç¡®ä¿URLå¤„ç†å®Œæˆåä»å¤„ç†é˜Ÿåˆ—ä¸­ç§»é™¤

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼Œç”¨äºç”Ÿæˆå“åº”
            urls: è¦å¤„ç†çš„URLåˆ—è¡¨
        """
        # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
        analysis_results = []

        # è¿‡æ»¤æ‰æ­£åœ¨å¤„ç†çš„URLï¼Œé¿å…é‡å¤åˆ†æ
        filtered_urls = []
        for url in urls:
            if url not in self.processing_urls:
                filtered_urls.append(url)
                # æ·»åŠ åˆ°æ­£åœ¨å¤„ç†çš„é›†åˆä¸­ï¼Œé˜²æ­¢é‡å¤å¤„ç†
                self.processing_urls.add(url)
            else:
                logger.info(f"URL {url} æ­£åœ¨å¤„ç†ä¸­ï¼Œè·³è¿‡é‡å¤åˆ†æ")

        # å¦‚æœæ‰€æœ‰URLéƒ½æ­£åœ¨å¤„ç†ä¸­ï¼Œç›´æ¥è¿”å›
        if not filtered_urls:
            return

        try:
            # åˆ›å»ºWebAnalyzerå®ä¾‹ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºæ­£ç¡®é‡Šæ”¾
            async with WebAnalyzer(
                self.max_content_length,
                self.timeout,
                self.user_agent,
                self.proxy,
                self.retry_count,
                self.retry_delay,
            ) as analyzer:
                # ä½¿ç”¨asyncio.gatherå¹¶å‘å¤„ç†å¤šä¸ªURLï¼Œæé«˜æ•ˆç‡
                import asyncio

                # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
                tasks = [
                    self._process_single_url(event, url, analyzer)
                    for url in filtered_urls
                ]
                # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
                analysis_results = await asyncio.gather(*tasks)

            # å‘é€æ‰€æœ‰åˆ†æç»“æœ
            async for result in self._send_analysis_result(event, analysis_results):
                yield result
        finally:
            # æ— è®ºå¤„ç†æˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼Œéƒ½è¦ä»å¤„ç†é›†åˆä¸­ç§»é™¤URL
            for url in filtered_urls:
                if url in self.processing_urls:
                    self.processing_urls.remove(url)

    async def analyze_with_llm(
        self, event: AstrMessageEvent, content_data: dict
    ) -> str:
        """è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)è¿›è¡Œæ™ºèƒ½å†…å®¹åˆ†æå’Œæ€»ç»“

        è¿™æ˜¯å®ç°AIæ™ºèƒ½åˆ†æçš„æ ¸å¿ƒæ–¹æ³•ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å¯¹ç½‘é¡µå†…å®¹è¿›è¡Œæ·±åº¦ç†è§£ï¼Œ
        æ”¯æŒçµæ´»çš„é…ç½®å’Œä¼˜åŒ–ï¼š
        
        ğŸ”§ åŠŸèƒ½ç‰¹æ€§ï¼š
        1. âœ… æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨å’Œå¯ç”¨
        2. ğŸ¤– è·å–åˆé€‚çš„LLMæä¾›å•†
        3. ğŸ’¬ æ„å»ºä¼˜åŒ–çš„æç¤ºè¯ï¼ˆæ”¯æŒè‡ªå®šä¹‰æç¤ºè¯ï¼‰
        4. ğŸ“ è°ƒç”¨LLMç”Ÿæˆé«˜è´¨é‡åˆ†æç»“æœ
        5. ğŸ¨ ç¾åŒ–å’Œæ ¼å¼åŒ–åˆ†æç»“æœ
        6. ğŸ”„ LLMä¸å¯ç”¨æ—¶è‡ªåŠ¨å›é€€åˆ°åŸºç¡€åˆ†æ

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼Œç”¨äºè·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
            content_data: åŒ…å«ç½‘é¡µå†…å®¹çš„ç»“æ„åŒ–å­—å…¸ï¼š
                {
                    'title': ç½‘é¡µæ ‡é¢˜,
                    'content': ç½‘é¡µæ­£æ–‡å†…å®¹,
                    'url': ç½‘é¡µURLåœ°å€
                }

        Returns:
            æ ¼å¼åŒ–çš„AIåˆ†æç»“æœæ–‡æœ¬ï¼ŒåŒ…å«æ ‡é¢˜ã€é“¾æ¥ã€åˆ†æå†…å®¹ç­‰
        """
        try:
            title = content_data["title"]
            content = content_data["content"]
            url = content_data["url"]

            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨å’Œå¯ç”¨
            if not hasattr(self.context, "llm_generate") or not self.llm_enabled:
                # LLMä¸å¯ç”¨æˆ–æœªå¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
                return self.get_enhanced_analysis(content_data)

            # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„LLMæä¾›å•†ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨å½“å‰ä¼šè¯çš„æ¨¡å‹
            provider_id = self.llm_provider
            if not provider_id:
                umo = event.unified_msg_origin
                provider_id = await self.context.get_current_chat_provider_id(umo=umo)

            if not provider_id:
                # æ— æ³•è·å–LLMæä¾›å•†ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
                return self.get_enhanced_analysis(content_data)

            # æ„å»ºä¼˜åŒ–çš„LLMæç¤ºè¯
            emoji_prefix = "æ¯ä¸ªè¦ç‚¹ç”¨emojiå›¾æ ‡æ ‡è®°" if self.enable_emoji else ""

            # ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯æˆ–é»˜è®¤æç¤ºè¯
            if self.custom_prompt:
                # æ›¿æ¢è‡ªå®šä¹‰æç¤ºè¯ä¸­çš„å˜é‡
                prompt = self.custom_prompt.format(
                    title=title,
                    url=url,
                    content=content,
                    max_length=self.max_summary_length,
                )
            else:
                # é»˜è®¤æç¤ºè¯ï¼ŒåŒ…å«è¯¦ç»†çš„åˆ†æè¦æ±‚å’Œæ ¼å¼è¦æ±‚
                prompt = f"""è¯·å¯¹ä»¥ä¸‹ç½‘é¡µå†…å®¹è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{title}
- é“¾æ¥ï¼š{url}

**ç½‘é¡µå†…å®¹**ï¼š
{content}

**åˆ†æè¦æ±‚**ï¼š
1. **æ ¸å¿ƒæ‘˜è¦**ï¼šç”¨50-100å­—æ¦‚æ‹¬ç½‘é¡µçš„æ ¸å¿ƒå†…å®¹å’Œä¸»æ—¨
2. **å…³é”®è¦ç‚¹**ï¼šæå–2-3ä¸ªæœ€é‡è¦çš„ä¿¡æ¯ç‚¹æˆ–è§‚ç‚¹
3. **å†…å®¹ç±»å‹**ï¼šåˆ¤æ–­ç½‘é¡µå±äºä»€ä¹ˆç±»å‹ï¼ˆæ–°é—»ã€æ•™ç¨‹ã€åšå®¢ã€äº§å“ä»‹ç»ç­‰ï¼‰
4. **ä»·å€¼è¯„ä¼°**ï¼šç®€è¦è¯„ä»·å†…å®¹çš„ä»·å€¼å’Œå®ç”¨æ€§
5. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜é€‚åˆå“ªäº›äººç¾¤é˜…è¯»

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{self.max_summary_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚"""

            # ä½¿ç”¨å½“å‰ä¼šè¯çš„èŠå¤©æ¨¡å‹IDè°ƒç”¨å¤§æ¨¡å‹
            llm_resp = await self.context.llm_generate(
                chat_provider_id=provider_id,  # ä½¿ç”¨å½“å‰ä¼šè¯çš„èŠå¤©æ¨¡å‹
                prompt=prompt,
            )

            if llm_resp and llm_resp.completion_text:
                # ç¾åŒ–LLMè¿”å›çš„ç»“æœ
                analysis_text = llm_resp.completion_text.strip()

                # é™åˆ¶æ‘˜è¦é•¿åº¦ï¼Œé¿å…ç»“æœè¿‡é•¿
                if len(analysis_text) > self.max_summary_length:
                    analysis_text = analysis_text[: self.max_summary_length] + "..."

                # æ·»åŠ æ ‡é¢˜å’Œæ ¼å¼ç¾åŒ–
                link_emoji = "ğŸ”—" if self.enable_emoji else ""
                title_emoji = "ğŸ“" if self.enable_emoji else ""

                formatted_result = "**AIæ™ºèƒ½ç½‘é¡µåˆ†ææŠ¥å‘Š**\n\n"
                formatted_result += f"{link_emoji} **åˆ†æé“¾æ¥**: {url}\n"
                formatted_result += f"{title_emoji} **ç½‘é¡µæ ‡é¢˜**: {title}\n\n"
                formatted_result += "---\n\n"
                formatted_result += analysis_text
                formatted_result += "\n\n---\n"
                formatted_result += "*åˆ†æå®Œæˆï¼Œå¸Œæœ›å¯¹æ‚¨æœ‰å¸®åŠ©ï¼*"

                return formatted_result
            else:
                # LLMè¿”å›ä¸ºç©ºï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
                return self.get_enhanced_analysis(content_data)

        except Exception as e:
            logger.error(f"LLMåˆ†æå¤±è´¥: {e}")
            # å¦‚æœLLMåˆ†æå¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
            return f"âŒ LLMåˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"

    def get_enhanced_analysis(self, content_data: dict) -> str:
        """å¢å¼ºç‰ˆåŸºç¡€åˆ†æ - LLMä¸å¯ç”¨æ—¶çš„æ™ºèƒ½å›é€€æ–¹æ¡ˆ

        å½“LLMä¸å¯ç”¨æˆ–æœªå¯ç”¨æ—¶ï¼Œæä¾›å¯é çš„åŸºç¡€åˆ†æåŠŸèƒ½ï¼Œ
        åŒ…å«å¤šç§æ™ºèƒ½åˆ†æç‰¹æ€§ï¼Œç¡®ä¿æ’ä»¶åœ¨å„ç§ç¯å¢ƒä¸‹éƒ½èƒ½æ­£å¸¸å·¥ä½œï¼š
        
        ğŸ“Š åˆ†æå†…å®¹ï¼š
        1. ğŸ”¢ å†…å®¹ç»Ÿè®¡ï¼ˆå­—ç¬¦æ•°ã€æ®µè½æ•°ã€è¯æ•°ï¼‰
        2. ğŸ§  æ™ºèƒ½å†…å®¹ç±»å‹æ£€æµ‹ï¼ˆæ–°é—»ã€æ•™ç¨‹ã€åšå®¢ç­‰ï¼‰
        3. ğŸ” æå–å…³é”®å¥å­ä½œä¸ºå†…å®¹æ‘˜è¦
        4. â­ å†…å®¹è´¨é‡è¯„ä¼°
        5. ğŸ¨ ç¾è§‚çš„æ ¼å¼åŒ–è¾“å‡º

        âœ¨ é…ç½®æ”¯æŒï¼š
        - æ ¹æ®é…ç½®æ˜¾ç¤º/éšè—emoji
        - æ ¹æ®é…ç½®æ˜¾ç¤º/éšè—è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        - æ”¯æŒè‡ªå®šä¹‰æ ¼å¼

        Args:
            content_data: åŒ…å«ç½‘é¡µå†…å®¹çš„ç»“æ„åŒ–å­—å…¸ï¼š
                {
                    'title': ç½‘é¡µæ ‡é¢˜,
                    'content': ç½‘é¡µæ­£æ–‡å†…å®¹,
                    'url': ç½‘é¡µURLåœ°å€
                }

        Returns:
            æ ¼å¼åŒ–çš„åŸºç¡€åˆ†æç»“æœæ–‡æœ¬ï¼ŒåŒ…å«æ‰€æœ‰åˆ†æå†…å®¹
        """
        title = content_data["title"]
        content = content_data["content"]
        url = content_data["url"]

        # è®¡ç®—å†…å®¹ç»Ÿè®¡ä¿¡æ¯
        char_count = len(content)
        word_count = len(content.split())

        # æ™ºèƒ½æ£€æµ‹å†…å®¹ç±»å‹
        content_lower = content.lower()
        content_type = "æ–‡ç« "
        if any(
            keyword in content_lower for keyword in ["æ–°é—»", "æŠ¥é“", "æ¶ˆæ¯", "æ—¶äº‹"]
        ):
            content_type = "æ–°é—»èµ„è®¯"
        elif any(
            keyword in content_lower
            for keyword in ["æ•™ç¨‹", "æŒ‡å—", "æ•™å­¦", "æ­¥éª¤", "æ–¹æ³•"]
        ):
            content_type = "æ•™ç¨‹æŒ‡å—"
        elif any(
            keyword in content_lower
            for keyword in ["åšå®¢", "éšç¬”", "æ—¥è®°", "ä¸ªäºº", "è§‚ç‚¹"]
        ):
            content_type = "ä¸ªäººåšå®¢"
        elif any(
            keyword in content_lower
            for keyword in ["äº§å“", "æœåŠ¡", "è´­ä¹°", "ä»·æ ¼", "ä¼˜æƒ "]
        ):
            content_type = "äº§å“ä»‹ç»"
        elif any(
            keyword in content_lower
            for keyword in ["æŠ€æœ¯", "å¼€å‘", "ç¼–ç¨‹", "ä»£ç ", "API"]
        ):
            content_type = "æŠ€æœ¯æ–‡æ¡£"

        # æå–å…³é”®æ®µè½ä½œä¸ºå†…å®¹æ‘˜è¦
        paragraphs = [p.strip() for p in content.split("\n") if p.strip()]
        key_sentences = paragraphs[:3]

        # è¯„ä¼°å†…å®¹è´¨é‡
        quality_indicator = "å†…å®¹ä¸°å¯Œ" if char_count > 1000 else "å†…å®¹ç®€æ´"
        if char_count > 5000:
            quality_indicator = "å†…å®¹è¯¦å®"

        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨emoji
        robot_emoji = "ğŸ¤–" if self.enable_emoji else ""
        page_emoji = "ğŸ“„" if self.enable_emoji else ""
        info_emoji = "ğŸ“" if self.enable_emoji else ""
        stats_emoji = "ğŸ“Š" if self.enable_emoji else ""
        search_emoji = "ğŸ”" if self.enable_emoji else ""
        light_emoji = "ğŸ’¡" if self.enable_emoji else ""

        # æ„å»ºåˆ†æç»“æœ
        result = f"{robot_emoji} **æ™ºèƒ½ç½‘é¡µåˆ†æ** {page_emoji}\n\n"

        # æ·»åŠ åŸºæœ¬ä¿¡æ¯
        if self.enable_emoji:
            result += f"**{info_emoji} åŸºæœ¬ä¿¡æ¯**\n"
        else:
            result += "**åŸºæœ¬ä¿¡æ¯**\n"
        result += f"- **æ ‡é¢˜**: {title}\n"
        result += f"- **é“¾æ¥**: {url}\n"
        result += f"- **å†…å®¹ç±»å‹**: {content_type}\n"
        result += f"- **è´¨é‡è¯„ä¼°**: {quality_indicator}\n\n"

        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if self.enable_statistics:
            if self.enable_emoji:
                result += f"**{stats_emoji} å†…å®¹ç»Ÿè®¡**\n"
            else:
                result += "**å†…å®¹ç»Ÿè®¡**\n"
            result += f"- å­—ç¬¦æ•°: {char_count:,}\n"
            result += f"- æ®µè½æ•°: {len(paragraphs)}\n"
            result += f"- è¯æ•°: {word_count:,}\n\n"

        # æ·»åŠ å†…å®¹æ‘˜è¦
        if self.enable_emoji:
            result += f"**{search_emoji} å†…å®¹æ‘˜è¦**\n"
        else:
            result += "**å†…å®¹æ‘˜è¦**\n"
        result += f"{chr(10).join(['â€¢ ' + sentence[:100] + ('...' if len(sentence) > 100 else '') for sentence in key_sentences])}\n\n"

        # æ·»åŠ åˆ†æè¯´æ˜
        if self.enable_emoji:
            result += f"**{light_emoji} åˆ†æè¯´æ˜**\n"
        else:
            result += "**åˆ†æè¯´æ˜**\n"
        result += "æ­¤åˆ†æåŸºäºç½‘é¡µå†…å®¹æå–ï¼Œå¦‚éœ€æ›´æ·±å…¥çš„AIæ™ºèƒ½åˆ†æï¼Œè¯·ç¡®ä¿AstrBotå·²æ­£ç¡®é…ç½®LLMåŠŸèƒ½ã€‚\n\n"
        result += "*æç¤ºï¼šå®Œæ•´å†…å®¹é¢„è§ˆè¯·æŸ¥çœ‹åŸå§‹ç½‘é¡µ*"

        return result

    @filter.command("web_config", alias={"ç½‘é¡µåˆ†æé…ç½®", "ç½‘é¡µåˆ†æè®¾ç½®"})
    async def show_config(self, event: AstrMessageEvent):
        """æ˜¾ç¤ºå½“å‰æ’ä»¶çš„è¯¦ç»†é…ç½®ä¿¡æ¯

        è¿™ä¸ªå‘½ä»¤å…è®¸ç”¨æˆ·æŸ¥çœ‹æ’ä»¶çš„æ‰€æœ‰é…ç½®é¡¹ï¼Œæ–¹ä¾¿äº†è§£æ’ä»¶çš„å½“å‰çŠ¶æ€å’Œè®¾ç½®ï¼Œ
        æ”¯æŒå¤šç§å‘½ä»¤åˆ«åï¼Œæ–¹ä¾¿ç”¨æˆ·è°ƒç”¨ã€‚

        ğŸ“‹ æ˜¾ç¤ºå†…å®¹ï¼š
        - ğŸ› ï¸  åŸºæœ¬è®¾ç½®ï¼ˆè¶…æ—¶ã€é‡è¯•ã€è‡ªåŠ¨åˆ†æç­‰ï¼‰
        - ğŸš«  åŸŸåæ§åˆ¶ï¼ˆå…è®¸/ç¦æ­¢åˆ—è¡¨ï¼‰
        - ğŸ‘¥  ç¾¤èŠæ§åˆ¶ï¼ˆé»‘åå•ï¼‰
        - ğŸ“Š  åˆ†æè®¾ç½®ï¼ˆemojiã€ç»Ÿè®¡ã€æ‘˜è¦é•¿åº¦ç­‰ï¼‰
        - ğŸ§   LLMé…ç½®ï¼ˆæä¾›å•†ã€è‡ªå®šä¹‰æç¤ºè¯ç­‰ï¼‰
        - ğŸŒ  ç¿»è¯‘è®¾ç½®ï¼ˆè‡ªåŠ¨ç¿»è¯‘ã€ç›®æ ‡è¯­è¨€ç­‰ï¼‰
        - ğŸ’¾  ç¼“å­˜è®¾ç½®ï¼ˆè¿‡æœŸæ—¶é—´ã€æœ€å¤§æ•°é‡ç­‰ï¼‰
        - ğŸ“‹  å†…å®¹æå–è®¾ç½®ï¼ˆæå–ç±»å‹ç­‰ï¼‰

        ğŸ“ ä½¿ç”¨ç¤ºä¾‹ï¼š
        - `/web_config` - æŸ¥çœ‹é…ç½®
        - `/ç½‘é¡µåˆ†æé…ç½®` - ä¸­æ–‡å‘½ä»¤
        - `/ç½‘é¡µåˆ†æè®¾ç½®` - ä¸­æ–‡åˆ«å

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼Œç”¨äºç”Ÿæˆå“åº”
        """
        config_info = f"""**ç½‘é¡µåˆ†ææ’ä»¶é…ç½®ä¿¡æ¯**

**åŸºæœ¬è®¾ç½®**
- æœ€å¤§å†…å®¹é•¿åº¦: {self.max_content_length} å­—ç¬¦
- è¯·æ±‚è¶…æ—¶æ—¶é—´: {self.timeout} ç§’
- LLMæ™ºèƒ½åˆ†æ: {"âœ… å·²å¯ç”¨" if self.llm_enabled else "âŒ å·²ç¦ç”¨"}
- è‡ªåŠ¨åˆ†æé“¾æ¥: {"âœ… å·²å¯ç”¨" if self.auto_analyze else "âŒ å·²ç¦ç”¨"}
- åˆå¹¶è½¬å‘åŠŸèƒ½(ç¾¤èŠ): {"âœ… å·²å¯ç”¨" if self.merge_forward_enabled["group"] else "âŒ å·²ç¦ç”¨"}
- åˆå¹¶è½¬å‘åŠŸèƒ½(ç§èŠ): {"âœ… å·²å¯ç”¨" if self.merge_forward_enabled["private"] else "âŒ å·²ç¦ç”¨"}

**åŸŸåæ§åˆ¶**
- å…è®¸åŸŸå: {len(self.allowed_domains)} ä¸ª
- ç¦æ­¢åŸŸå: {len(self.blocked_domains)} ä¸ª

**ç¾¤èŠæ§åˆ¶**
- ç¾¤èŠé»‘åå•: {len(self.group_blacklist)} ä¸ªç¾¤èŠ

**åˆ†æè®¾ç½®**
- å¯ç”¨emoji: {"âœ… å·²å¯ç”¨" if self.enable_emoji else "âŒ å·²ç¦ç”¨"}
- æ˜¾ç¤ºç»Ÿè®¡: {"âœ… å·²å¯ç”¨" if self.enable_statistics else "âŒ å·²ç¦ç”¨"}
- æœ€å¤§æ‘˜è¦é•¿åº¦: {self.max_summary_length} å­—ç¬¦
- å¯ç”¨æˆªå›¾: {"âœ… å·²å¯ç”¨" if self.enable_screenshot else "âŒ å·²ç¦ç”¨"}
- æˆªå›¾è´¨é‡: {self.screenshot_quality}
- æˆªå›¾å®½åº¦: {self.screenshot_width}px
- æˆªå›¾é«˜åº¦: {self.screenshot_height}px
- æˆªå›¾æ ¼å¼: {self.screenshot_format}
- æˆªå–æ•´é¡µ: {"âœ… å·²å¯ç”¨" if self.screenshot_full_page else "âŒ å·²ç¦ç”¨"}
- æˆªå›¾ç­‰å¾…æ—¶é—´: {self.screenshot_wait_time}ms

**LLMé…ç½®**
- æŒ‡å®šæä¾›å•†: {self.llm_provider if self.llm_provider else "ä½¿ç”¨ä¼šè¯é»˜è®¤"}
- è‡ªå®šä¹‰æç¤ºè¯: {"âœ… å·²å¯ç”¨" if self.custom_prompt else "âŒ æœªè®¾ç½®"}

**ç¿»è¯‘è®¾ç½®**
- å¯ç”¨ç½‘é¡µç¿»è¯‘: {"âœ… å·²å¯ç”¨" if self.enable_translation else "âŒ å·²ç¦ç”¨"}
- ç›®æ ‡è¯­è¨€: {self.target_language}
- ç¿»è¯‘æä¾›å•†: {self.translation_provider}
- è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯: {"âœ… å·²å¯ç”¨" if self.custom_translation_prompt else "âŒ æœªè®¾ç½®"}

**ç¼“å­˜è®¾ç½®**
- å¯ç”¨ç»“æœç¼“å­˜: {"âœ… å·²å¯ç”¨" if self.enable_cache else "âŒ å·²ç¦ç”¨"}
- ç¼“å­˜è¿‡æœŸæ—¶é—´: {self.cache_expire_time} åˆ†é’Ÿ
- æœ€å¤§ç¼“å­˜æ•°é‡: {self.max_cache_size} ä¸ª

**å†…å®¹æå–è®¾ç½®**
- å¯ç”¨ç‰¹å®šå†…å®¹æå–: {"âœ… å·²å¯ç”¨" if self.enable_specific_extraction else "âŒ å·²ç¦ç”¨"}
- æå–å†…å®¹ç±»å‹: {", ".join(self.extract_types)}

*æç¤º: å¦‚éœ€ä¿®æ”¹é…ç½®ï¼Œè¯·åœ¨AstrBotç®¡ç†é¢æ¿ä¸­ç¼–è¾‘æ’ä»¶é…ç½®*"""

        yield event.plain_result(config_info)

    @filter.command("test_merge", alias={"æµ‹è¯•åˆå¹¶è½¬å‘", "æµ‹è¯•è½¬å‘"})
    async def test_merge_forward(self, event: AstrMessageEvent):
        """æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½

        è¿™ä¸ªå‘½ä»¤ç”¨äºæµ‹è¯•æ’ä»¶çš„åˆå¹¶è½¬å‘åŠŸèƒ½ï¼Œä»…åœ¨ç¾¤èŠç¯å¢ƒä¸­å¯ç”¨ï¼Œ
        æ–¹ä¾¿ç”¨æˆ·éªŒè¯åˆå¹¶è½¬å‘åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚

        ğŸ“ ä½¿ç”¨ç¤ºä¾‹ï¼š
        - `/test_merge` - æµ‹è¯•åˆå¹¶è½¬å‘
        - `/æµ‹è¯•åˆå¹¶è½¬å‘` - ä¸­æ–‡å‘½ä»¤
        - `/æµ‹è¯•è½¬å‘` - ä¸­æ–‡åˆ«å

        âœ¨ åŠŸèƒ½è¯´æ˜ï¼š
        - åˆ›å»ºæµ‹è¯•ç”¨çš„åˆå¹¶è½¬å‘æ¶ˆæ¯
        - åŒ…å«æ ‡é¢˜èŠ‚ç‚¹å’Œä¸¤ä¸ªå†…å®¹èŠ‚ç‚¹
        - æ¼”ç¤ºåˆå¹¶è½¬å‘çš„åŸºæœ¬ç”¨æ³•å’Œæ•ˆæœ
        - éªŒè¯åˆå¹¶è½¬å‘åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼Œç”¨äºç”Ÿæˆæµ‹è¯•æ¶ˆæ¯
        """
        from astrbot.api.message_components import Node, Plain, Nodes

        # æ£€æŸ¥æ˜¯å¦ä¸ºç¾¤èŠæ¶ˆæ¯ï¼Œåˆå¹¶è½¬å‘ä»…æ”¯æŒç¾¤èŠ
        group_id = None
        if hasattr(event, "group_id") and event.group_id:
            group_id = event.group_id
        elif (
            hasattr(event, "message_obj")
            and hasattr(event.message_obj, "group_id")
            and event.message_obj.group_id
        ):
            group_id = event.message_obj.group_id

        if group_id:
            # åˆ›å»ºæµ‹è¯•ç”¨çš„åˆå¹¶è½¬å‘èŠ‚ç‚¹
            nodes = []

            # æ ‡é¢˜èŠ‚ç‚¹
            title_node = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•åˆå¹¶è½¬å‘",
                content=[Plain("è¿™æ˜¯åˆå¹¶è½¬å‘æµ‹è¯•æ¶ˆæ¯")],
            )
            nodes.append(title_node)

            # å†…å®¹èŠ‚ç‚¹1
            content_node1 = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•èŠ‚ç‚¹1",
                content=[Plain("è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•èŠ‚ç‚¹å†…å®¹")],
            )
            nodes.append(content_node1)

            # å†…å®¹èŠ‚ç‚¹2
            content_node2 = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•èŠ‚ç‚¹2",
                content=[Plain("è¿™æ˜¯ç¬¬äºŒä¸ªæµ‹è¯•èŠ‚ç‚¹å†…å®¹")],
            )
            nodes.append(content_node2)

            # ä½¿ç”¨NodesåŒ…è£…æ‰€æœ‰èŠ‚ç‚¹ï¼Œåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
            merge_forward_message = Nodes(nodes)
            yield event.chain_result([merge_forward_message])
            logger.info(f"æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½ï¼Œç¾¤èŠ {group_id}")
        else:
            yield event.plain_result("åˆå¹¶è½¬å‘åŠŸèƒ½ä»…æ”¯æŒç¾¤èŠæ¶ˆæ¯æµ‹è¯•")
            logger.info("ç§èŠæ¶ˆæ¯æ— æ³•æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½")

    @filter.command("group_blacklist", alias={"ç¾¤é»‘åå•", "é»‘åå•"})
    async def manage_group_blacklist(self, event: AstrMessageEvent):
        """ç®¡ç†ç¾¤èŠé»‘åå•

        è¿™ä¸ªå‘½ä»¤å…è®¸ç”¨æˆ·ç®¡ç†æ’ä»¶çš„ç¾¤èŠé»‘åå•ï¼Œæ§åˆ¶å“ªäº›ç¾¤èŠä¸èƒ½ä½¿ç”¨æ’ä»¶ï¼Œ
        æ”¯æŒå¤šç§æ“ä½œï¼ŒåŒ…æ‹¬æŸ¥çœ‹ã€æ·»åŠ ã€ç§»é™¤å’Œæ¸…ç©ºé»‘åå•ã€‚

        ğŸ“‹ å‘½ä»¤ç”¨æ³•ï¼š
        1. ğŸ” æŸ¥çœ‹é»‘åå•ï¼š`/group_blacklist`
        2. â• æ·»åŠ ç¾¤èŠï¼š`/group_blacklist add <ç¾¤å·>`
        3. â– ç§»é™¤ç¾¤èŠï¼š`/group_blacklist remove <ç¾¤å·>`
        4. ğŸ—‘ï¸ æ¸…ç©ºé»‘åå•ï¼š`/group_blacklist clear`

        ğŸ”¤ æ”¯æŒåˆ«åï¼š
        - `/ç¾¤é»‘åå•`
        - `/é»‘åå•`

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼Œç”¨äºè·å–å‘½ä»¤å‚æ•°å’Œç”Ÿæˆå“åº”
        """
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()

        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰é»‘åå•åˆ—è¡¨
        if len(message_parts) <= 1:
            if not self.group_blacklist:
                yield event.plain_result("å½“å‰ç¾¤èŠé»‘åå•ä¸ºç©º")
                return

            blacklist_info = "**å½“å‰ç¾¤èŠé»‘åå•**\n\n"
            for i, group_id in enumerate(self.group_blacklist, 1):
                blacklist_info += f"{i}. {group_id}\n"

            blacklist_info += "\nä½¿ç”¨ `/group_blacklist add <ç¾¤å·>` æ·»åŠ ç¾¤èŠåˆ°é»‘åå•"
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist remove <ç¾¤å·>` ä»é»‘åå•ç§»é™¤ç¾¤èŠ"
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist clear` æ¸…ç©ºé»‘åå•"

            yield event.plain_result(blacklist_info)
            return

        # è§£ææ“ä½œç±»å‹å’Œå‚æ•°
        action = message_parts[1].lower() if len(message_parts) > 1 else ""
        group_id = message_parts[2] if len(message_parts) > 2 else ""

        # æ·»åŠ ç¾¤èŠåˆ°é»‘åå•
        if action == "add" and group_id:
            if group_id in self.group_blacklist:
                yield event.plain_result(f"ç¾¤èŠ {group_id} å·²åœ¨é»‘åå•ä¸­")
                return

            self.group_blacklist.append(group_id)
            self._save_group_blacklist()
            yield event.plain_result(f"âœ… å·²æ·»åŠ ç¾¤èŠ {group_id} åˆ°é»‘åå•")

        # ä»é»‘åå•ç§»é™¤ç¾¤èŠ
        elif action == "remove" and group_id:
            if group_id not in self.group_blacklist:
                yield event.plain_result(f"ç¾¤èŠ {group_id} ä¸åœ¨é»‘åå•ä¸­")
                return

            self.group_blacklist.remove(group_id)
            self._save_group_blacklist()
            yield event.plain_result(f"âœ… å·²ä»é»‘åå•ç§»é™¤ç¾¤èŠ {group_id}")

        # æ¸…ç©ºé»‘åå•
        elif action == "clear":
            if not self.group_blacklist:
                yield event.plain_result("é»‘åå•å·²ä¸ºç©º")
                return

            self.group_blacklist.clear()
            self._save_group_blacklist()
            yield event.plain_result("âœ… å·²æ¸…ç©ºç¾¤èŠé»‘åå•")

        # æ— æ•ˆæ“ä½œ
        else:
            yield event.plain_result(
                "æ— æ•ˆçš„æ“ä½œï¼Œè¯·ä½¿ç”¨: add <ç¾¤å·>, remove <ç¾¤å·>, clear"
            )

    @filter.command("web_cache", alias={"ç½‘é¡µç¼“å­˜", "æ¸…ç†ç¼“å­˜"})
    async def manage_cache(self, event: AstrMessageEvent):
        """ç®¡ç†æ’ä»¶çš„ç½‘é¡µåˆ†æç»“æœç¼“å­˜

        è¿™ä¸ªå‘½ä»¤å…è®¸ç”¨æˆ·ç®¡ç†æ’ä»¶çš„ç¼“å­˜ï¼ŒåŒ…æ‹¬æŸ¥çœ‹ç¼“å­˜çŠ¶æ€å’Œæ¸…ç©ºç¼“å­˜ï¼Œ
        æ–¹ä¾¿ç”¨æˆ·æ§åˆ¶ç¼“å­˜çš„ä½¿ç”¨å’Œé‡Šæ”¾å­˜å‚¨ç©ºé—´ã€‚

        ğŸ“‹ å‘½ä»¤ç”¨æ³•ï¼š
        1. ğŸ” æŸ¥çœ‹ç¼“å­˜çŠ¶æ€ï¼š`/web_cache`
        2. ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ï¼š`/web_cache clear`

        ğŸ”¤ æ”¯æŒåˆ«åï¼š
        - `/ç½‘é¡µç¼“å­˜`
        - `/æ¸…ç†ç¼“å­˜`

        ğŸ’¡ ç¼“å­˜è¯´æ˜ï¼š
        - â° ç¼“å­˜æœ‰æ•ˆæœŸï¼šç”±é…ç½®ä¸­çš„`cache_expire_time`å†³å®š
        - ğŸ“Š æœ€å¤§ç¼“å­˜æ•°é‡ï¼šç”±é…ç½®ä¸­çš„`max_cache_size`å†³å®š
        - ğŸ“¦ ç¼“å­˜å†…å®¹ï¼šåŒ…æ‹¬ç½‘é¡µåˆ†æç»“æœå’Œæˆªå›¾æ•°æ®

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼Œç”¨äºè·å–å‘½ä»¤å‚æ•°å’Œç”Ÿæˆå“åº”
        """
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()

        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰ç¼“å­˜çŠ¶æ€
        if len(message_parts) <= 1:
            cache_stats = self.cache_manager.get_stats()
            cache_info = "**å½“å‰ç¼“å­˜çŠ¶æ€**\n\n"
            cache_info += f"- ç¼“å­˜æ€»æ•°: {cache_stats['total']} ä¸ª\n"
            cache_info += f"- æœ‰æ•ˆç¼“å­˜: {cache_stats['valid']} ä¸ª\n"
            cache_info += f"- è¿‡æœŸç¼“å­˜: {cache_stats['expired']} ä¸ª\n"
            cache_info += f"- ç¼“å­˜è¿‡æœŸæ—¶é—´: {self.cache_expire_time} åˆ†é’Ÿ\n"
            cache_info += f"- æœ€å¤§ç¼“å­˜æ•°é‡: {self.max_cache_size} ä¸ª\n"
            cache_info += (
                f"- ç¼“å­˜åŠŸèƒ½: {'âœ… å·²å¯ç”¨' if self.enable_cache else 'âŒ å·²ç¦ç”¨'}\n"
            )

            cache_info += "\nä½¿ç”¨ `/web_cache clear` æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"

            yield event.plain_result(cache_info)
            return

        # è§£ææ“ä½œç±»å‹
        action = message_parts[1].lower() if len(message_parts) > 1 else ""

        # æ¸…ç©ºç¼“å­˜æ“ä½œ
        if action == "clear":
            # æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
            self.cache_manager.clear()
            cache_stats = self.cache_manager.get_stats()
            yield event.plain_result(
                f"âœ… å·²æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ï¼Œå½“å‰ç¼“å­˜æ•°é‡: {cache_stats['total']} ä¸ª"
            )

        # æ— æ•ˆæ“ä½œ
        else:
            yield event.plain_result("æ— æ•ˆçš„æ“ä½œï¼Œè¯·ä½¿ç”¨: clear")

    @filter.command("web_export", alias={"å¯¼å‡ºåˆ†æç»“æœ", "ç½‘é¡µå¯¼å‡º"})
    async def export_analysis_result(self, event: AstrMessageEvent):
        """å¯¼å‡ºç½‘é¡µåˆ†æç»“æœ

        è¿™ä¸ªå‘½ä»¤å…è®¸ç”¨æˆ·å¯¼å‡ºç½‘é¡µåˆ†æç»“æœï¼Œæ”¯æŒå¤šç§æ ¼å¼å’Œå¯¼å‡ºèŒƒå›´ï¼Œ
        æ–¹ä¾¿ç”¨æˆ·ä¿å­˜å’Œåˆ†äº«åˆ†æç»“æœã€‚

        ğŸ“‹ å‘½ä»¤ç”¨æ³•ï¼š
        1. ğŸ“„ å¯¼å‡ºå•ä¸ªURLï¼š`/web_export https://example.com [æ ¼å¼]`
        2. ğŸ“š å¯¼å‡ºæ‰€æœ‰ç¼“å­˜ï¼š`/web_export all [æ ¼å¼]`

        ğŸ“ æ”¯æŒçš„æ ¼å¼ï¼š
        - ğŸ“ `md/markdown`ï¼šMarkdownæ ¼å¼ï¼Œé€‚åˆé˜…è¯»å’Œç¼–è¾‘
        - ğŸ“Š `json`ï¼šJSONæ ¼å¼ï¼Œé€‚åˆç¨‹åºå¤„ç†
        - ğŸ“„ `txt`ï¼šçº¯æ–‡æœ¬æ ¼å¼ï¼Œé€‚åˆç®€å•æŸ¥çœ‹

        ğŸ”¤ æ”¯æŒåˆ«åï¼š
        - `/å¯¼å‡ºåˆ†æç»“æœ`
        - `/ç½‘é¡µå¯¼å‡º`

        âœ¨ åŠŸèƒ½ç‰¹ç‚¹ï¼š
        - ğŸ”„ æ”¯æŒå¯¼å‡ºå•ä¸ªURLçš„åˆ†æç»“æœ
        - ğŸ“š æ”¯æŒå¯¼å‡ºæ‰€æœ‰ç¼“å­˜çš„åˆ†æç»“æœ
        - ğŸ“¤ å¯¼å‡ºæ–‡ä»¶ä¼šè‡ªåŠ¨å‘é€ç»™ç”¨æˆ·
        - ğŸ” å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ç»“æœï¼Œä¼šå…ˆè¿›è¡Œåˆ†æ
        - ğŸ“¦ æ”¯æŒå¤šç§å¯¼å‡ºæ ¼å¼ï¼Œæ»¡è¶³ä¸åŒéœ€æ±‚

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼Œç”¨äºè·å–å‘½ä»¤å‚æ•°å’Œç”Ÿæˆå“åº”
        """
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()

        # æ£€æŸ¥å‚æ•°æ˜¯å¦è¶³å¤Ÿ
        if len(message_parts) < 2:
            yield event.plain_result(
                "è¯·æä¾›è¦å¯¼å‡ºçš„URLé“¾æ¥å’Œæ ¼å¼ï¼Œä¾‹å¦‚ï¼š/web_export https://example.com md æˆ– /web_export all json"
            )
            return

        # è·å–å¯¼å‡ºèŒƒå›´å’Œæ ¼å¼
        url_or_all = message_parts[1]
        format_type = message_parts[2] if len(message_parts) > 2 else "md"

        # éªŒè¯æ ¼å¼ç±»å‹æ˜¯å¦æ”¯æŒ
        supported_formats = ["md", "markdown", "json", "txt"]
        if format_type.lower() not in supported_formats:
            yield event.plain_result(
                f"ä¸æ”¯æŒçš„æ ¼å¼ç±»å‹ï¼Œè¯·ä½¿ç”¨ï¼š{', '.join(supported_formats)}"
            )
            return

        # å‡†å¤‡å¯¼å‡ºæ•°æ®
        export_results = []

        if url_or_all.lower() == "all":
            # å¯¼å‡ºæ‰€æœ‰ç¼“å­˜çš„åˆ†æç»“æœ
            if not self.cache:
                yield event.plain_result("å½“å‰æ²¡æœ‰ç¼“å­˜çš„åˆ†æç»“æœ")
                return

            for url, cache_data in self.cache.items():
                export_results.append({"url": url, "result": cache_data["result"]})
        else:
            # å¯¼å‡ºæŒ‡å®šURLçš„åˆ†æç»“æœ
            url = url_or_all

            # æ£€æŸ¥URLæ ¼å¼æ˜¯å¦æœ‰æ•ˆ
            if not self.analyzer.is_valid_url(url):
                yield event.plain_result("æ— æ•ˆçš„URLé“¾æ¥")
                return

            # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰è¯¥URLçš„åˆ†æç»“æœ
            cached_result = self._check_cache(url)
            if cached_result:
                export_results.append({"url": url, "result": cached_result})
            else:
                # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå…ˆè¿›è¡Œåˆ†æ
                yield event.plain_result("ç¼“å­˜ä¸­æ²¡æœ‰è¯¥URLçš„åˆ†æç»“æœï¼Œæ­£åœ¨è¿›è¡Œåˆ†æ...")

                # æŠ“å–å¹¶åˆ†æç½‘é¡µ
                async with WebAnalyzer(
                    self.max_content_length,
                    self.timeout,
                    self.user_agent,
                    self.proxy,
                    self.retry_count,
                    self.retry_delay,
                ) as analyzer:
                    html = await analyzer.fetch_webpage(url)
                    if not html:
                        yield event.plain_result(f"æ— æ³•æŠ“å–ç½‘é¡µå†…å®¹: {url}")
                        return

                    content_data = analyzer.extract_content(html, url)
                    if not content_data:
                        yield event.plain_result(f"æ— æ³•è§£æç½‘é¡µå†…å®¹: {url}")
                        return

                    # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
                    if self.enable_translation:
                        translated_content = await self._translate_content(
                            event, content_data["content"]
                        )
                        translated_content_data = content_data.copy()
                        translated_content_data["content"] = translated_content
                        analysis_result = await self.analyze_with_llm(
                            event, translated_content_data
                        )
                    else:
                        analysis_result = await self.analyze_with_llm(
                            event, content_data
                        )

                    # æå–ç‰¹å®šå†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    specific_content = self._extract_specific_content(html, url)
                    if specific_content:
                        # åœ¨åˆ†æç»“æœä¸­æ·»åŠ ç‰¹å®šå†…å®¹
                        specific_content_str = "\n\n**ç‰¹å®šå†…å®¹æå–**\n"

                        if "images" in specific_content and specific_content["images"]:
                            specific_content_str += (
                                f"\nğŸ“· å›¾ç‰‡é“¾æ¥ ({len(specific_content['images'])}):\n"
                            )
                            for img_url in specific_content["images"]:
                                specific_content_str += f"- {img_url}\n"

                        if "links" in specific_content and specific_content["links"]:
                            specific_content_str += (
                                f"\nğŸ”— ç›¸å…³é“¾æ¥ ({len(specific_content['links'])}):\n"
                            )
                            for link in specific_content["links"][
                                :5
                            ]:  # åªæ˜¾ç¤ºå‰5ä¸ªé“¾æ¥
                                specific_content_str += (
                                    f"- [{link['text']}]({link['url']})\n"
                                )

                        if (
                            "code_blocks" in specific_content
                            and specific_content["code_blocks"]
                        ):
                            specific_content_str += f"\nğŸ’» ä»£ç å— ({len(specific_content['code_blocks'])}):\n"
                            for i, code in enumerate(
                                specific_content["code_blocks"][:2]
                            ):  # åªæ˜¾ç¤ºå‰2ä¸ªä»£ç å—
                                specific_content_str += f"```\n{code}\n```\n"

                        analysis_result += specific_content_str

                    # å‡†å¤‡å¯¼å‡ºæ•°æ®
                    export_results.append(
                        {
                            "url": url,
                            "result": {
                                "url": url,
                                "result": analysis_result,
                                "screenshot": None,
                            },
                        }
                    )

        # æ‰§è¡Œå¯¼å‡ºæ“ä½œ
        try:
            import os
            import json
            import time

            # åˆ›å»ºdataç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            data_dir = os.path.join(os.path.dirname(__file__), "data")
            os.makedirs(data_dir, exist_ok=True)

            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = int(time.time())
            if len(export_results) == 1:
                # å•ä¸ªURLå¯¼å‡ºï¼Œä½¿ç”¨åŸŸåä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†
                url = export_results[0]["url"]
                from urllib.parse import urlparse

                parsed = urlparse(url)
                domain = parsed.netloc.replace(".", "_")
                filename = f"web_analysis_{domain}_{timestamp}"
            else:
                # å¤šä¸ªURLå¯¼å‡º
                filename = f"web_analysis_all_{timestamp}"

            # ç¡®å®šæ–‡ä»¶æ‰©å±•å
            file_extension = format_type.lower()
            if file_extension == "markdown":
                file_extension = "md"

            file_path = os.path.join(data_dir, f"{filename}.{file_extension}")

            if format_type.lower() in ["md", "markdown"]:
                # ç”ŸæˆMarkdownæ ¼å¼å†…å®¹
                md_content = "# ç½‘é¡µåˆ†æç»“æœå¯¼å‡º\n\n"
                md_content += f"å¯¼å‡ºæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}\n\n"
                md_content += f"å…± {len(export_results)} ä¸ªåˆ†æç»“æœ\n\n"
                md_content += "---\n\n"

                for i, export_item in enumerate(export_results, 1):
                    url = export_item["url"]
                    result_data = export_item["result"]

                    md_content += f"## {i}. {url}\n\n"
                    md_content += result_data["result"]
                    md_content += "\n\n"
                    md_content += "---\n\n"

                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(md_content)

            elif format_type.lower() == "json":
                # ç”ŸæˆJSONæ ¼å¼å†…å®¹
                json_data = {
                    "export_time": timestamp,
                    "export_time_str": time.strftime(
                        "%Y-%m-%d %H:%M:%S", time.localtime(timestamp)
                    ),
                    "total_results": len(export_results),
                    "results": [],
                }

                for export_item in export_results:
                    url = export_item["url"]
                    result_data = export_item["result"]

                    json_data["results"].append(
                        {
                            "url": url,
                            "analysis_result": result_data["result"],
                            "has_screenshot": result_data["screenshot"] is not None,
                        }
                    )

                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    json.dump(json_data, f, ensure_ascii=False, indent=2)

            elif format_type.lower() == "txt":
                # ç”Ÿæˆçº¯æ–‡æœ¬æ ¼å¼å†…å®¹
                txt_content = "ç½‘é¡µåˆ†æç»“æœå¯¼å‡º\n"
                txt_content += f"å¯¼å‡ºæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}\n"
                txt_content += f"å…± {len(export_results)} ä¸ªåˆ†æç»“æœ\n"
                txt_content += "=" * 50 + "\n\n"

                for i, export_item in enumerate(export_results, 1):
                    url = export_item["url"]
                    result_data = export_item["result"]

                    txt_content += f"{i}. {url}\n"
                    txt_content += "-" * 30 + "\n"
                    txt_content += result_data["result"]
                    txt_content += "\n\n"
                    txt_content += "=" * 50 + "\n\n"

                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(txt_content)

            # å‘é€å¯¼å‡ºæˆåŠŸæ¶ˆæ¯ï¼Œå¹¶é™„å¸¦å¯¼å‡ºæ–‡ä»¶
            from astrbot.api.message_components import Plain, File

            # æ„å»ºæ¶ˆæ¯é“¾
            message_chain = [
                Plain("âœ… åˆ†æç»“æœå¯¼å‡ºæˆåŠŸï¼\n\n"),
                Plain(f"å¯¼å‡ºæ ¼å¼: {format_type}\n"),
                Plain(f"å¯¼å‡ºæ•°é‡: {len(export_results)}\n\n"),
                Plain("ğŸ“ å¯¼å‡ºæ–‡ä»¶ï¼š\n"),
                File(file=file_path, name=os.path.basename(file_path)),
            ]

            yield event.chain_result(message_chain)

            logger.info(
                f"æˆåŠŸå¯¼å‡º {len(export_results)} ä¸ªåˆ†æç»“æœåˆ° {file_path}ï¼Œå¹¶å‘é€ç»™ç”¨æˆ·"
            )

        except Exception as e:
            logger.error(f"å¯¼å‡ºåˆ†æç»“æœå¤±è´¥: {e}")
            yield event.plain_result(f"âŒ å¯¼å‡ºåˆ†æç»“æœå¤±è´¥: {str(e)}")

    def _save_group_blacklist(self):
        """ä¿å­˜ç¾¤èŠé»‘åå•åˆ°é…ç½®æ–‡ä»¶

        è¯¥æ–¹æ³•è´Ÿè´£å°†ç¾¤èŠé»‘åå•åˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œå¹¶ä¿å­˜åˆ°é…ç½®ä¸­

        åŠŸèƒ½è¯´æ˜ï¼š
        - å°†ç¾¤èŠIDåˆ—è¡¨è½¬æ¢ä¸ºæ¢è¡Œåˆ†éš”çš„æ–‡æœ¬
        - æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„group_blacklistå­—æ®µ
        - ä¿å­˜é…ç½®æ›´æ”¹
        - å¤„ç†ä¿å­˜è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„å¼‚å¸¸
        """
        try:
            # å°†ç¾¤èŠåˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªç¾¤èŠID
            group_text = "\n".join(self.group_blacklist)
            # æ›´æ–°é…ç½®å¹¶ä¿å­˜åˆ°æ–‡ä»¶
            self.config["group_blacklist"] = group_text
            self.config.save_config()
        except Exception as e:
            logger.error(f"ä¿å­˜ç¾¤èŠé»‘åå•å¤±è´¥: {e}")

    def _check_cache(self, url: str) -> dict:
        """æ£€æŸ¥æŒ‡å®šURLçš„ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ

        Args:
            url: è¦æ£€æŸ¥ç¼“å­˜çš„ç½‘é¡µURL

        Returns:
            - å¦‚æœç¼“å­˜å­˜åœ¨ä¸”æœ‰æ•ˆï¼Œè¿”å›ç¼“å­˜çš„åˆ†æç»“æœ
            - å¦‚æœç¼“å­˜ä¸å­˜åœ¨æˆ–æ— æ•ˆï¼Œè¿”å›None
        """
        if not self.enable_cache:
            return None

        return self.cache_manager.get(url)

    def _update_cache(self, url: str, result: dict):
        """æ›´æ–°æŒ‡å®šURLçš„ç¼“å­˜

        Args:
            url: è¦æ›´æ–°ç¼“å­˜çš„ç½‘é¡µURL
            result: åŒ…å«åˆ†æç»“æœçš„å­—å…¸ï¼Œæ ¼å¼ä¸_check_cacheè¿”å›å€¼ä¸€è‡´
        """
        if not self.enable_cache:
            return

        self.cache_manager.set(url, result)

    def _clean_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜

        æ³¨æ„ï¼šç¼“å­˜ç®¡ç†å™¨ä¼šè‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜ï¼Œå› æ­¤è¯¥æ–¹æ³•ç›®å‰ç•™ç©º
        å¦‚éœ€æ‰‹åŠ¨æ¸…ç†ï¼Œå¯ä»¥åœ¨æ­¤æ–¹æ³•ä¸­æ·»åŠ ç›¸åº”é€»è¾‘
        """
        # ç¼“å­˜ç®¡ç†å™¨ä¼šè‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜ï¼Œè¿™é‡Œç•™ç©ºå³å¯
        pass

    async def _translate_content(self, event: AstrMessageEvent, content: str) -> str:
        """ç¿»è¯‘ç½‘é¡µå†…å®¹

        è¯¥æ–¹æ³•è´Ÿè´£è°ƒç”¨LLMå¯¹ç½‘é¡µå†…å®¹è¿›è¡Œç¿»è¯‘

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
            content: è¦ç¿»è¯‘çš„ç½‘é¡µå†…å®¹

        Returns:
            - å¦‚æœç¿»è¯‘æˆåŠŸï¼Œè¿”å›ç¿»è¯‘åçš„å†…å®¹
            - å¦‚æœç¿»è¯‘å¤±è´¥æˆ–æœªå¯ç”¨ç¿»è¯‘ï¼Œè¿”å›åŸå§‹å†…å®¹
        """
        if not self.enable_translation:
            return content

        try:
            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨
            if not hasattr(self.context, "llm_generate"):
                logger.error("LLMä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘")
                return content

            # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„LLMæä¾›å•†ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨å½“å‰ä¼šè¯çš„æ¨¡å‹
            provider_id = self.llm_provider
            if not provider_id:
                umo = event.unified_msg_origin
                provider_id = await self.context.get_current_chat_provider_id(umo=umo)

            if not provider_id:
                logger.error("æ— æ³•è·å–LLMæä¾›å•†IDï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘")
                return content

            # ä½¿ç”¨è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯æˆ–é»˜è®¤æç¤ºè¯
            if self.custom_translation_prompt:
                # æ›¿æ¢è‡ªå®šä¹‰æç¤ºè¯ä¸­çš„å˜é‡
                prompt = self.custom_translation_prompt.format(
                    content=content, target_language=self.target_language
                )
            else:
                # é»˜è®¤ç¿»è¯‘æç¤ºè¯
                prompt = f"è¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆ{self.target_language}è¯­è¨€ï¼Œä¿æŒåŸæ–‡æ„æ€ä¸å˜ï¼Œè¯­è¨€æµç•…è‡ªç„¶ï¼š\n\n{content}"

            # è°ƒç”¨LLMè¿›è¡Œç¿»è¯‘
            llm_resp = await self.context.llm_generate(
                chat_provider_id=provider_id, prompt=prompt
            )

            if llm_resp and llm_resp.completion_text:
                return llm_resp.completion_text.strip()
            else:
                logger.error("LLMç¿»è¯‘è¿”å›ä¸ºç©º")
                return content
        except Exception as e:
            logger.error(f"ç¿»è¯‘å†…å®¹å¤±è´¥: {e}")
            return content

    def _extract_specific_content(self, html: str, url: str) -> dict:
        """æå–ç‰¹å®šç±»å‹çš„å†…å®¹

        è¯¥æ–¹æ³•è´Ÿè´£ä»HTMLä¸­æå–ç‰¹å®šç±»å‹çš„å†…å®¹ï¼Œå¦‚å›¾ç‰‡ã€é“¾æ¥ã€ä»£ç å—ç­‰

        Args:
            html: ç½‘é¡µHTMLå†…å®¹
            url: ç½‘é¡µURL

        Returns:
            - å¦‚æœæå–æˆåŠŸï¼Œè¿”å›åŒ…å«ç‰¹å®šå†…å®¹çš„å­—å…¸
            - å¦‚æœæå–å¤±è´¥æˆ–æœªå¯ç”¨ç‰¹å®šå†…å®¹æå–ï¼Œè¿”å›ç©ºå­—å…¸
        """
        if not self.enable_specific_extraction:
            return {}

        try:
            # ä½¿ç”¨WebAnalyzerå®ä¾‹æå–ç‰¹å®šå†…å®¹
            analyzer = WebAnalyzer(
                self.max_content_length, self.timeout, self.user_agent
            )
            return analyzer.extract_specific_content(html, url, self.extract_types)
        except Exception as e:
            logger.error(f"æå–ç‰¹å®šå†…å®¹å¤±è´¥: {e}")
            return {}

    async def _send_analysis_result(self, event, analysis_results):
        """å‘é€åˆ†æç»“æœï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨åˆå¹¶è½¬å‘

        è¯¥æ–¹æ³•è´Ÿè´£å°†åˆ†æç»“æœå‘é€ç»™ç”¨æˆ·ï¼Œæ”¯æŒæ™®é€šæ¶ˆæ¯å’Œåˆå¹¶è½¬å‘ä¸¤ç§æ–¹å¼

        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
            analysis_results: åŒ…å«æ‰€æœ‰åˆ†æç»“æœçš„åˆ—è¡¨
        """
        try:
            from astrbot.api.message_components import Node, Plain, Nodes, Image
            import tempfile
            import os

            # æ£€æŸ¥æ˜¯å¦ä¸ºç¾¤èŠæ¶ˆæ¯ä¸”åˆå¹¶è½¬å‘åŠŸèƒ½å·²å¯ç”¨
            group_id = None
            if hasattr(event, "group_id") and event.group_id:
                group_id = event.group_id
            elif (
                hasattr(event, "message_obj")
                and hasattr(event.message_obj, "group_id")
                and event.message_obj.group_id
            ):
                group_id = event.message_obj.group_id

            # æ ¹æ®æ¶ˆæ¯ç±»å‹å†³å®šæ˜¯å¦ä½¿ç”¨åˆå¹¶è½¬å‘
            is_group = bool(group_id)
            is_private = not is_group
            
            # å¦‚æœæ˜¯ç¾¤èŠä¸”ç¾¤èŠåˆå¹¶è½¬å‘å·²å¯ç”¨ï¼Œæˆ–è€…æ˜¯ç§èŠä¸”ç§èŠåˆå¹¶è½¬å‘å·²å¯ç”¨
            if (is_group and self.merge_forward_enabled["group"]) or (is_private and self.merge_forward_enabled["private"]):
                # ä½¿ç”¨åˆå¹¶è½¬å‘ - å°†æ‰€æœ‰åˆ†æç»“æœåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
                nodes = []

                # æ·»åŠ æ€»æ ‡é¢˜èŠ‚ç‚¹
                total_title_node = Node(
                    uin=event.get_sender_id(),
                    name="ç½‘é¡µåˆ†æç»“æœæ±‡æ€»",
                    content=[Plain(f"å…±{len(analysis_results)}ä¸ªç½‘é¡µåˆ†æç»“æœ")],
                )
                nodes.append(total_title_node)

                # ä¸ºæ¯ä¸ªURLæ·»åŠ åˆ†æç»“æœèŠ‚ç‚¹
                for i, result_data in enumerate(analysis_results, 1):
                    url = result_data["url"]
                    analysis_result = result_data["result"]
                    screenshot = result_data.get("screenshot")

                    # æ·»åŠ å½“å‰URLçš„æ ‡é¢˜èŠ‚ç‚¹
                    url_title_node = Node(
                        uin=event.get_sender_id(),
                        name=f"åˆ†æç»“æœ {i}",
                        content=[Plain(f"ç¬¬{i}ä¸ªç½‘é¡µåˆ†æç»“æœ - {url}")],
                    )
                    nodes.append(url_title_node)

                    # æ·»åŠ å½“å‰URLçš„å†…å®¹èŠ‚ç‚¹
                    content_node = Node(
                        uin=event.get_sender_id(),
                        name="è¯¦ç»†åˆ†æ",
                        content=[Plain(analysis_result)],
                    )
                    nodes.append(content_node)

                # ä½¿ç”¨NodesåŒ…è£…æ‰€æœ‰èŠ‚ç‚¹ï¼Œåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
                merge_forward_message = Nodes(nodes)

                # å‘é€åˆå¹¶è½¬å‘æ¶ˆæ¯
                yield event.chain_result([merge_forward_message])

                # å¦‚æœæœ‰æˆªå›¾ï¼Œé€ä¸ªå‘é€æˆªå›¾
                for result_data in analysis_results:
                    screenshot = result_data.get("screenshot")
                    if screenshot:
                        try:
                            # æ ¹æ®æˆªå›¾æ ¼å¼è®¾ç½®æ–‡ä»¶åç¼€
                            suffix = (
                                f".{self.screenshot_format}"
                                if self.screenshot_format
                                else ".jpg"
                            )
                            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æˆªå›¾
                            with tempfile.NamedTemporaryFile(
                                suffix=suffix, delete=False
                            ) as temp_file:
                                temp_file.write(screenshot)
                                temp_file_path = temp_file.name

                            # ä½¿ç”¨Image.fromFileSystem()æ–¹æ³•å‘é€å›¾ç‰‡
                            image_component = Image.fromFileSystem(temp_file_path)
                            yield event.chain_result([image_component])
                            logger.info(
                                f"ç¾¤èŠ {group_id} ä½¿ç”¨åˆå¹¶è½¬å‘å‘é€åˆ†æç»“æœï¼Œå¹¶å‘é€æˆªå›¾"
                            )

                            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                            os.unlink(temp_file_path)
                        except Exception as e:
                            logger.error(f"å‘é€æˆªå›¾å¤±è´¥: {e}")
                            # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
                            if "temp_file_path" in locals() and os.path.exists(
                                temp_file_path
                            ):
                                os.unlink(temp_file_path)
                logger.info(
                    f"ç¾¤èŠ {group_id} ä½¿ç”¨åˆå¹¶è½¬å‘å‘é€{len(analysis_results)}ä¸ªåˆ†æç»“æœ"
                )
            else:
                # æ™®é€šå‘é€ - é€ä¸ªå‘é€åˆ†æç»“æœ
                for i, result_data in enumerate(analysis_results, 1):
                    url = result_data["url"]
                    analysis_result = result_data["result"]
                    screenshot = result_data.get("screenshot")

                    # å‘é€åˆ†æç»“æœæ–‡æœ¬
                    if len(analysis_results) == 1:
                        result_text = f"ç½‘é¡µåˆ†æç»“æœï¼š\n{analysis_result}"
                    else:
                        result_text = f"ç¬¬{i}/{len(analysis_results)}ä¸ªç½‘é¡µåˆ†æç»“æœï¼š\n{analysis_result}"
                    yield event.plain_result(result_text)

                    # å¦‚æœæœ‰æˆªå›¾ï¼Œå‘é€æˆªå›¾
                    if screenshot:
                        try:
                            # æ ¹æ®æˆªå›¾æ ¼å¼è®¾ç½®æ–‡ä»¶åç¼€
                            suffix = (
                                f".{self.screenshot_format}"
                                if self.screenshot_format
                                else ".jpg"
                            )
                            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æˆªå›¾
                            with tempfile.NamedTemporaryFile(
                                suffix=suffix, delete=False
                            ) as temp_file:
                                temp_file.write(screenshot)
                                temp_file_path = temp_file.name

                            # ä½¿ç”¨Image.fromFileSystem()æ–¹æ³•å‘é€å›¾ç‰‡
                            image_component = Image.fromFileSystem(temp_file_path)
                            yield event.chain_result([image_component])
                            logger.info("æ™®é€šå‘é€åˆ†æç»“æœï¼Œå¹¶å‘é€æˆªå›¾")

                            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                            os.unlink(temp_file_path)
                        except Exception as e:
                            logger.error(f"å‘é€æˆªå›¾å¤±è´¥: {e}")
                            # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
                            if "temp_file_path" in locals() and os.path.exists(
                                temp_file_path
                            ):
                                os.unlink(temp_file_path)
                message_type = "ç¾¤èŠ" if group_id else "ç§èŠ"
                logger.info(
                    f"{message_type}æ¶ˆæ¯æ™®é€šå‘é€{len(analysis_results)}ä¸ªåˆ†æç»“æœ"
                )
        except Exception as e:
            logger.error(f"å‘é€åˆ†æç»“æœå¤±è´¥: {e}")
            yield event.plain_result(f"âŒ å‘é€åˆ†æç»“æœå¤±è´¥: {str(e)}")

    async def terminate(self):
        """æ’ä»¶å¸è½½æ—¶çš„æ¸…ç†å·¥ä½œ"""
        logger.info("ç½‘é¡µåˆ†ææ’ä»¶å·²å¸è½½")
